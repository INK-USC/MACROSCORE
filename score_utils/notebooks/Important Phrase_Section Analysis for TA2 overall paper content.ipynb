{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_path = \"data/TA2_classify_data_final_with_imp_sentences_bilstm_only.json\"\n",
    "full_data_path = \"data/ta2_classify_folds/fold_full/data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886\n",
      "886\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "\n",
    "with open(seg_path, \"r\") as f:\n",
    "    seg_data = json.load(f)\n",
    "    \n",
    "print(len(data))\n",
    "\n",
    "\n",
    "with open(full_data_path, \"r\") as f:\n",
    "    full_data = json.load(f)\n",
    "    \n",
    "print(len(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the important phrase extractions to reproducible vs non-reproducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_path1 = \"data/ta2_important_sentences_phrases/socrepr_sentences_full_v1.json\"\n",
    "phrase_path2 = \"data/ta2_important_sentences_phrases/socrepr_sentences_full_v2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(phrase_path1, \"r\") as f:\n",
    "    phrase_data = json.load(f)\n",
    "    \n",
    "with open(phrase_path2, \"r\") as f:\n",
    "    phrase_data += json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paper_id and segment heading map:\n",
    "\n",
    "seg_data_map = {}\n",
    "for i in range(len(seg_data)):\n",
    "    paper_id = seg_data[i][\"paper_id\"]\n",
    "    \n",
    "    seg_data_map[paper_id] = {}\n",
    "    seg_data_map[paper_id][\"important_segment_idx\"] = seg_data[i][\"important_segment_idx\"]\n",
    "    seg_data_map[paper_id][\"important_section_heading_or_idx\"] = seg_data[i][\"important_section_heading_or_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the seg data with phrase data:\n",
    "\n",
    "for i in range(len(phrase_data)):\n",
    "    paper_id = phrase_data[i][\"paper_id\"]\n",
    "    if seg_data_map.get(paper_id) is not None:\n",
    "        for k, v in seg_data_map[paper_id].items():\n",
    "            phrase_data[i][k] = v\n",
    "    else:\n",
    "        print(\"Error for \", paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into reprod vs non-reprod:\n",
    "\n",
    "def seperateData(cur_phrase_data):\n",
    "    reprod = []\n",
    "    non_reprod = []\n",
    "    \n",
    "    for i in range(len(cur_phrase_data)):\n",
    "        if cur_phrase_data[i][\"label\"] == 0 and cur_phrase_data[i][\"predicted_label\"] == 0:\n",
    "            non_reprod.append(cur_phrase_data[i])\n",
    "        elif cur_phrase_data[i][\"label\"] == 1 and cur_phrase_data[i][\"predicted_label\"] == 1:\n",
    "            reprod.append(cur_phrase_data[i])\n",
    "            \n",
    "    print(len(reprod), len(non_reprod))\n",
    "    return reprod, non_reprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314 377\n"
     ]
    }
   ],
   "source": [
    "reprod_phrase_data, non_reprod_phrase_data = seperateData(phrase_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data splits:\n",
    "\n",
    "base_path = \"TA2_phrase_level_extractions_from_overall_paper_content.json\"\n",
    "reprod_path = os.path.join(\"reproducibility_papers_\" + base_path)\n",
    "non_reprod_path = os.path.join(\"non_reproducibility_papers_\" + base_path)\n",
    "full_path = os.path.join(\"all_papers_\" + base_path)\n",
    "\n",
    "with open(reprod_path, \"w\") as f:\n",
    "    json.dump(reprod_phrase_data, f, indent=2)\n",
    "    \n",
    "with open(non_reprod_path, \"w\") as f:\n",
    "    json.dump(non_reprod_phrase_data, f, indent=2)\n",
    "    \n",
    "with open(full_path, \"w\") as f:\n",
    "    json.dump(phrase_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the phrase data:\n",
    "\n",
    "def filterPhrases(cur_phrase_list, topk=3):\n",
    "    new_phrase_list = []\n",
    "    hash_dict = {}\n",
    "    \n",
    "    max_phrase_length = -1\n",
    "    for cur_phrase in cur_phrase_list:\n",
    "        if len(cur_phrase[1].split()) > max_phrase_length:\n",
    "            max_phrase_length = len(cur_phrase[1].split())\n",
    "    \n",
    "    phrase_length_limit = 0.80 * max_phrase_length\n",
    "    for cur_phrase in cur_phrase_list:\n",
    "        \n",
    "        # Remove long phrases\n",
    "        if len(cur_phrase[1].split()) >= phrase_length_limit:\n",
    "            continue\n",
    "            \n",
    "        # Remove duplicates\n",
    "        if hash_dict.get(cur_phrase[1]) is not None:\n",
    "            continue\n",
    "        hash_dict[cur_phrase[1]] = True\n",
    "        \n",
    "        \n",
    "        new_phrase_list.append(cur_phrase)\n",
    "        if len(new_phrase_list) >= topk:\n",
    "            break\n",
    "            \n",
    "    assert(len(new_phrase_list) <= topk)\n",
    "    return new_phrase_list\n",
    "\n",
    "\n",
    "def filterData(cur_phrase_data, topk=3):\n",
    "    new_phrase_data = []\n",
    "    for i in range(len(cur_phrase_data)):\n",
    "        cur_record = {}\n",
    "        for k, v in cur_phrase_data[i].items():\n",
    "            cur_record[k] = v\n",
    "            \n",
    "        cur_record[\"important_phrases\"] = filterPhrases(cur_record[\"important_phrases\"], topk=topk)\n",
    "        new_phrase_data.append(cur_record)\n",
    "        \n",
    "    return new_phrase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_data_filtered = filterData(phrase_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314 377\n"
     ]
    }
   ],
   "source": [
    "reprod_phrase_data, non_reprod_phrase_data = seperateData(phrase_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data splits:\n",
    "\n",
    "base_path = \"filtered_TA2_phrase_level_extractions_from_overall_paper_content.json\"\n",
    "reprod_path = os.path.join(\"reproducibility_papers_\" + base_path)\n",
    "non_reprod_path = os.path.join(\"non_reproducibility_papers_\" + base_path)\n",
    "full_path = os.path.join(\"all_papers_\" + base_path)\n",
    "\n",
    "with open(reprod_path, \"w\") as f:\n",
    "    json.dump(reprod_phrase_data, f, indent=2)\n",
    "    \n",
    "with open(non_reprod_path, \"w\") as f:\n",
    "    json.dump(non_reprod_phrase_data, f, indent=2)\n",
    "    \n",
    "with open(full_path, \"w\") as f:\n",
    "    json.dump(phrase_data_filtered, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get important segment distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_paper_sentence_wise(self, content):\n",
    "    device = self.device\n",
    "    embeddings = []\n",
    "    for cur_section in content:\n",
    "        if len(cur_section['text']) == 0:\n",
    "            continue\n",
    "        split_sentences = nltk.sent_tokenize(cur_section['text'])\n",
    "        for sentence in split_sentences:\n",
    "            cur_sentence_tokens = sentence.split()\n",
    "\n",
    "            # if a single sentence is more than window size (cut it down)\n",
    "            if len(cur_sentence_tokens) > self.window_size:\n",
    "                cur_sentence_tokens = cur_sentence_tokens[:self.window_size]\n",
    "            else:\n",
    "                cur_sentence_tokens = cur_sentence_tokens\n",
    "\n",
    "            # Filter out sentences with only urls/citations etc.\n",
    "            if len(cur_sentence_tokens) >= 7:\n",
    "                cur_sentence = \" \".join(cur_sentence_tokens)\n",
    "                input_ids = torch.tensor(self.lm_tokenizer.encode(cur_sentence)).unsqueeze(0)  # Batch size 1\n",
    "                try:\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    outputs = self.lm_embeddings_model(input_ids)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "                cls_embedding = outputs[0][:, 0, :].squeeze().detach().cpu()\n",
    "                del outputs\n",
    "                del input_ids\n",
    "                embeddings.append(cls_embedding)\n",
    "\n",
    "    final_embs = torch.stack(embeddings)\n",
    "    return final_embs, len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
