{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get phrase-level extraction data for reproducible vs non-reproducible papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extraction_path = \"../repr_claims_results/socrepr_claims_train.json\"\n",
    "dev_extraction_path = \"../repr_claims_results/socrepr_claims_dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_extraction_path, \"r\") as f:\n",
    "    train_res = json.load(f)\n",
    "    \n",
    "with open(dev_extraction_path, \"r\") as f:\n",
    "    dev_res = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_full = train_res.copy()\n",
    "res_full = res_full + dev_res\n",
    "df_res = pd.DataFrame(res_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"important_segment_idx\"] = df_res.apply(lambda x: pid_seg_idx_map[x[\"paper_id\"]][0], axis=1)\n",
    "df_res[\"important_segment\"] = df_res.apply(lambda x: pid_seg_idx_map[x[\"paper_id\"]][1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(845, 6)\n",
      "Index(['paper_id', 'label', 'predicted_label', 'important_segment',\n",
      "       'important_phrases', 'important_segment_idx'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_res.shape)\n",
    "print(df_res.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_seg_map = {\n",
    "    '0': \"claim2\",\n",
    "    '1':\"claim3a\",\n",
    "    '2':\"claim3b\",\n",
    "    '3':\"claim4\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"important_segment_idx\"] = df_res.apply(lambda x: reverse_seg_map[str(x[\"important_segment_idx\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPhrases(phrase_list):\n",
    "    out_list = []\n",
    "    for cur_phrase in phrase_list:\n",
    "        new_phrase = cur_phrase[1].replace(\"- lrb -\", \"(\").replace(\"- rrb -\", \")\")\n",
    "        out_list.append([cur_phrase[0], new_phrase])\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"- lrb - and - rrb -\" tokens:\n",
    "df_res[\"important_phrases\"] = df_res.apply(lambda x: cleanPhrases(x[\"important_phrases\"][0:5]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11 = df_res[(df_res[\"label\"] == 1) & (df_res[\"predicted_label\"] == 1)]\n",
    "df_00 = df_res[(df_res[\"label\"] == 0) & (df_res[\"predicted_label\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_path = \"phrase_extractions_for_reproducible_papers_TA2.json\"\n",
    "df_11_json_out = df_11.to_json(orient=\"records\")\n",
    "\n",
    "# with open(rep_path, \"w\") as f:\n",
    "#     json.dump(json.loads(df_11_json_out), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rep_path = \"phrase_extractions_for_non_reproducible_papers_TA2.json\"\n",
    "df_00_json_out = df_00.to_json(orient=\"records\")\n",
    "\n",
    "# with open(non_rep_path, \"w\") as f:\n",
    "#     json.dump(json.loads(df_00_json_out), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature extraction data for reproducible and non-reproducible papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_path = \"../danny_submission/feature_extractions_TA2/feature_extractions_for_all_papers_TA2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(feat_path, \"r\") as f:\n",
    "    feat_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_feat_list = {}\n",
    "non_rep_feat_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in feat_data.items():\n",
    "    if pid_label_map.get(key) is None:\n",
    "        continue\n",
    "        \n",
    "    if pid_label_map[key] == 1:\n",
    "        rep_feat_list[key] = val\n",
    "        rep_feat_list[key][\"label\"] = 1\n",
    "    elif pid_label_map[key] == 0:\n",
    "        non_rep_feat_list[key] = val\n",
    "        non_rep_feat_list[key][\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStats(cur_feat_data):\n",
    "    count_dict = defaultdict(lambda : 0)\n",
    "    count_none_dict = defaultdict(lambda : 0)\n",
    "    \n",
    "    for k, v in cur_feat_data.items():\n",
    "        ns = v[\"Validity_of_Inference\"][\"Number_of_Studies\"]\n",
    "        es = v[\"Validity_of_Inference\"][\"Effect_Size\"]\n",
    "        nm = v[\"Validity_of_Inference\"][\"Number_of_Models\"]\n",
    "        pv = v[\"Validity_of_Inference\"][\"P_Values\"]\n",
    "        mn = v[\"Validity_of_Inference\"][\"Model_Names\"]\n",
    "        ss = v[\"Design_Quality\"][\"Sample_Sizes\"]\n",
    "        claim_cnts = {\"claim2_distance\": 0, \"claim3a_distance\": 0, \"claim3b_distance\": 0, \"claim4_distance\": 0}\n",
    "        \n",
    "        if ns is None:\n",
    "            count_none_dict[\"Number_of_Studies\"] += 1\n",
    "        else:\n",
    "            count_dict[\"Number_of_Studies\"] += 1\n",
    "            \n",
    "        if es is None:\n",
    "            count_none_dict[\"Effect_Size\"] += 1\n",
    "        elif len(es) !=0:\n",
    "            count_dict[\"Effect_Size\"] += 1\n",
    "            cur_val = es[0]\n",
    "            for k, v in cur_val.items():\n",
    "                if k != \"sent_idx\" and v is not None:\n",
    "                    claim_cnts[k] = 1\n",
    "            \n",
    "        if nm is None:\n",
    "            count_none_dict[\"Number_of_Models\"] += 1\n",
    "        else:\n",
    "            count_dict[\"Number_of_Models\"] += 1\n",
    "            \n",
    "        if pv is None:\n",
    "            count_none_dict[\"P_Values\"] += 1\n",
    "        elif len(pv) !=0:\n",
    "            count_dict[\"P_Values\"] += 1\n",
    "            cur_val = pv[0]\n",
    "            for k, v in cur_val.items():\n",
    "                if k != \"sent_idx\" and v is not None:\n",
    "                    claim_cnts[k] = 1\n",
    "        \n",
    "        if mn is None:\n",
    "            count_none_dict[\"Model_Names\"] += 1\n",
    "        elif len(mn) !=0:\n",
    "            count_dict[\"Model_Names\"] += 1\n",
    "            cur_val = mn[0]\n",
    "            for k, v in cur_val.items():\n",
    "                if k != \"sent_idx\" and v is not None:\n",
    "                    claim_cnts[k] = 1\n",
    "        \n",
    "        if ss is None:\n",
    "            count_none_dict[\"Sample_Sizes\"] += 1\n",
    "        elif len(ss) !=0:\n",
    "            count_dict[\"Sample_Sizes\"] += 1\n",
    "            cur_val = ss[0]\n",
    "            for k, v in cur_val.items():\n",
    "                if k != \"sent_idx\" and v is not None:\n",
    "                    claim_cnts[k] = 1\n",
    "                    \n",
    "        for k, v in claim_cnts.items():\n",
    "            if k==\"value\":\n",
    "                continue\n",
    "            if v == 1:\n",
    "                count_dict[k] += 1\n",
    "            else:\n",
    "                count_none_dict[k] += 1\n",
    "                \n",
    "    count_dict = dict(sorted(dict(count_dict).items(), key=lambda x: x[0]))\n",
    "    count_none_dict = dict(sorted(dict(count_none_dict).items(), key=lambda x: x[0]))\n",
    "    \n",
    "    percent_dict = {}\n",
    "    percent_none_dict = {}\n",
    "    total_num_papers = len(cur_feat_data.keys())\n",
    "    for k, v in count_dict.items():\n",
    "        percent_dict[k] = round(v * 100/total_num_papers, 2)\n",
    "    \n",
    "    for k, v in count_none_dict.items():\n",
    "        percent_none_dict[k] = round(v * 100/total_num_papers, 2)\n",
    "    \n",
    "    print(\"Count of non-None entries in papers:\")\n",
    "    print(count_dict, \"\\n\")\n",
    "    \n",
    "    print(\"Percent of non-None entries in papers:\")\n",
    "    print(percent_dict, \"\\n\")\n",
    "    \n",
    "    print(\"Count of None entries in papers:\")\n",
    "    print(count_none_dict)\n",
    "    \n",
    "    print(\"Percent of None entries in papers:\")\n",
    "    print(percent_none_dict, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all 2380 TA2 papers: \n",
      "Count of non-None entries in papers:\n",
      "{'Effect_Size': 569, 'Model_Names': 2367, 'Number_of_Models': 273, 'Number_of_Studies': 87, 'P_Values': 1748, 'Sample_Sizes': 2329, 'claim2_distance': 1722, 'claim3a_distance': 656, 'claim3b_distance': 977, 'claim4_distance': 617} \n",
      "\n",
      "Percent of non-None entries in papers:\n",
      "{'Effect_Size': 23.91, 'Model_Names': 99.45, 'Number_of_Models': 11.47, 'Number_of_Studies': 3.66, 'P_Values': 73.45, 'Sample_Sizes': 97.86, 'claim2_distance': 72.35, 'claim3a_distance': 27.56, 'claim3b_distance': 41.05, 'claim4_distance': 25.92} \n",
      "\n",
      "Count of None entries in papers:\n",
      "{'Effect_Size': 1811, 'Model_Names': 13, 'Number_of_Models': 2107, 'Number_of_Studies': 2293, 'P_Values': 632, 'Sample_Sizes': 36, 'claim2_distance': 658, 'claim3a_distance': 1724, 'claim3b_distance': 1403, 'claim4_distance': 1763}\n",
      "Percent of None entries in papers:\n",
      "{'Effect_Size': 76.09, 'Model_Names': 0.55, 'Number_of_Models': 88.53, 'Number_of_Studies': 96.34, 'P_Values': 26.55, 'Sample_Sizes': 1.51, 'claim2_distance': 27.65, 'claim3a_distance': 72.44, 'claim3b_distance': 58.95, 'claim4_distance': 74.08} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For all {} TA2 papers: \".format(len(feat_data)))\n",
    "getStats(feat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 392 reproducible papers: \n",
      "Count of non-None entries in papers:\n",
      "{'Effect_Size': 82, 'Model_Names': 387, 'Number_of_Models': 51, 'Number_of_Studies': 12, 'P_Values': 237, 'Sample_Sizes': 376, 'claim2_distance': 276, 'claim3a_distance': 118, 'claim3b_distance': 165, 'claim4_distance': 121} \n",
      "\n",
      "Percent of non-None entries in papers:\n",
      "{'Effect_Size': 20.92, 'Model_Names': 98.72, 'Number_of_Models': 13.01, 'Number_of_Studies': 3.06, 'P_Values': 60.46, 'Sample_Sizes': 95.92, 'claim2_distance': 70.41, 'claim3a_distance': 30.1, 'claim3b_distance': 42.09, 'claim4_distance': 30.87} \n",
      "\n",
      "Count of None entries in papers:\n",
      "{'Effect_Size': 310, 'Model_Names': 5, 'Number_of_Models': 341, 'Number_of_Studies': 380, 'P_Values': 155, 'Sample_Sizes': 14, 'claim2_distance': 116, 'claim3a_distance': 274, 'claim3b_distance': 227, 'claim4_distance': 271}\n",
      "Percent of None entries in papers:\n",
      "{'Effect_Size': 79.08, 'Model_Names': 1.28, 'Number_of_Models': 86.99, 'Number_of_Studies': 96.94, 'P_Values': 39.54, 'Sample_Sizes': 3.57, 'claim2_distance': 29.59, 'claim3a_distance': 69.9, 'claim3b_distance': 57.91, 'claim4_distance': 69.13} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For {} reproducible papers: \".format(len(rep_feat_list)))\n",
    "getStats(rep_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 492 non-reproducible papers: \n",
      "Count of non-None entries in papers:\n",
      "{'Effect_Size': 145, 'Model_Names': 490, 'Number_of_Models': 53, 'Number_of_Studies': 36, 'P_Values': 426, 'Sample_Sizes': 488, 'claim2_distance': 339, 'claim3a_distance': 128, 'claim3b_distance': 181, 'claim4_distance': 126} \n",
      "\n",
      "Percent of non-None entries in papers:\n",
      "{'Effect_Size': 29.47, 'Model_Names': 99.59, 'Number_of_Models': 10.77, 'Number_of_Studies': 7.32, 'P_Values': 86.59, 'Sample_Sizes': 99.19, 'claim2_distance': 68.9, 'claim3a_distance': 26.02, 'claim3b_distance': 36.79, 'claim4_distance': 25.61} \n",
      "\n",
      "Count of None entries in papers:\n",
      "{'Effect_Size': 347, 'Model_Names': 2, 'Number_of_Models': 439, 'Number_of_Studies': 456, 'P_Values': 66, 'Sample_Sizes': 1, 'claim2_distance': 153, 'claim3a_distance': 364, 'claim3b_distance': 311, 'claim4_distance': 366}\n",
      "Percent of None entries in papers:\n",
      "{'Effect_Size': 70.53, 'Model_Names': 0.41, 'Number_of_Models': 89.23, 'Number_of_Studies': 92.68, 'P_Values': 13.41, 'Sample_Sizes': 0.2, 'claim2_distance': 31.1, 'claim3a_distance': 73.98, 'claim3b_distance': 63.21, 'claim4_distance': 74.39} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For {} non-reproducible papers: \".format(len(non_rep_feat_list)))\n",
    "getStats(non_rep_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_feat_path = \"feature_extractions_for_reproducible_papers_TA2.json\"\n",
    "non_rep_feat_path = \"feature_extractions_for_non_reproducible_papers_TA2.json\"\n",
    "\n",
    "# with open(rep_feat_path, \"w\") as f:\n",
    "#     json.dump(rep_feat_list, f, indent=2)\n",
    "    \n",
    "# with open(non_rep_feat_path, \"w\") as f:\n",
    "#     json.dump(non_rep_feat_list, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order the feature extractions based on distance to claims:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList(input_list, topk=5):\n",
    "    out_list = [None] * topk\n",
    "    based_on = None\n",
    "    \n",
    "    if input_list is None or len(input_list) == 0:\n",
    "        return out_list, based_on\n",
    "    \n",
    "    first_val = input_list[0]\n",
    "    \n",
    "    if first_val.get(\"claim4_distance\") is not None:\n",
    "        sorted_list = sorted(input_list, key=lambda x: abs(x[\"claim4_distance\"]))\n",
    "        sorted_list = [x[\"value\"] for x in sorted_list]\n",
    "        based_on = \"claim4_distance\"\n",
    "        \n",
    "    elif first_val.get(\"claim3b_distance\") is not None:\n",
    "        sorted_list = sorted(input_list, key=lambda x: abs(x[\"claim3b_distance\"]))\n",
    "        sorted_list = [x[\"value\"] for x in sorted_list]\n",
    "        based_on = \"claim3b_distance\"\n",
    "        \n",
    "    else:\n",
    "        return out_list, based_on\n",
    "    \n",
    "    for i in range(len(sorted_list)):\n",
    "        if i>=topk:\n",
    "            break\n",
    "        out_list[i] = sorted_list[i]\n",
    "    return out_list, based_on\n",
    "\n",
    "\n",
    "def convertToCSV(input_data):\n",
    "    df_result = []\n",
    "    \n",
    "    df_cols = [\"paper_id\", \"Number_of_Studies\", \"Number_of_Models\"]\n",
    "    for i in range(1, 6):\n",
    "        df_cols.append(\"Effect_Size_\" + str(i))\n",
    "    for i in range(1, 6):\n",
    "        df_cols.append(\"P_Values_\" + str(i))\n",
    "    for i in range(1, 6):\n",
    "        df_cols.append(\"Model_Names_\" + str(i))\n",
    "    for i in range(1, 6):\n",
    "        df_cols.append(\"Sample_Sizes_\" + str(i))\n",
    "    df_cols.append(\"sorting_order_based_on\")\n",
    "    \n",
    "    for cur_pid, cur_val in input_data.items():\n",
    "        num_studies = cur_val[\"Validity_of_Inference\"][\"Number_of_Studies\"]\n",
    "        num_models = cur_val[\"Validity_of_Inference\"][\"Number_of_Models\"]\n",
    "        \n",
    "        effect_size_list, based_on = getList(cur_val[\"Validity_of_Inference\"][\"Effect_Size\"])\n",
    "        pv_list, based_on = getList(cur_val[\"Validity_of_Inference\"][\"P_Values\"])\n",
    "        model_names_list, based_on = getList(cur_val[\"Validity_of_Inference\"][\"Model_Names\"])\n",
    "        ss_list, based_on = getList(cur_val[\"Design_Quality\"][\"Sample_Sizes\"])\n",
    "        \n",
    "        cur_row = [cur_pid, num_studies, num_models]\n",
    "        cur_row += effect_size_list\n",
    "        cur_row += pv_list\n",
    "        cur_row += model_names_list\n",
    "        cur_row += ss_list\n",
    "        cur_row += [based_on]\n",
    "        \n",
    "        df_result.append(cur_row)\n",
    "    \n",
    "    df_res = pd.DataFrame(data=df_result, columns=df_cols)\n",
    "    df_res = df_res[df_res[\"sorting_order_based_on\"].notnull()]\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 216 reproducible TA2 papers, count of non-null values in each columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paper_id                  216\n",
       "Number_of_Studies           6\n",
       "Number_of_Models           32\n",
       "Effect_Size_1              41\n",
       "Effect_Size_2              30\n",
       "Effect_Size_3              18\n",
       "Effect_Size_4              13\n",
       "Effect_Size_5              11\n",
       "P_Values_1                125\n",
       "P_Values_2                107\n",
       "P_Values_3                 97\n",
       "P_Values_4                 88\n",
       "P_Values_5                 82\n",
       "Model_Names_1             215\n",
       "Model_Names_2             214\n",
       "Model_Names_3             211\n",
       "Model_Names_4             203\n",
       "Model_Names_5             187\n",
       "Sample_Sizes_1            216\n",
       "Sample_Sizes_2            205\n",
       "Sample_Sizes_3            192\n",
       "Sample_Sizes_4            175\n",
       "Sample_Sizes_5            159\n",
       "sorting_order_based_on    216\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_path = \"feature_extractions_for_reproducible_papers_TA2.json\"\n",
    "with open(feat_path, \"r\") as f:\n",
    "    feat_data = json.load(f)\n",
    "    \n",
    "feat_path_csv = feat_path.split(\".\")[0] + \".csv\"\n",
    "df_feat = convertToCSV(feat_data)\n",
    "df_feat.to_csv(feat_path_csv, index=False)\n",
    "print(\"For {} reproducible TA2 papers, count of non-null values in each columns\".format(df_feat.shape[0]))\n",
    "df_feat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 248 non-reproducible TA2 papers, count of non-null values in each columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paper_id                  248\n",
       "Number_of_Studies           8\n",
       "Number_of_Models           27\n",
       "Effect_Size_1              74\n",
       "Effect_Size_2              52\n",
       "Effect_Size_3              36\n",
       "Effect_Size_4              25\n",
       "Effect_Size_5              19\n",
       "P_Values_1                205\n",
       "P_Values_2                198\n",
       "P_Values_3                189\n",
       "P_Values_4                181\n",
       "P_Values_5                171\n",
       "Model_Names_1             247\n",
       "Model_Names_2             238\n",
       "Model_Names_3             231\n",
       "Model_Names_4             224\n",
       "Model_Names_5             211\n",
       "Sample_Sizes_1            248\n",
       "Sample_Sizes_2            234\n",
       "Sample_Sizes_3            212\n",
       "Sample_Sizes_4            195\n",
       "Sample_Sizes_5            172\n",
       "sorting_order_based_on    248\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_path = \"feature_extractions_for_non_reproducible_papers_TA2.json\"\n",
    "with open(feat_path, \"r\") as f:\n",
    "    feat_data = json.load(f)\n",
    "    \n",
    "feat_path_csv = feat_path.split(\".\")[0] + \".csv\"\n",
    "df_feat = convertToCSV(feat_data)\n",
    "df_feat.to_csv(feat_path_csv, index=False)\n",
    "print(\"For {} non-reproducible TA2 papers, count of non-null values in each columns\".format(df_feat.shape[0]))\n",
    "df_feat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1245 all TA2 papers, count of non-null values in each columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paper_id                  1245\n",
       "Number_of_Studies           26\n",
       "Number_of_Models           136\n",
       "Effect_Size_1              273\n",
       "Effect_Size_2              190\n",
       "Effect_Size_3              128\n",
       "Effect_Size_4               92\n",
       "Effect_Size_5               66\n",
       "P_Values_1                 852\n",
       "P_Values_2                 788\n",
       "P_Values_3                 733\n",
       "P_Values_4                 674\n",
       "P_Values_5                 631\n",
       "Model_Names_1             1241\n",
       "Model_Names_2             1223\n",
       "Model_Names_3             1196\n",
       "Model_Names_4             1129\n",
       "Model_Names_5             1046\n",
       "Sample_Sizes_1            1245\n",
       "Sample_Sizes_2            1187\n",
       "Sample_Sizes_3            1106\n",
       "Sample_Sizes_4            1009\n",
       "Sample_Sizes_5             892\n",
       "sorting_order_based_on    1245\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_path = \"feature_extractions_for_all_papers_TA2.json\"\n",
    "with open(feat_path, \"r\") as f:\n",
    "    feat_data = json.load(f)\n",
    "    \n",
    "feat_path_csv = feat_path.split(\".\")[0] + \".csv\"\n",
    "df_feat = convertToCSV(feat_data)\n",
    "df_feat.to_csv(feat_path_csv, index=False)\n",
    "print(\"For {} all TA2 papers, count of non-null values in each columns\".format(df_feat.shape[0]))\n",
    "df_feat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For RPP data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_path = \"../danny_submission/feature_extractions_RPP/RPP_feature_extraction_results.json\"\n",
    "rpp_with_labels_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(inp_path, \"r\") as f:\n",
    "    rpp_feat_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpp_csv = convertToCSV(rpp_feat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 24)\n"
     ]
    }
   ],
   "source": [
    "print(rpp_csv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For TA1 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 582 all TA1 papers, count of non-null values in each columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paper_id                  582\n",
       "Number_of_Studies          13\n",
       "Number_of_Models           66\n",
       "Effect_Size_1             122\n",
       "Effect_Size_2              87\n",
       "Effect_Size_3              57\n",
       "Effect_Size_4              39\n",
       "Effect_Size_5              27\n",
       "P_Values_1                417\n",
       "P_Values_2                383\n",
       "P_Values_3                359\n",
       "P_Values_4                327\n",
       "P_Values_5                302\n",
       "Model_Names_1             581\n",
       "Model_Names_2             573\n",
       "Model_Names_3             563\n",
       "Model_Names_4             536\n",
       "Model_Names_5             497\n",
       "Sample_Sizes_1            582\n",
       "Sample_Sizes_2            552\n",
       "Sample_Sizes_3            518\n",
       "Sample_Sizes_4            474\n",
       "Sample_Sizes_5            413\n",
       "sorting_order_based_on    582\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_path = \"TA1_feature_extraction_results.json\"\n",
    "with open(feat_path, \"r\") as f:\n",
    "    feat_data = json.load(f)\n",
    "    \n",
    "feat_path_csv = feat_path.split(\".\")[0] + \".csv\"\n",
    "df_feat = convertToCSV(feat_data)\n",
    "df_feat.to_csv(feat_path_csv, index=False)\n",
    "print(\"For {} all TA1 papers, count of non-null values in each columns\".format(df_feat.shape[0]))\n",
    "df_feat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full paper data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_data = \"../segment_data/TA2_classify_data_final_with_folds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_full = pd.read_json(paper_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_claims_map = {}\n",
    "for idx, cur_row in df_data_full.iterrows():\n",
    "    try:\n",
    "        cur_pid = cur_row[\"DOI_CR\"]\n",
    "        cur_claims = [cur_row[\"coded_claim2\"], cur_row[\"coded_claim3a\"], cur_row[\"coded_claim3b\"], cur_row[\"coded_claim4\"]]\n",
    "        pid_claims_map[cur_pid] = cur_claims\n",
    "    except e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Segment data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_seg_path = \"../segment_data/TA2_classify_data_final_with_imp_claims_only.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(inp_seg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    493\n",
       "1    393\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(886, 5)\n",
      "Index(['paper_id', 'important_segment', 'important_segment_idx', 'label',\n",
      "       'Fold_Id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "pid_seg_idx_map = {}\n",
    "pid_label_map = {}\n",
    "for idx, cr in df.iterrows():\n",
    "    pid_seg_idx_map[cr[\"paper_id\"]] = [cr[\"important_segment_idx\"], cr[\"important_segment\"]]\n",
    "    pid_label_map[cr[\"paper_id\"]] = cr[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
