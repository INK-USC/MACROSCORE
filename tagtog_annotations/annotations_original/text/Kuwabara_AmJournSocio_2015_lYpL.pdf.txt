Kuwabara_AmJournSocio_2015_lYpL.pdf
aCvvgttVNbIzI7ikwFSP5YVBUTUW-Kuwabara_AmJournSocio_2015_lYpL.pdf.plain.html

Do Reputation Systems Undermine Trust? Divergent Effects of Enforcement Type on Generalized Trust and Trustworthiness1 Ko Kuwabara Columbia University Research shows that enforcing cooperation using contracts or tangi- ble sanctions can backfire, undermining people’s intrinsic motivation to cooperate: when the enforcement is removed, people are less trust- ing or trustworthy than when there is no enforcement to begin with. The author examines whether reputation systems have similar conse- quences for generalized trust and trustworthiness. Using a web-based experiment simulating online market transactions ðstudies 1 and 2Þ, he shows that reputation systems can reinforce generalized trust and trust- worthiness, unlike contractual enforcement or relational enforcement based on repeated interactions. In a survey experiment ðstudy 3Þ, he finds that recalling their eBay feedback scores made participants more trusting and trustworthy. These results are predicated on the diffuse nature of reputational enforcement to reinforce perceptions of trust and trustworthiness. These results have implications for understand- ing how different forms of governance affect generalized trust and trust- worthiness. Pierre Omidyar founded eBay on a simple idea: People are basically good. This fundamental belief created a completely new kind of marketplace, forever transforming e-commerce. ðeBay.comÞ The problem of trust is a fundamental fact of social and economic life: from time to time, people engage in exchange of various goods and resources with clients and colleagues, buyers and sellers, teams or organizations without knowing whether, when, and how specific terms of exchange will be honored. 1390 AJS Volume 120 Number 5 (March 2015): 1390–1428 1 This research was partially supported by National Science Foundation grant 0602212. The author thanks Brandy Aven, Batia Wiesenfeld, Stephan Meier, and Sigrid Seutens for generous feedback on earlier drafts. Direct correspondence to Ko Kuwabara, 703 Uris © 2015 by The University of Chicago. All rights reserved. 0002-9602/2015/12005-0003$10.00 To enable successful exchange in such situations, actors often rely on formal governance, such as contracts, hierarchies, or regulations that provide mon- itoring and sanctioning to ensure cooperation between people who might not sufficiently trust each other ðWilliamson 1981Þ. A growing body of re- search has found, however, that many institutional devices for enforcing cooperation can backfire, undermining people’s intrinsic motivation to co- operate ðMolm, Takahashi, and Peterson 2000; Bohnet, Frey, and Huck 2001; Malhotra and Murnighan 2002; Fehr and Rockenbach 2003; Mulder et al. 2006; Bowles 2008Þ. Under strong, conspicuous forms of enforce- ment, such as formal contracts or monetary sanctions, acts of cooperation may be perceived to be motivated by external incentives or constraints rather than intrinsic motives—that is, in order to avoid punishment or at- tain rewards rather than out of one’s own goodwill or moral responsibil- ity. An ironic consequence is that, when the enforcement is removed, peo- ple are less trusting or cooperative than when there is no enforcement to begin with. In the current research, I examine whether reputational enforcement through peer-to-peer feedback might have similar consequences as con- tractual enforcement, making people less trusting or trustworthy toward strangers in the absence of enforcement. There is perhaps no better test of generalized trust and trustworthiness—for example, the belief that “people are basically good”—than online markets ruled by anonymity and pervasive automation, where durable relations are notably absent and virtual strangers come to exchange goods and services halfway across countries, often with- out the prospect of future interactions. To mitigate the risks of anonymous transactions, many online markets maintain reputation systems, such as eBay’s Feedback Forum, that provide platforms for collecting, aggregat- ing, and displaying numerical ratings and reviews of various products and services. Following eBay’s lead, peer-to-peer reputation systems have be- come one of the most ubiquitous institutions on the Internet. Users and scholars alike have marveled at how effectively even the simplest reputa- tion system can curtail fraud and sustain trust in markets that are too large and decentralized for contractual enforcement or centralized policing ðKol- lock 1994; Resnick et al. 2000; Dellarocas 2003a; Bolton, Katok, and Ocken- fels 2004Þ. Yet, such optimism may be questioned if reputation systems are eroding our trust and trustworthiness offline. I also compare reputational enforcement to another important form of governance: relational enforcement through direct reciprocity in repeated interactions ðAxelrod 1984Þ. In the absence of external enforcement, peo- ple naturally repeat interactions with specific others in order to curtail mal- Hall, 2033 Broadway, Columbia Business School, New York, New York 10027. E-mail: kk2558@columbia.edu Do Reputation Systems Undermine Trust? 1391 feasance and reduce uncertainty on their own. However, such relations can lock people in and inhibit exploration that may cultivate generalized trust and trustworthiness toward unfamiliar others. My theoretical argument builds on the idea that generalized trust and trustworthiness toward strangers develop through gradual exposure to strangers under conditions of moderate social risk that test and affirm people’s perceptions about themselves and each other as intrinsically or dis- positionally trusting or trustworthy, even in the absence of strong enforce- ment ðMolm et al. 2000; Macy and Sato 2002Þ. These conditions, I argue, are more likely under reputational enforcement than either contractual or rela- tional enforcement or in the absence of any enforcement. Anonymous ex- changes are too risky without enforcement and too safe under contractual enforcement to cultivate trust and trustworthiness. In comparison, repu- tational enforcement provides social feedback that can reinforce intrinsic motivations ðDeci, Koestner, and Ryan 1999Þ instead of weakening them by eliminating risk entirely. Unlike relational enforcement, reputational enforce- ment also promotes exposure to new partners. Across three sets of web-based experiments, I find compelling support for my central prediction that reputational enforcement—compared to con- tractual or relational enforcement—can reinforce both generalized trust and trustworthiness. These results have direct implications for understanding the consequences of market designs and institutions for user experiences ðDellarocas 2003a; Nissenbaum 2004; Pavlou and Gefen 2004Þ. Although online feedback mechanisms have been studied extensively ðe.g., Kollock 1999; Resnick et al. 2000; Dellarocas 2003a; Bolton, Greiner, and Ocken- fels 2012; Diekmann et al. 2014Þ, how they affect people’s behaviors and decisions outside of particular systems or communities has been largely ne- glected. As online economies continue to grow in scale and form, and as IT- based solutions pervade our lives inside and outside organizations, under- standing how they shape our lives, both online and offline, is an important task for social scientists in this increasingly digital and borderless age ðCook, Snijders et al. 2009Þ. More generally, by comparing different forms of enforcement that pre- vail in markets ðreputationalÞ, hierarchies ðcontractualÞ, and networks ðre- lationalÞ ðPowell 2003Þ, current research contributes to our understanding of how different forms of governance affect generalized trust and trustwor- thiness. Although social control through peer enforcement ðe.g., Durkheim 1893; Homans 1961; Coleman 1990Þ and institutional governance ðWil- liamson 1981; Powell 2003; Cook, Hardin, and Levi 2005Þ has long been a central topic in sociology, far less attention has been paid to how the effects of enforcement might “spill over” to exchange decisions and actions outside of specific relations or institutions. American Journal of Sociology 1392 THEORETICAL BACKGROUND The Problem of Generalized Trust and Trustworthiness How trust develops between strangers has been a topic of considerable interest across social sciences ðe.g., Putnam 1993; Kramer and Tyler 1996; La Porta et al. 1997; Yamagishi 1998; Hardin 2002; Cook, Snijders et al. 2009Þ. In contrast to personalized trust that develops between particular individuals through repeated interactions, generalized trust refers to trust toward strangers based on dispositional tendencies to trust unfamiliar oth- ers ðRotter 1971Þ, a general belief in human benevolence ðKosugi and Ya- magishi 1998Þ, or moral obligation to “trust as if people are trustworthy” ðUslaner 2002Þ. Similarly, generalized trustworthiness refers to intrinsic motivations or dispositional tendencies to refrain from cheating and be- traying strangers on the basis of a moral faith that trustworthiness is part of social obligations toward people ðSimpson and Eriksson 2009Þ. While much light has been cast on the development of trust between particular individuals over repeated interactions ðe.g., Axelrod 1984; Kramer 1999; Molm et al. 2000Þ, scholars continue to debate how such experiences of personalized trust might generalize to trust and trustwor- thiness toward strangers in other contexts. According to the social learning perspective, generalized trust develops as individuals extrapolate from their personal experiences in localized settings—such as schools, neighborhoods, or civic associations—to form certain expectancies about similar others or contexts ðPutnam 2000; Stolle 2001; Hardin 2002; Glanville 2004; Glanville and Paxton 2007Þ. Through repeated interactions with different friends, neighbors, coworkers, or community members, people learn what types of people can be trusted. In this view, generalized trust is grounded in the basic process of social perception in which people actively learn to discern who is or is not trustworthy by making sense of observable traits and sig- nals, learned information, or other telltale signs of character ðFrank 1988; Macy and Skvoretz 1998Þ. The social learning perspective has been challenged on the grounds, however, that localized interactions—whether through civic associations, sociopolitical affiliations, or informal networks of friends and neighbors—are generally too homogeneous and cohesive to cultivate generalized trust to- ward people in more diverse social settings outside of embedded relations ðUslaner 2002, 2012Þ. Localized interactions may be safe grounds for per- sonalized trust, but it is theoretically unclear how they might cultivate beliefs that “most people” are trustworthy. What, for instance, do people learn from one book club, about members of another book club or a bowl- ing team or a new neighborhood? In an extensive review of studies using survey data to examine generalized trust, Uslaner ð2002Þ in fact finds no Do Reputation Systems Undermine Trust? 1393 compelling evidence that participation in civic, fraternal, religious, political, or recreational associations increases generalized trust.2 It is also not clear how the logic of social learning might account for gen- eralized trustworthiness, that is, why some people resist taking advantage of others. Learning to discern who is trusting or not is a moot issue, and it may in fact encourage dishonesty, because the sequential nature of trust exchange ensures that the truster ðe.g., buyerÞ must commit to the ex- change first. Rather, the critical issue for the trusted party is learning to be trustworthy and to refrain from cheating regardless of who the truster is; in this regard, social perception is agnostic about who refrains from ex- ploiting others and why. An alternative hypothesis links generalized trust and trustworthiness to reinforcement learning ðMacy and Sato 2002Þ rather than social learning— that is, to the reinforcement of self-perception rather than social perception about others. The logic of self-perception is simple: people form attitudes and beliefs by observing their own actions and inferring what caused them, just as they form impressions or opinions about others by observing their actions ðBem 1967Þ. Thus, rather than learning to discern who is or is not trustworthy, people come to view themselves as a trusting or trustworthy person by observing their own acts and learning to value trust and trust- worthiness as a moral or personal virtue ðUslaner 2002Þ. In this view, trust is motivated not by the expected utility of trusting someone who may turn out to be trustworthy but by a sense of self that identifies with acts of trust, that is, by the conviction that one is a trusting person. Self-perceptions of trust do not preclude strategic decisions or cost-benefit calculus based on social perceptions about others, but they can precede or anchor such deci- sions. It is not unlike the concept of dispositional trust, based on stable and innate personality attributes ðRotter 1971; Smith 2010Þ, but self-perception emphasizes situational changes in beliefs rather than stable traits. For in- stance, a recent study byGrand andDutton ð2012Þ demonstrates that simply reflecting on being a giver ðself-perceptionÞ makes people more giving than reflecting on receiving benefits from others ðsocial perceptionÞ in social ex- change. Moreover, and in contrast to perceptions about other people’s trust- worthiness, perceptions about one’s own trustworthiness can directly pro- mote acts of trustworthiness by invoking a self-image of oneself as someone who resists opportunism, even in situations in which dishonesty yields a greater immediate payoff. 2 It should be noted that Uslaner’s ð2002Þ results are still far from conclusive. Reviewing the literature, Paxton ð2007Þ notes that empirical support for the relationship between trust and civic association remains rather mixed and calls for greater attention to different forms of associations. American Journal of Sociology 1394 Self-perceptions of trust and trustworthiness are often traced to social- ization through early parenting, religious upbringing, or moral education ðRotter 1971; Luhmann 1979; Fukuyama 1995Þ. Uslaner ð2002Þ argues that this form of trust—trusting as if people are trustworthy—is a moral value, linked closely to one’s identity and general faith in the goodwill of the other. Exchange theory suggests, however, that perceptions of trust and trust- worthiness, including self-perceptions, may also be reinforced by success- ful exchanges that yield material payoffs ðMacy and Sato 2002Þ, emotional “buzz” ðLawler 2001Þ, or social approval ðWiller 2009Þ under conditions that lead actors to attribute cooperative acts to themselves and each other rather than external enforcement. Such conditions, I argue below, are more per- vasive under reputational systems than under contracts. Contractual Enforcement Between strangers, many transactions occur only under formal contracts that reduce the risk of exploitation through binding agreements that impose relatively direct and tangible sanctions in case of violations or negligence. Ironically, contractual enforcement—and the threat of tangible sanctions in general—canbackfire, reducing cooperation once enforcement is removed to a greater extent than when there are no contracts or sanctions to begin with ðMalhotra and Murnighan 2002; Fehr and Rockenbach 2003; Mulder et al. 2006Þ. One reason is that formal enforcement may interfere with people’s social perception through which they come to attribute each other’s coop- eration to their intrinsic motives or dispositions to refrain from opportun- ism. Although the goal of enforcement is to reduce risks of exploitation, the development of trust depends critically on risk, because only in the presence of potential exploitation can acts of cooperation be reliably or meaningfully attributed to each other’s goodwill and benign intentions. Under binding agreements, cooperation may be perceived to be coerced or externally moti- vated because people cannot adequately demonstrate their willingness to place or honor trust. As a consequence, once enforcement is removed, peo- ple realize that they actually have not learned to trust each other or to iden- tify who is trustworthy or not, thus withholding cooperation ðMalhotra and Murnighan 2002; Simpson and Eriksson 2009Þ. In a market with a hetero- geneous population of honest and dishonest sellers, enforcement can also compromise social perceptions of trust by reducing the need to safeguard against the risk of exploitation, thus “crowding in” dishonest sellers who, in the absence of enforcement, may be driven out of the market by wary buyers ðAkerlof 1970; Bohnet et al. 2001Þ. The threat of enforcement might keep dishonest types in check, but once enforcement is removed, the market be- gins to collapse unless people quickly learn to identify honest and dishonest types rather than withdrawing from the market altogether. Do Reputation Systems Undermine Trust? 1395 Another reason why formal enforcement might sustain cooperation but reduce trust is that it can interfere with people’s self-perceptions of their own trust and trustworthiness. By this account, trusters might attribute their own decisions to place and honor trust to the threat of enforcement and come to perceive themselves as less trusting or trustworthy than they actually are ðKramer 1999; Simpson and Eriksson 2009Þ. By ðmisÞattrib- uting their own actions to extrinsic incentives, people lose their intrin- sic motivation to act prosocially ðGneezy and Rustichini 2000; Gneezy, Meier, and Rey-Biel 2011Þ, much like students losing interest in the very subjects in which they care excelling if they receive financial incentives for good grades ðLepper, Greene, andNisbett 1973; Deci et al. 1999Þ. Reputational Enforcement In online markets that are too large and decentralized for formal enforce- ment by centralized agents, reputation systems have proven to be a highly effective solution to the problem of trust. Peer-to-peer transactions are sig- nificantly riskier online than in face-to-face exchanges, because people must take a leap of faith in faceless strangers halfway across the Internet, sepa- rated in both time and space. Yet, remarkably, eBay has become the largest peer-to-peer market in the history of mankind without offering so much as a warranty or central policing, only a simple reputation system that allows buyers and sellers to rate each other ð11 or −1Þ and leave comments ðe.g., “Nice person to do business with! Highly recommended”Þ after transactions. EBay posts users’ feedback points next to their screen names to help mitigate the risk of online transactions and attract future buyers and sellers. Each user can receive only one feedback point from a given partner, and they are not retractable once posted. A substantial body of research attests to the effectiveness of even the simplest reputation systems. Research on eBay finds robust effects of both positive and negative feedback points on likelihood of sales and final auction prices ðe.g., Bajari and Hortaçsu 2003; Resnick et al. 2006Þ, while laboratory studies show that displaying no more than binary feedback ða positive or negative pointÞ from just one previous interaction suffices to sustain high levels of cooperation between strangers engaged in anony- mous, one-shot exchanges ðe.g., Keser 2003; Bolton, Katok, and Ockenfels 2005Þ. Following eBay’s lead, online reputation systems have become a ubiquitous feature of online markets and communities ðKollock 1999; Res- nick et al. 2000Þ. Thus far, research on online reputation systems has focused almost ex- clusively on dyadic trust between particular buyers and sellers ðe.g., Resnick et al. 2006; Dellarocas 2010Þ, although few studies have gone beyond this scope to examine the role of reputation systems in promoting generalized American Journal of Sociology 1396 trust toward the general community of sellers in a particular market ðPavlou and Gefen 2004Þ. The scope of my interest is even broader: how people’s exposure to reputational enforcement affects their generalized trust and trustworthiness outside of particular reputation systems or markets. This means my focus is not on the effects of particular reputations associated with particular individuals or communities but on the effects of reputa- tion systems. Do reputation systems make people more or less intrinsically trusting and trustworthy—that is, even in the absence of feedback mech- anisms? While contracts and reputation systems present alternative forms of en- forcement, one key difference is that reputational enforcement is generally more diffuse than contractual enforcement. In contrast to contractual enforce- ment, reputational sanctioning—posting and sharing feedback—is much less direct and tangible. Providing feedback has no directmaterial consequences for the recipient, only indirect and often marginal consequences insofar as transactions are enforced by other community members who collectively mon- itor and aggregate reputational information about each other, and victims of malfeasance receive no material reparations from submitting feedback. Thus, the effectiveness of reputational enforcement depends on whether others in the market will actually observe the seller’s reputation and act appropriately ðe.g., avoid or penalize sellers with negative reputationsÞ in future transactions ðGreif et al. 1998; Dellarocas 2003bÞ. This diffuse nature of reputational enforcement has important conse- quences for generalized trust. Although trust grows under risk ðMolm et al. 2000Þ, unenforced transactions present too much risk while contractual enforcement creates too little risk to cultivate perceptions of trust and trust- worthiness. Under such conditions, people are unlikely to make strong in- ferences about each other’s trustworthiness or their own willingness to trust, because they may be better off avoiding unenforced transactions alto- gether, or, given contracts, they might enter exchanges with strangers too readily without proper due diligence and discretion. By comparison, rep- utation systems create conditions of relatively moderate risk, enough to spur measured cooperation without placing blind bets. Under such conditions, market transactions can expose people gradually to exchange opportuni- ties with strangers in ways that test and affirm their own sense of trust and integrity. Although simulation studies have explored this logic ðMacy and Sato 2002Þ, the current research, to my knowledge, is the first to em- pirically test idea. Another difference between contracts and reputations is that social sig- naling plays a much more prominent role in reputational enforcement. Sig- nals are observable actions or markers that identify people with certain unobservable traits and qualities, such as a college degree that reflects one’s intelligence ðSpence 1974Þ. Contracts are primarily designed to enforce co- Do Reputation Systems Undermine Trust? 1397 operation by providing negative sanctions for noncompliance regardless of unobservable traits or intentions, and they do more to mask than to reveal signals of trust or trustworthiness: under contracts, it is difficult ðand un- necessaryÞ to tell who is actually trustworthy or not. If contracts are op- tional, the very act of proposing contracts can signal distrust rather than trust and dampen the partner’s motivation to honor or reciprocate trust ðFehr and Falk 2002; Puranam and Vanneste 2009Þ. In comparison, the primary function of many reputation systems is to create positive signals that not only help identify who is trustworthy but also frame transactions in terms of finding people to trust rather than constraining their behavior ðKollock 1999Þ.3 Although negative feedback is used from time to time, online feedback is overwhelmingly positive ðResnick et al. 2006; Bolton et al. 2012Þ. Thus, people in reputation systems evaluate one another and enter transactions primarily on the basis of positive reputations that elicit perceptions of quality, trustworthiness, and collective social approval rather than negative sanctions that presume strangers should not be trusted. And yet, because reputational enforcement is diffuse, reputation systems still re- quire buyers to place a certain amount of trust in each seller, even if they can observe seller reputations ex ante. In this sense, contracts are designed to replace or substitute for trust ðCook et al. 2005Þ, whereas reputation systems are designed to invoke and complement trust, reinforcing people’s perception that they are willing to trust strangers in the absence of strong enforcement. The informational nature of social feedback in reputational enforce- ment is critical for trustworthiness as well. Overjustification theory ðDeci et al. 1999, 2001Þ suggests that tangible incentives ðe.g., monetary rewards or finesÞ can diminish intrinsic motivations, but intangible incentives ðe.g., verbal feedback such as praiseÞ can enhance them. Tangible rewards are generally perceived to be more controlling or coercive, particularly when they are expected, inhibiting attributions to intrinsic motivation. By this token, contracts that impose sanctions for specific terms of exchange are likely to be regarded as more controlling, undermining prosocial motives ðMalhotra andMurnighan 2002; Simpson and Eriksson 2009Þ. Studies show that even weak incentives based on small monetary fines or rewards can erode intrinsic motivations ðTenbrunsel and Messick 1999; Gneezy and Rustichini 2000Þ. In contrast, intangible rewards provide informational feedback that af- firms one’s intrinsic motivation and sense of competence or worth without controlling people’s behavior. For example, reputational enforcement is based on feedback points that aggregate over time into a reputation, often 3I thank an AJS reviewer for suggesting this point. American Journal of Sociology 1398 earning special status ðsuch as starsÞ, reinforcing the person’s moral values or identity, much like a student who becomes an “A student” instead of merely acing tests.4 Furthermore, it is noteworthy that reputational enforcement is often unpredictable, since posting feedback is largely voluntary in many reputation systems. It has been known since Skinner’s ð1969Þ work on operant conditioning that behavioral reinforcement is significantly more effective on variable rather than regular schedules, that is, when feedback is somewhat unpredictable or unexpected. Hence, unlike contractual enforcement that might interfere with both social and self-perceptions of trust and trustworthiness, the combination of diffuse sanctions and social feedback in reputation systems can create microconditions that facilitate the development of both generalized trust and trustworthiness. From these ideas, I expect reputational enforcement to produce higher generalized trust ðhypothesis 1Þ and trustworthiness ðhypoth- esis 2Þ than contractual enforcement. Relational Enforcement In the absence of contractual or reputational enforcement, actors often en- gage in repeated interactions with familiar partners, which enables relational enforcement through reciprocity ðGouldner 1960; Axelrod 1984; Granovetter 1985Þ. Through positive reciprocity, actors reward each other by offering continued cooperation; through negative reciprocity, actors punish each other by reducing cooperation or terminating relationships. Over time, exchange partners develop mutual knowledge, identification, and commit- ment that safeguard against the risks and uncertainties of one-shot, un- enforced exchanges. In game theory, relational enforcement, or relational contracting, is often modeled as repeated games with private reputation gleaned from direct interactions rather than public or third-party reputation aggregated from others’ experiences ðWilson 1997Þ. In this sense, relational enforcement is a form of reputational enforcement. However, the relational bonds that de- velop between people through repeated interactions, including trust, are re- lation specific and limited to particular partners. Indeed, studies have dem- onstrated negative effects of particularized trust on generalized trust: strong ties to familiar others can reduce trust toward outsiders ðHayashi and Ya- magishi 1998; Yamagishi, Cook, and Watabe 1998; Stolle 2001Þ. Conversely, 4Kollock ð1999Þ has made a similar observation about the role of positive vs. negative reputations ðalso see Cook and Hardin 2001; Yamagishi et al. 2009Þ. Whereas negative reputations can create incentives to change or relinquish one’s identity ðe.g., by relocating to a new market or rebranding oneselfÞ, positive reputations play an important role in affirming and stabilizing one’s existing identity. Do Reputation Systems Undermine Trust? 1399 if generalized trust is reinforced through gradual exposure to strangers, we should expect reputational enforcement to produce greater generalized trust than relational enforcement in fixed dyads ðhypothesis 3Þ. Likewise, if gen- eralized trustworthiness is reinforced by social feedback from different part- ners, then, reputation systems that are specifically designed to aggregate feedback from different partners should produce more generalized trustwor- thiness than relational enforcement ðhypothesis 4Þ. OVERVIEW OF THE STUDIES To test the four hypotheses, I conducted three sets of web-based experi- ments designed to simulate particular aspects of reputational enforcement. In all three studies, transactions were modeled after the investment game ðBerg, Dickhaut, and McCabe 1995Þ, in which one actor is assigned to the role of the truster and the other the trustee. In each round, the truster is given an endowment ðe.g., 10 pointsÞ and asked to send or entrust any portion of it to the trustee. The trustee receives a multiple ðtypically 3×Þ of the entrust- ment and is asked whether to share any portion of it with the truster. This task has been used extensively to study generalized trust ðe.g., Glaeser et al. 2000Þ and trustworthiness ðe.g., Simpson and Eriksson 2009Þ. Using this paradigm, study 1A tested hypotheses 1 and 3 for generalized trust, and study 1B tested hypotheses 2 and 4 for generalized trustworthiness, af- ter exposing participants to different types of enforcement over a series of anonymous exchanges. Study 2 modified study 1 to rule out alternative explanations ðe.g., contrast effectÞ and further explore how enforcement affects generalized trust and trustworthiness using self-reported measures. Finally, in order to probe the theoretical mechanism more directly and provide external validity, study 3 used actual eBay users to show that their feedback scores from eBay can reinforce their self-perceptions of trust and trustworthiness. No one study was designed to directly test between social and self- perception since they can co-occur in many situations, and both explana- tions converge closely on the same predictions. Study 3 manipulates, but does not measure, self-perception. In the discussion section below, I con- sider several reasons why self-perception still offers a more compelling explanation than social perception for my results across the three studies and for exchange theory more generally. Finally, it is important to distinguish the predicted effects of reputa- tional enforcement that reinforce perceptions of trust and trustworthiness from dependence on reputational information that reduces people’s will- ingness to engage in transactions outside of reputation systems. For many people, it is hard to imagine going to a new restaurant, watching a new movie, or shopping online without consulting consumer reviews first. It is American Journal of Sociology 1400 no surprise that such people would be more likely to avoid new services or businesses when reputation systems are unavailable. What has not been tested thus far, however, is whether they are still more trusting or trust- worthy as a result of their experiences in reputation systems than they would otherwise be without prior exposure to reputational enforcement. STUDY 1A: DO REPUTATION SYSTEMS UNDERMINE TRUST? Study 1A was designed to test hypotheses 1 and 3 for trust using invest- ment games to simulate transactions in an online market for “clients” hiring “agents” for “professional services” under different types of enforcement. Fol- lowing the standard paradigm for studying the spillover effects of enforce- ment ðBohnet et al. 2001; Malhotra and Murnighan 2002; Simpson and Eriksson 2009Þ, the experimental treatments first exposed participants to a series of transactions with anonymous partners under contractual, rela- tional, or reputational enforcement before presenting several additional rounds of one-shot exchange without any enforcement. This pretest-posttest design helps evaluate how participants change after exposure to different types of enforcement. To isolate the effects of enforcement, partners in both studies were simulated and programmed to offer full cooperation. The transactions were framed as online transactions for high- or low-quality ser- vices.5 Experimental control was crucial for testing my hypotheses under four conditions that are difficult to achieve in natural settings: ð1Þ creating com- parable conditions for contractual, relational, and reputational enforce- ment, ð2Þ removing enforcement at a specific point while holding other market conditions constant, ð3Þ measuring the effects of enforcement on trust and trustworthiness separately, and ð4Þ ensuring high levels of coop- eration by partners. The third condition is critical because my substantive goal is to understand how different types of enforcement affect individual buyers and sellers rather than a community of buyers and sellers as a whole. The fourth condition specifies an important scope condition for my predic- tions: the enforcement system must be ðperceived asÞ reasonably effective if it is to invoke perceptions of trust and trustworthiness ðPavlou and Gefen 5Although this is a departure from markets for goods like eBay, services and products are treated as interchangeable for the purpose of this study insofar as they both present moral hazard. I used services in order to facilitate attributions to people rather than products. Cook, Cheshire et al. ð2009Þ note that the basis of evaluation for transactions for service vs. goods in online markets may be different. Buyers care more about the motivation than the competence of the seller to provide high-quality service, but they care more about the competence than the motivation of seller to deliver high-quality goods. Such issues are absent in the current study. Do Reputation Systems Undermine Trust? 1401 2004; Cheshire 2011Þ. Anecdotal and empirical evidence shows that many online markets with reputation systems, including eBay, are indeed over- whelmingly safe despite enduring concerns about fraud and deception ðKollock 1999; Resnick et al. 2000; Diekmann et al. 2009Þ. Design and Procedures One hundred and one students ð58 male, 20.5 years oldÞ completed the study. They were recruited using a standard protocol for behavioral ex- periments, via mailing lists and flyers posted around campus, to partici- pate in an “online market study” that promised $14–$20 in cash, based on performance, paid upon full completion of the study “for just a few minutes of your time each day for a total of less than 60 minutes over the course of five to seven days, played from any computer with Internet access.” After signing up for the study, participants were directed by e-mail to a custom web page that guided them through the consent form, experi- mental instructions, and practice trials before they were “randomly” as- signed to play the client role in a series of online transactions with anony- mous exchange partners playing the agent role. In each transaction, the client was given an endowment of 10 points for hiring an agent and decided how many hours of service, 0 ≤ s ≤ 10, to hire the agent for at 1 point per hour. In turn, the agent decided whether to provide high- or low-quality ser- vice for the total hours hired. High-quality service earned the client 3s points and the agent s points, while low-quality service earned the client .5s points and the agent 2s points in each transaction. Any remaining amount 10 − s was added to the client’s total earnings. Participants were told that the full payoffs were public knowledge to both clients and agents. Thus, the dilemma for the client was deciding how many hours to hire the agent for without knowing the quality of service that would be delivered. The client made a profit only if the agent provided high-quality service. Each participant was assigned randomly to one of four experimental conditions, or “markets.” In the unenforced condition ðN 5 30Þ, the mar- ket consisted of a series of one-shot transactions without contractual, repu- tational, or relational enforcement. In the contract condition ðN 5 23Þ, each transaction was automatically placed under a contract that imposed, at no cost to the client, a penalty of 10 points on the agent with a 90% probability for providing low-quality service. In the fixed-partner condition ðN 5 24Þ, participants repeated exchanges with the same agent. In the reputation con- dition ðN 5 24Þ, each agent was given a numerical reputation score, and clients were given an opportunity to rate each transaction with binary feed- back ð1 or −Þ. Participants were given information only about the spe- cific market to which they were assigned. American Journal of Sociology 1402 Enforcement can be said to undermine trust if removing it results in lower levels of trust compared to the unenforced ðbaselineÞ condition. Thus, each condition consisted of two phases: participants were told that the experiment would last up to 15 rounds, but after completing eight trans- actions in their respective markets ðphase 1Þ, they were asked to complete additional transactions in the unenforced condition ðphase 2Þ. The depen- dent measure of trust was hours of service purchased in phase 2. In previous studies ðe.g., Bohnet et al. 2001Þ, enforcement was simply removed without pretext. However, it is unlikely that reputation systems are suddenly removed from real online markets. A more plausible scenario is that people ðare forced toÞ pursue alternative transactions outside of rep- utation systems for better prices or customers; for instance, one might resort to Craigslist if a particular item is not available on eBay. Thus, par- ticipants were told that, because “no new agents are currently available in your assigned market,” they need be transferred to a different market, namely, the unenforced condition with a different set of participants. Apart from ensuring greater realism, this also minimizes the “history” effect—the possibility that participants might simply continue to behave as if they were still in the same market with the same population of partners as in phase 1 before the removal of the enforcement. Participants were not told that they would be transferred to the unenforced market until round 9 or that they would complete exactly three more transactions in phase 2. Ta- ble 1 summarizes the experimental design. In order to isolate the effects of enforcement on trust from partner effects, all participants except those in the fixed-partner condition faced the same sequence of agents ðtable 2Þ, all preprogrammed to deliver high- quality service in every round. In the reputation condition, all agents were associated with largely positive reputation scores in order to ensure that the reputation system was perceived to be reasonably effective. However, TABLE 1 Summary of the Experimental Design in Study 1A Condition Phase 1 Phase 2 Unenforced . . . . Unenforced transactions with random partners Unenforced transactions with random partners Contracts . . . . . Random partners under contractual enforcement Unenforced transactions with random partners Fixed partner . . . Fixed partner under relational enforcement Unenforced transactions with random partners Reputation . . . . Random partner under reputational enforcement Unenforced transactions with random partners NOTE.—Phase 1 is rounds 1–8; phase 2 is rounds 9–11. Do Reputation Systems Undermine Trust? 1403 agents in rounds 4 and 6 were given a negative point for realism and to see whether participants are paying attention to the reputation scores. Contracts and reputation systems exist in various forms. For instance, as Durkheim ð1893Þ noted, some contracts are repressive or retributive ðpenalizing offendersÞ while others are restitutive ðcompensating victimsÞ. For the purpose of the current research, I focused on retributive enforce- ment in order to ensure that the enforcement mechanisms were as com- parable as possible across conditions except in their defining differences. For instance, although both voluntary and involuntary contracts have been shown to undermine cooperation ðMalhotra and Murnighan 2002Þ, the con- tracts in the current studies were involuntary and free, provided in every transaction at no cost to the client. This ensures comparability to many rep- utation systems that are also involuntary and free ði.e., every transaction could be rated at no real costÞ. The critical difference is that enforcement resulted in the agent incurring direct financial penalties under contractual terms, whereas reputational enforcement imposed no such penalties that reduced participant earnings directly.6 Because agents were programmed to always fully cooperate, however, no contractual penalties were actually imposed on agents, thus holding constant direct material consequences of en- forcement and varying only their perceived effects. TABLE 2 Agents in Study 1A Round Agent ID Transactions Completed* Positive Feedback Points ð%Þ* 1 . . . 3E 17 100 2 . . . 8A 24 100 3 . . . 12C 20 100 4 . . . 7B 21 95.2 5 . . . 5C 18 100 6 . . . 14A 23 91.3 7 . . . 9B 18 100 8 . . . 4D 26 100 9 . . . 2C NA NA 10 . . . 5B NA NA 11 . . . 10A NA NA NOTE.—Agent ID did not change in the fixed-partner condition in rounds 1–8. * Information was presented in the reputation condition only. 6This can create differences in final earnings that may affect participants’ trust. However, because partners were programmed to only cooperate, no such differences actually emerged. American Journal of Sociology 1404 Setting Participants completed the entire experiment online by logging into a cus- tom website. Sometime after each transaction, the server sent e-mail to the participant notifying that the result of the most recent transaction was now available. The results screen showed howmany hours of service and at what level of quality the agent provided ðalways highÞ, the client’s total earning from the transaction and the client’s net cumulative earnings. At this point, clients in the contract condition were told that the agent satisfied the terms of the contract ðthus, the agent could not be punishedÞ, while clients in the reputation condition were asked to submit a rating. Those in the other con- ditions received no such message. Next, the screen matched the client with a new agent for the next transaction ðor the same agent in the fixed-partner conditionÞ. Participants were given 10 hours to complete each transaction and five days to complete all 11 transactions.7 The experiment was conducted online to create greater psychological realism, including perceptions of risk that stem from asynchronous, online interactions. The trade-off is loss of experimental control. Outside of the lab- oratory, participants are more likely to talk to each other, get distracted, or drop out. So long as noise is random across the conditions, however, such issues are unlikely to bias the results. For precaution, participants were in- structed not to discuss the study with others until the end of the semester. During debriefing, they were asked whether they discussed the study with anyone. No such case was reported. Results and Discussion Figure1 shows the mean level of trust ðthe number of hours commissionedÞ in each round by condition. In phase 1 ðrounds 1–8Þ trust was substantially higher in all of the treatment conditions compared to the baseline level in the unenforced condition, which hovered between 30% and 40%. Not sur- prisingly, the contract condition produced nearly full cooperation in all rounds. In contrast, trust rose more slowly in both the fixed-partner and the reputation conditions. Trust in the reputation condition dips markedly in response to the agent with a negative reputation in round 6 but recovers immediately in the next round. These observations were submitted to Tobit 7If the client did not initiate the next transaction within 10 hours, the transaction expired, and the server matched the client with the next agent. Up to two e-mail reminders were sent out to make sure transactions did not expire. Among participants who completed the entire study, only 0.8% of all transactions expired, 14 participants let at least one transaction expire, four participants let two transactions expire, and no one let more than two trans- actions expire. In statistical analyses, expired rounds were denoted as 0 points in trust and dummied as expired. Do Reputation Systems Undermine Trust? 1405 regression to account for the dependent variable censored at 10 maximum hours, with standard errors clustered for repeated measures.8 The results indicate that, in phase 1, the contract condition produced more trust than the fixed-partner condition ðb 5 4.41, P 5 .002Þ, which produced more trust than the reputation condition ðb 5 .75, P 5 .001Þ, which produced more trust than the unenforced condition ðb 5 .97, P < .001Þ. To test hypotheses 1 ðreputation > contractsÞ and 3 ðreputation > fixed re- lationsÞ, I examined trust in phase 2. As shown in figure 1, the trust levels dropped precipitously after round 8 in all treatment conditions. The con- tract condition resulted in both the biggest drop and subsequently the lowest level of trust of all, significantly below the unenforced condition ðb 5 22.16, P5 .006Þ, consistent with previous research on the detri- mental effects of contracts. The fixed-partner condition showed no differ- ence from the unenforced condition ðb 5 2.22, P 5 .59Þ, but it was signif- icantly higher than the contracts condition ðb5 2.30, P5 .01Þ. In contrast, trust in the reputation condition remained roughly 1.6 service hours above the unenforced condition ðb 5 .43, P < .001Þ and the fixed-partner con- FIG. 1.—Levels of trust ðhours of service purchasedÞ by condition in study 1A 8All tests reported are two-tailed. Race, gender, age, and major had no significant effects and are not reported. American Journal of Sociology 1406 dition ðb 5 .54, P 5 .01Þ. Altogether, these results provide strong support for both hypothesis 1 and hypothesis 3. It may be argued that rounds 9–11 in the treatment conditions are better compared to the first three rounds of the unenforced condition than the last three, when participants in the treatment conditions are first exposed to the unenforced condition. Redoing the comparisons accordingly shows ro- bust support for hypotheses 1 and 3: rounds 9–11 in the reputation con- dition > rounds 1–3 in the unenforced condition ðb 5 .28, P 5 .002Þ; rounds 1–3 in the unenforced condition 5 rounds 9–11 in the fixed-partner condition ðb 5 2.15, P 5 .67Þ; and rounds 1–3 in the unenforced condi- tion > rounds 9–11 in the contracts condition ðb 5 2.55, P < .001Þ. The analyses were also repeated using all 11 rounds of the unenforced condition against the last three rounds of the treatment conditions, producing stronger differences. STUDY 1B: DO REPUTATION SYSTEMS UNDERMINE TRUSTWORTHINESS? Design and Procedures To test hypotheses 2 and 4 for trustworthiness, study 1B modified the ex- perimental design above by assigning participants to the role of agents against ðsimulatedÞ clients who decided whether to hire them for 10 hours of service. If a client chose to hire the agent, the agent decided how many hours of high- ð1 ≤ h ≤ 10Þ or low-quality service ð10 2 hÞ to provide. High- quality service earned the agent h points and the client 2h points, while low-quality service earned the agent 3ð10 2 hÞ points and the client h points per transaction. As in study 1A, three enforcement conditions were created in addition to the unenforced condition ðN 5 21Þ, with minor differences. In the con- tracts condition ðN 5 18Þ, high-quality service was enforced with 90% probability, which cost the agent 4 points for each hour of low-quality service delivered. In the fixed-partner condition ðN 5 23Þ, the client was programmed to withhold cooperation ðcommissionÞ in the next round with the inverse probability of the number of high-quality service hours pro- vided, 1 2 ðh/10Þ. For instance, after receiving seven hours of high-quality service ðthus three hours of low-quality serviceÞ, the client chose zero hours in the next round with 30% probability. In the reputation condition ðN 5 21Þ, the agent’s reputation score was aggregated from all rounds as a per- centage of positive points. Each agent began with a null reputation score and received a positive feedback point with ðunbeknownst to partici- pantsÞ 10% probability for each hour of high-quality service rendered and a negative feedback point otherwise. For example, providing six hours of Do Reputation Systems Undermine Trust? 1407 high-quality service earned a positive feedback point with 60% probability. In addition, clients were programmed to reject the agent, on the basis of the most recent feedback point received. Specifically, the agent was rejected and rematched with another client with 65% probability after receiving a negative rating and 5% after a positive rating. Agents were notified by e-mail every time they were rejected, and they forfeited the exchange round entirely after three rejections. Although rejections did not cost the agents, and less than 2% of rounds were forfeited, they served to reinforce the idea that clients trusted agents on the basis of reputation scores. The same pretest-posttest design as study 1A was used to compare trust- worthiness across conditions. The instructions specified that the experi- ment would last up to 30 transactions, but each agent actually completed 20 transactions before phase 2, in which they played an additional three transactions without enforcement. In phase 2, agents were hired in every round; no one was rejected. More transaction rounds were provided in study 1B to ensure enough rounds to accrue comparable numbers of feed- back points as in study 1A. Except those in the fixed-partner condition, participants were told that they would not be matched with the same client more than once. Eighty-three participants ð39 men, 20.04 years oldÞ completed the study. They were recruited for cash payment as in study 1A. After signing up, participants were led to a web page that guided them through consent, instructions, and practice trials before they were assigned to the agent role. All transactions took place online over a period of up to 10 days. At the beginning of each transaction, the server contacted the participant via e-mail that a particular client has chosen him or her for 10 hours of service. The next screen asked how many hours of high-quality service to deliver, cal- culated the agent’s net earnings from the transaction, and completed the transaction by ostensibly releasing an e-mail message to the client inform- ing the agent’s decision. To ensure timely decisions, e-mail reminders were sent out to participants to respond to their clients, as in study 1A. In study 1B, participants were told that letting a transaction expire results in providing zero hours of high-quality service and were penalized with a negative rating or contract enforcement. Among those who completed the study, less than 3% of all transactions expired. Results and Discussion Figure 2 plots the number of high-quality service hours provided in each round. As expected, the treatment conditions sustained significantly higher levels of effort than the unenforced condition during phase 1 ðrounds 1–20Þ, and the contract condition produced almost full cooperation from the be- ginning. Tobit regression correcting for repeated measures shows that the American Journal of Sociology 1408 fixed-partner condition was slightly but consistently less effective than the contracts ðb 5 22.65, P < .001Þ in sustaining trustworthiness, and the rep- utation system even less so, relative to the fixed-partner condition ðb 5 21.743, P 5 .025Þ. In phase 2 ðrounds 21–23Þ, participants were removed from their treat- ment conditions and matched with new partners in the unenforced con- dition, resulting in trustworthiness dropping sharply in all treatment con- ditions. As expected, the biggest drop occurred in the contract condition, falling marginally below the baseline level in the unenforced condition ðb 5 21.76, P 5 .078Þ. The fixed-partner condition dropped to the baseline level ðb 5 .055, P 5 .82Þ but still significantly above the contract condition ðb 5 .95, P 5 .02Þ. The reputation system fell the least, remaining signif- icantly above the baseline level ðb 5 .57, P 5 .03Þ, the fixed partner con- dition ðb 5 1.19, P 5 .01Þ, and the contracts condition ðb 5 2.19, P < .01Þ. These results converge with study 1A to support the prediction that rep- utation systems can produce greater generalized trustworthiness than con- tractual enforcement ðhypothesis 2Þ as well as relational enforcement ðhy- pothesis 4Þ. Once again, a reasonable concern is whether the unenforced condition in rounds 21–23 provides valid comparisons to the treatment conditions FIG. 2.—Levels of trustworthiness in study 1B ðhours of high-quality service de- liveredÞ. Do Reputation Systems Undermine Trust? 1409 in phase 2. Using the first three rounds of the unenforced condition in- stead reveals no statistically significant difference between the reputation and unenforced conditions ðb 5 2.52, P 5 .12Þ. However, a comparison using all 23 rounds of the unenforced condition reveals a significant pos- itive effect of the reputation condition ðb 5 .62, P 5 .048Þ. Thus, a conser- vative conclusion is that reputational enforcement may not necessarily pro- duce higher levels of trustworthiness than unenforced exchanges, but it can sustain trustworthiness from falling as much as it does when contrac- tual or relational enforcement is removed. STUDIES 2A AND 2B: RULING OUT CONTRAST EFFECTS In study 1A, participants in phase 2 withdrew trust in inverse proportion to the level of cooperation they displayed in phase 1. Although this pattern is consistent with the idea that risk-taking is necessary to cultivate gen- eralized trust, it could be due to the effect of contrast in perceived risk from phase 1 to 2—the less risk clients felt in phase 1, the more risk they felt in phase 2 in contrast—rather than the direct effect of risk-taking in phase 1. Similarly, the patterns in study 1B could be due to the contrast between the levels of enforcement agents felt in phase 1 versus 2 rather than the dif- ferences in the nature of enforcement in phase 1. Study 2 was designed to address these concerns as well as to sample from a different, more urban university, five years after study 1, to demonstrate the robustness of my original findings. Design and Procedures The experimental design and procedures were identical to studies 1A and 1B, with two modifications. First, contractual enforcement was reduced from a 90% to a 50% enforcement rate, which reduced trust and trust- worthiness to the level of reputational enforcement in a pilot study. If the patterns in study 1 are due entirely to contrast effects in perceived risk, we should observe no difference in phase 2 between contracts at 50% en- forcement and the reputation system.9 Second, I administered a postexperi- mental survey to further evaluate how enforcement affected participants’ generalized trust and trustworthiness. One to two days after completing the online transactions, participants playing the agent’s role in study 2A ðN 5 62, 34 men, 20.8 years oldÞ responded to a six-item generalized social trust scale ðYamagishi andYamagishi 1994Þ. The questions asked howmuch participants agree that “most people are basically honest ½or trustworthy, 9To ensure comparable levels of risk in phase 1 across conditions, the negative reputation score in round 6 ðsee table 2Þwas removed. American Journal of Sociology 1410 good and kind, trustful of others,” “I am trustful,” and “most people will respond in kind when they are trusted by others.” Responses were mea- suredon seven-point scales and aggregated into a single index ðCronbach’s a 5 .93Þ. In study 2B ðN 5 58, 32 men, 20.2 years oldÞ, participants playing the client’s role responded to questions adopted from the Machiavellianism scale ðMach IV; Christie and Geis 1970Þ to measure people’s attitude to- ward opportunistic behavior: “It is hard to get ahead without cutting cor- ners here and there,” “Honesty is the best policy in all cases,” and “Most people who get ahead in the world lead clean, moral lives.”10 Mach IV has been shown to predict untrustworthiness in investment games ðGunnthors- dottir, McCabe, and Smith 2002Þ. Responses were reverse coded to mea- sure “anti-Machiavellianism” ðCronbach’s a 5 .83Þ. Three additional ques- tions were added to measure reputational concerns: “One should not be overly concerned with his or her own image if one wants to be successful,” “A person’s reputation is not very useful in judging his or her true char- acter,” and “I care about what others think of me, even if I won’t see them again.” The first two questions were adopted from Yamagishi and Yama- gishi ð1994Þ and reverse coded. Responses were measured on a seven-point scale ðCronbach’s a 5 .90Þ. Results Figure 3 plots the number of high-quality service hours commissioned ðstudy 2AÞ or provided ðstudy 2BÞ in each round. As intended, there was no difference between the contract and the reputation conditions in either trust ðb 5 .40, P 5 .68Þ or trustworthiness ðb 5 .56, P 5 .37Þ in phase 1.11 However, the reputation condition produced greater trust than the con- tracts condition after removing enforcement in phase 2 ðrounds 12–15; b 5 1.56, P 5 .028Þ. Similarly, the reputation condition produced greater trustworthiness than the contracts condition after removing enforcement ðrounds 20–23; b 5 1.14, P 5 .046Þ. These patterns converge with the re- sults from study 1 and rule out effects of contrast in risk between phases 1 and 2, since no differences in phase 1 were found across conditions in either study. Results from the postexperimental questionnaire provide additional sup- port for hypotheses 1 ðreputation > contractsÞ and 3 ðreputation > fixed rela- tionsÞ. As figure 4A shows, participants in the contracts condition reported 10Mach IV is based on Machiavelli’s The Prince and contains questions measuring manipulativeness and cynicism also, but those questions were precluded. 11To examine the effects of enforcement over a longer period of time in phase 2, phase 2 ended in round 15 instead of 14 in study 2A, and phase 1 ended in round 19 instead 20 in study 2B. Do Reputation Systems Undermine Trust? 1411 F IG . 3 .— A ,L ev el s of tr u st fr om st u d y 2A ;B ,t ru st w or th in es s fr om st u d y 2B .E n fo rc em en t w as re m ov ed af te r ro u n d 11 in st u d y 2A an d ro u n d 19 in st u d y 2B . 1412 F IG . 4 .— A , S el f- re p or te d ge n er al iz ed tr u st ; B , at ti tu d e to w ar d op p or tu n is m ðM ac h ia v el lia n is m , re v er se co d ed Þ; an d C , co n ce rn s ab ou t re p u ta ti on s fr om th e p os te xp er im en ta l q u es ti on n ai re s in st u d ie s 2A an d 2B . L ik er t- ty p e se v en -p oi n t sc al es w er e u se d fo r al l it em s. E rr or b ar s in d ic at e S D s. 1413 lower levels of generalized trust ð4.10 ± 1.48Þ than those in the reputation condition ð5.01 ± 1.26; tð60Þ 5 2.61, P 5 .012Þ. In study 2B, participants from the reputation condition reported greater disapproval of opportun- ism ðanti-Machiavellianism; fig. 4B; 5.33 ± 1.19Þ than those from the con- tracts condition ð4.82 ± 1.45Þ, although this difference did not reach sig- nificance ðtð56Þ 5 1.49, P 5 .14Þ. However, those from the reputation condition reported greater reputational concerns ð5.83 ± .97Þ than those from the contracts condition ð4.92 ± 1.79; tð56Þ 5 2.40, P 5 .02; fig. 4CÞ. Controlling for age, race, and sex did not change the statistical significance of these results. STUDY 3: DO EBAY USERS BECOME MORE TRUSTING OR TRUSTWORTHY? Studies 1 and 2 found that participants show greater generalized trust and trustworthiness after even relatively brief exposure to reputation systems than to contracts in simulated transactions. These results are predicated on the idea that reputational enforcement promotes perception of trust and trustworthiness between strangers, including self-perception. How- ever, I have not directly tested for self-perception as a possible theoretical mechanism. Study 3 was designed to address this limitation by manipulat- ing self-perception of generalized trust and trustworthiness and to provide external validity by recruiting actual eBay users to see whether exposure to a real life feedback system promotes trust and trustworthiness. Design and Procedures Study 3 used a survey experiment in a 2 ðrole: truster vs. trusteeÞ × 2 ðself- perception: salient vs. nonsalientÞ design to manipulate self-perceptions of trust and trustworthiness. Participants were 475 people ð31.27 years old, 41% maleÞ recruited from Amazon Mechanical Turk ðMTurkÞ to complete a short survey. The study was advertised as restricted to people with an active eBay account.12 The instruction stated that the survey consisted of two unrelated studies: ð1Þ background questions about the respondent’s sex, age, race, education level, and eBay profile, including number of positive and negative feedback 12MTurk is an online labor market for crowdsourcing various microtasks to hu- man “workers” for small payments ðtypically a few centsÞ. Today, it claims hundreds of thousands of active workers and is finding increasing popularity among researchers for recruiting participants for simple studies quickly and cheaply from a considerably more diverse population than is typically available on college campuses. Studies show sig- nificant convergence between results fromMTurk and lab experiments ðfor overviews, see Buhrmester, Kwang, and Gosling 2011; Mason and Suri 2012Þ. American Journal of Sociology 1414 points received in the last 12 months, what percentage of their feedback points was earned as a buyer rather than a seller, and the user name, and ð2Þ an investment game with another anonymous person from MTurk to measure respondents’ generalized trust or trustworthiness. For the in- vestment game, respondents were randomly assigned to the truster or the trustee role. Those playing the role of the truster were given $5, which could be sent to an anonymous responder in increments of 50 cents. Their entrustments were tripled, which responders divided between themselves and their senders. Those playing the trustee were told that their truster had sent them $3.50 out of $5 ðhence receiving $11.50 to divideÞ. Participants received 10 cents for completing the survey. In addition, they received an additional payment with 25% probability based on randomly matching their response in the investment game with another participant’s. Feedback points were elicited to predict respondents’ decisions in the in- vestment game as a function of their exposure to eBay’s reputation system. One possibility is that the more positive feedback a respondent has earned, the more often that the reputation system has presumably reinforced his self-perceived trust and trustworthiness. This idea is problematic for two reasons, however. One is selection bias: trusting or trustworthy people may be more active on eBay. Another reason is that a variety of services against fraud have been available to eBay users in recent years, including escrow services or third-party protection by credit card companies. Because these services are tantamount to contractual enforcement with binding terms, rep- utation scores from eBay may reflect effects of contractual rather than repu- tational enforcement. The real reason for eliciting reputation scores was to experimentally manipulate the salience of self-perceived trust and trustworthiness. To this end, the survey was randomized to ask the background questions ðincluding feedback scores from eBayÞ first or present the investment game first. Solic- iting background information first was intended to invoke self-perceptions of trust or trustworthiness on the basis of one’s own feedback score before playing the investment game. To reinforce this manipulation, respondents were asked to report their feedback score and “spend a minute to think about how you earned this score. What percentage of your feedback points did you earn as a buyer ½seller?” Such manipulations have been used in experiments on honesty ðMazar, Amir, and Ariely 2008Þ; for instance, people are less likely to lie or misreport on insurance forms if they are asked to sign their names before, rather than after, completing the form ðShu et al. 2012Þ. Similarly, I predicted that respondents would show greater trust and trust- worthiness after reporting their positive feedback scores, controlling for neg- ative scores. Respondents’ usernames from eBay were used to verify the reported feedback scores against their public profiles on eBay. Sixty-two people were Do Reputation Systems Undermine Trust? 1415 removed due to missing or incorrect information. Of the remaining sam- ple, 245 people were active eBay users in the past 12 months; the others were coded as inactive. In the final data set, 36% were female, 62% were white, 32% Asian, 2.6% Hispanic, and 1.8% black; the average age was 31 ± 10.76; and 85.5% had a college degree or above. The average num- ber of positive reputation points was 135 ðSD 5 698.6Þ and that of neg- ative points was .49 ðSD 5 1.51Þ.13 Of the reported transactions in the last 12 months, 63.5% were as buyers. Results Following studies 1 and 2, I used Tobit regression to predict decisions in the investment game. Models 1 and 2 in table 3 show the results for re- spondents in the role of the truster. As shown, being an eBay user by itself had no effect, whether respondents reported their reputation scores first or not. However, having positive feedback points shows a positive effect ðb 5 .71, SE 5 .25, P < .01Þ, but only when it was reported first, be- fore the investment game. I obtained substantively identical results for trustworthiness in models 4 and 5. Because total positive feedback scores include points earned as a buyer or seller or both, they may conflate self-perceptions of trust and trustwor- thiness. To address this issue, I broke down total positive scores into positive scores by role, using self-reports of percentage of transactions as a buyer versus seller. Using these predictors instead of total positive feedback score, I found patterns suggesting self-perception. Specifically, for those in the truster role in the investment game, having positive points as a buyer was positive and significant ðb 5 .76, SE 5 .31, P 5.014Þ, but having positive points as a seller was not ðb 5 2.10, SE 5 .22, P 5.66Þ when the back- ground questions were presented first ðmodel 3Þ; neither variable was significant when the investment game was presented first ðnot shownÞ. In mirror contrast, for those in the trustee role in the investment game, having positive points as a seller was positive and significant ðb 5 1.46, SE5 .52, P5 .006Þ, but having positive points as a buyer was not ðb5 .76, SE 5 .47, P 5 .11Þ when the background questions were presented first ðmodel 6Þ; neither variable was significant when the investment game was presented first. Thus, reputations for trust and trustworthiness both affected decisions to place and honor trust in the investment game, but only when people reflected on their reputations in the relevant role. 13The large standard deviations are due to one person with 13,109 positive points and 212 negative points. Excluding this person did not change my main results. American Journal of Sociology 1416 T A B L E 3 T o b i t R e g r e s s i o n P r e d i c t i n g T r u s t a n d T r u s t w o r t h i n e s s i n a n I n v e s t m e n t G a m e , b e f o r e a n d a f t e r R e p o r t i n g e B a y F e e d b a c k S c o r e s T R U S T IN IG T R U S T W O R T H IN E SS IN IG M od el 1 IG F ir st M od el 2 Q s F ir st M od el 3 Q s F ir st M od el 4 IG F ir st M od el 5 Q s F ir st M od el 6 Q s F ir st Is an ac ti ve eB ay us er .. .. .. .. .. .. .. .. .. . .8 6 2 1. 78 2 .2 6 1. 91 2 1. 96 2 1. 58 ð1 .1 0Þ ð1 .3 1Þ ð1 .1 5Þ ð2 .7 5Þ ð2 .0 6Þ ð1 .7 2Þ P os it iv e fe ed b ac k p oi n ts ðlo gg ed Þ . . . . . .1 0 .7 1* * 2 .6 7 1. 57 ** ð.2 6Þ ð.2 5Þ ð.6 9Þ ð.5 7Þ N eg at iv e fe ed ba ck p oi n ts .. .. .. .. .. .. .. 2 .0 4 .4 1 .3 1 2 .0 4 2 .0 5 2 .0 6 ð.2 1Þ ð.3 0Þ ð.2 9Þ ð.0 3Þ ð.0 3Þ ð.0 3Þ P os it iv e fe ed ba ck as b uy er ðlo gg ed Þ .. .. .8 6* .7 6 ð.3 0Þ ð.4 7Þ P os it iv e fe ed ba ck as se lle r ðlo gg ed Þ. . . . 2 .1 0 1. 46 ** ð.2 2Þ ð.5 2Þ C on st an t . . . . . . . . . . . . . . . . . . . . . . . 4. 31 * 1. 90 4. 04 ** 8. 25 ** 6. 21 5. 57 ** ð1 .8 1Þ ð1 .8 4Þ ð.6 9Þ ð3 .4 0Þ ð3 .2 5Þ ð.8 0Þ N . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 6 11 5 11 5 96 86 86 P se ud o R 2 . . . . . . . . . . . . . . . . . . . . . . .0 1 .0 6 .0 4 .0 2 .0 3 .0 3 L og lik el ih oo d . . . . . . . . . . . . . . . . . . . 2 26 1. 27 2 25 1. 47 2 25 5. 93 2 26 4. 59 2 23 3. 41 2 23 2. 94 N O T E .— IG 5 in v es tm en tg am e; Q s 5 b ac k gr ou nd q u es ti on s. S E s ar e in p ar en th es es .A ll m od el s co n tr ol le d fo r ge n d er ,a ge ,r ac e, an d ed u ca ti on le v el .A ll tw o- ta ile d te st s. * P < .0 5. ** P < .0 1. DISCUSSION Three sets of studies supported my claim that reputational enforcement can create exchange conditions that promote generalized trust and trust- worthiness. Whereas binding contracts are designed to substitute for trust and trustworthiness by providing direct sanctions, reputations can comple- ment them by reinforcing people’s self-perception that they are willing to trust strangers or honor their trust, even in the absence of strong enforce- ment. This is in part because reputational enforcement is indirect and dif- fuse enough to mitigate the risks of exchange without eliminating them completely, in effect creating an intermediate condition between unenforced exchanges that are too risky to enter and contractual enforcement that makes transactions too safe yet too restrictive to nurture generalized trust or trust- worthiness. Reputational enforcement also produced greater generalized trust and trustworthiness than fixed-partner relations did. After repeated exchanges with the same partner, people were no more trusting or trustworthy than those in the unenforced condition. In contrast, people exposed to reputa- tional enforcement were more likely to place or honor trust in unenforced transactions thereafter. Through repeated interactions, people develop trust toward particular others, but not to generalized others. These results are predicated on the idea that different forms of gover- nance have different effects on perceptions of generalized trust and trust- worthiness, including self-perceptions. Even in one-shot, anonymous ex- changes with limited opportunities to learn about each other, actors can learn about themselves and change their self-perceptions through experience and reflection, viewing themselves as more and more trusting under certain con- ditions and less and less trusting under other conditions. While my results are consistent with this argument, it is important to note that none of the studies presented here provide a direct measure of self-perception, only be- havioral consequences of self-perception. Measuring self-perception directly is not as straightforward as measuring social perception, because self-reports of self-perception are likely to be biased in rather self-serving ways; many people are reluctant to admit that they see themselves as distrustful or un- trustworthy people. Furthermore, inmany situations, social and self-perception can occur concurrently as people attempt to make sense of their situations frommultiple perspectives to understand themselves and each other. Thus, the current studies should not be interpreted as conclusive or exclusive tests of self-perception to rule out social perception. Still, there are several reasons for why self-perception offers a more compelling explanation for the three studies than social perception. First, my studies focused on one-shot exchanges ðexcept the fixed-partner condi- tionsÞ with anonymous virtual partners. As research on attribution theory American Journal of Sociology 1418 has shown ðKelley 1967Þ, such situations are more conducive to learning about oneself than about others in general or in ways that might generalize to yet unknown others, because people can observe themselves, but not specific others, across situations. Second, with respect to trustworthiness, so- cial perception is a moot issue in investment games, because the second mover knows whether or exactly how much the first mover has decided to trust or not, and social perception is agnostic about whether people will actually refrain from cheating others. In comparison, self-perceptions of trustworthiness can directly promote acts of trustworthiness by invoking a sense of self that identifies with being trustworthy, even in situations in which cheating yields a greater immediate payoff. Thus, self-perception can better explain why participants in the reputation conditions were more trustworthy in all three studies. Third, with respect to study 3, social per- ception seems less likely, given the mechanics of feedback points in repu- tation systems like eBay. Feedback points are given to a buyer because the buyer was trusting, not because the seller was trustworthy. One’s feedback points should therefore reflect more directly on oneself ðself-perceptionÞ rather than on exchange partners ðsocial perceptionÞ. In this sense, it is difficult to explain why receiving positive feedback points should make people perceive others to be more trustworthy. Empirically, the idea that reputational enforcement can promote gen- eralized trust and trustworthiness extends two previous studies in partic- ular. In Malhotra and Murnighan ð2002Þ, participants were exposed to several rounds of prisoner’s dilemmas with an anonymous but fixed part- ner under binding versus nonbinding contracts. They found that, unlike binding contracts that coerce cooperation but undermine trust, nonbinding contracts can sustain relatively high levels of cooperation without dimin- ishing intrinsic motivations to cooperate once enforcement is removed, because offering nonbinding contracts signals positive intentions to enter transactions and offer cooperation without imposing strong external con- straints that eliminate risk completely. Reputational enforcement seems to work in a similar fashion, motivating cooperation without coercing it by signaling trust and trustworthiness. Bohnet and Huck ð2004Þ measured trust and trustworthiness in one-shot investment games after exposure to three experimental conditions: unen- forced one-shot exchanges ðbaselineÞ versus unenforced exchanges with fixed partners versus one-shot exchanges under a reputation system. They found that, after removing reputations, trust and trustworthiness both dropped to the baseline level, but not higher, thus showing no reinforce- ment of trust or trustworthiness under reputational enforcement. One possible reason is that, in their free-play design with real human ðrather than simulatedÞ partners, cooperation under enforcement rarely exceeded Do Reputation Systems Undermine Trust? 1419 50%, possibly because their reputation system was much shorter in scope, tracking only one previous round rather than aggregating over the entire history of transactions. This suggests that reputation systems may need to be effective enough to ensure high levels of cooperation in order to also rein- force trust and trustworthiness.14 Theoretically, the ideas about different forms of enforcement in the current research echo recent efforts by exchange theorists to specify how different forms of exchange can invoke different attributions and percep- tions that promote or inhibit the development of trust ðe.g., Molm et al. 2000; Lawler 2001Þ. In negotiated exchange, actors engage in direct bar- gaining over fixed resources to reach specific and binding agreements on the division of benefits. Each episode of exchange is a discrete event con- sisting of joint action by both parties under contractual enforcement. In comparison, reciprocal exchange generally involves sequences of unilat- eral giving and receiving in which actors provide favors or benefits to each other separately and without explicit bargaining. The norm of reciprocity creates diffuse obligations to return favors, but when and how to recipro- cate is unspecified. The current research builds on these ideas in several ways. First, I examined bilateral transactions, a hybrid form of exchange with features of both negotiated and reciprocal exchange. To wit, transactions were modeled using the investment game ðBerg et al. 1995Þ in which one actor is assigned to the role of the truster and the other the trustee. Like recip- rocal exchange, the investment game occurs without explicit bargaining, communication, or guarantee of repayment, creating risk of exploitation. Unlike reciprocal exchange, however, the investment game is a type of negotiated exchange characterized by a bilateral flow of benefits ðMolm et al. 2000Þ. Transactions can therefore take on characteristics of negoti- ated or reciprocal exchange, depending, for instance, on how they are en- forced. Formal enforcement using contracts or material sanctions can make transactions more similar to negotiated exchange by ensuring reciprocity and eliminating the risk of exploitation, with direct consequences for trust and trustworthiness. Despite their relevance to markets, transactions have been overlooked in exchange theory as a special case of negotiated exchange ðKuwabara 2011Þ. Second, using the investment game allowed me to examine trust and trustworthiness separately. While trust has been studied extensively by ex- change theorists, trustworthiness has received less attention ðbut see Hardin 2002; Simpson and Eriksson 2009Þ. One reason is that trustworthiness is 14My study differs from Bohnet and Huck ð2004Þ in other ways as well. First, they did not examine contractual enforcement. Second, they did not experimentally isolate trust and trustworthiness from each other. Thus, buyer behaviors may be driven by seller behaviors ðor vice versaÞ rather than enforcement effects. American Journal of Sociology 1420 irrelevant in negotiated exchange under binding agreements that ensures full compliance, while in reciprocal exchange, trustworthiness is conflated with reciprocal acts of trust since “the same act can complete one exchange and initiate another” ðMolm et al. 2000, p. 1400Þ. However, understanding bi- lateral transactions requires understanding trustworthiness as a critical but discrete component of the exchange. Third, I examined generalized rather than personalized trust and trust- worthiness by considering one-shot exchanges. While exchange theory has focused almost exclusively on repeated exchange, I show that the logic of so- cial exchange can be extended to one-shot, episodic exchange outside of fixed relations, by understanding self-perception as a process that occurs in parallel to social perception. Fourth, the pretest-posttest design evokes the emerging work on the sequencing effects of different exchange forms ðCheshire, Gerbasi, and Cook 2010; Molm, Whitham, and Melamed 2012Þ. Molm et al. ð2012Þ argue that beginning an exchange relation with negotiated exchange sensitizes actors to conflictual aspects of exchange and inhibits the development of soli- darity, even after they switch to reciprocal exchange, compared to relations consisting exclusively of reciprocal exchange.15 My findings are consistent with this argument: the contracts conditions in which participants ex- changed under binding terms weakened both trust and trustworthiness in unenforced transactions in phase 2, compared to both the fixed-partner or unenforced conditions. One important difference, however, is that my central mechanism is not sensitivity or contrast to conflictual aspects of negotiated exchange, as Molm et al. ð2012Þ argue, but the lack of risk under binding agreements.Bothmechanisms can reduce solidarity ðMolm, Collett, and Schaefer 2007Þ. Finally, my results speak to emerging ideas about fragile versus resilient trust. Molm et al. ð2009Þ show that, in fixed relations, reciprocal exchange is more likely to produce trust that is resilient to occasional mistrust, whereas negotiated exchange produces trust that is relatively fragile, because trust on the basis of binding agreement can last only so long as the terms of exchange are clearly met. Similarly, my studies show that gradual exposure to ðmoder- ateÞ risk under reputational enforcement is more likely than contractual en- forcement to promote resilient trust that persists even in the absence of en- forcement by inoculating people against risk. It is notable that, as the current research as well as other similar studies show, relatively brief exposure to different types of enforcement in an arti- 15Their experiments provided support for this prediction, but only for actors in power- disadvantaged relations who may be more sensitive to the risks and conflicts in exchange relations. Although my studies did not manipulate structural power in exchange networks, one-shot exchanges may have similar effects. Do Reputation Systems Undermine Trust? 1421 ficial setting is sufficient to induce systematic behavioral changes that per- sisted over, in my case, periods of several hours or a couple of days. Recent studies show that changes in prosocial behavior induced by self-perception can be surprisingly persistent ðGneezy and Rustichini 2000; Grant and Dutton 2012Þ. In real life, many people experience more sustained exposure to reputational enforcement, accruing hundreds and thousands of feedback points as repeat customers of eBay, Yelp, Amazon, and other online mar- kets with feedback mechanisms. Still, it is unlikely that the observed changes in trust and trustworthiness are permanent because reinforced behaviors can be unlearned over time. As study 3 suggests, self-perceptions are also situational, activated in certain contexts but not others. Without deliber- ately reflecting on their own reputation scores, respondents were no more trusting or trustworthy regardless of their prior history on eBay. The current experiments were designed to test the internal validity of my theoretical arguments under the most elemental conditions by using ex- periments to isolate theoretically and psychologically relevant features of reputation systems that distinguish them from other forms of enforcement. Although the results suffice to show that reputation systems can reinforce generalized trust and trustworthiness, reputation systems exist in many forms across different markets and communities. More research is needed to enrich our understanding of how and when reputation systems promote trust and trustworthiness under more varied conditions. First, how much cooperation is needed to reinforce trust and trustworthiness? In my study, transaction partners were simulated, given fixed reputations, and pro- grammed to offer full cooperation in every round. Although this is not an entirely unrealistic portrayal of many real world markets like eBay where transactions have been overwhelmingly positive and successful ðKollock 1999; Resnick et al. 2000Þ, fraud and mistrust do occur from time to time. How much fraud a reputation system can tolerate before it starts to col- lapse and erode people’s trust is an increasingly critical question for better understanding the efficacy of reputation systems in light of growing con- cerns about cybersecurity and dubious reputation management practices ðBilton 2011Þ. Second, my experiments simulated very simple reputation systems with only singular numerical ratings. Howmy results might extend to more varied types of reputation systems with multidimensional ratings or predomi- nantly qualitative evaluations is an important question for future research. On the one hand, it is plausible that the addition of verbal feedback or more fine-grained ratings might enhance people’s experience of trust ðPavlou and Dimoka 2006Þ. On the other hand, it is also plausible that reputation systems can become coercive enough to erode generalized trust and trust- worthiness ðNissenbaum 2004Þ. Designing an effective reputation system— or any benign institution, for that matter—is a balancing act. American Journal of Sociology 1422 CONCLUSION Despite the substantial body of research demonstrating the efficacy of rep- utation systems for sustaining cooperation, hardly any research has consid- ered their consequences for people’s actions outside of such systems. In- deed, although sociologists have long been interested in various forms of governance of exchange relations, very little attention has been paid to how a particular system of enforcement might affect behaviors and de- cisions in other domains. This gap in our understanding of the spillover effects of enforcement systems comes to bear particularly in light of stud- ies showing that formal enforcement using contracts or tangible sanctions can erode intrinsic motivation to cooperate. Whether reputation systems have similar consequences is a question with significant implications for research on social exchange and governance as well as for market design and social policies as online markets continue to grow in scope and scale ðNis- senbaum 2004; Cook et al. 2009Þ. By examining different forms of governance that are prevalent in mar- kets ðreputationÞ, hierarchies ðcontractsÞ, and networks ðrepeated exchangeÞ, the current research sheds new light on conditions under which trust and trustworthiness that people develop in local interactions might become gen- eralized beyond specific relations, organizations, or markets. While studies have linked generalized trust and trustworthiness to a wide range of so- cietal benefits, from civic participation and volunteerism ðPutnam 2000; Uslaner 2002Þ to social integration ðSmith 2010; Uslaner 2012Þ to economic growth ðPutnam 1993; Knack and Keefer 1997Þ, identifying the mech- anisms through which generalized trust and trustworthiness develop has remained rather elusive. Scholars have posited social learning processes in which trust that emerges from local interactions with familiar partners generalizes to other contexts ðPutnam 2000; Glanville and Paxton 2007Þ, although which social and institutional conditions enable such processes remain contested ðUslaner 2002; Glanville 2004Þ. The current study points to reputational enforcement as a potentially important mechanism for the development of generalized trust and trustworthiness. Reputations create conditions of moderate risk by identifying and matching people with reli- able exchange partners outside of repeated interactions. Through this pro- cess of entering and consummating exchanges with new partners on the basis of reputational enforcement, people increasingly come to perceive themselves as trusting and trustworthy. Although such processes may be more formalized in online reputation systems, they are also ubiquitous in offline markets and communities ðGreif 1989; Raub and Weesie 1990; Kollock 1994; Hillmann and Aven 2011Þ. My research therefore points to an important role that market transactions may play in promoting generalized trust and trustworthiness. Economic Do Reputation Systems Undermine Trust? 1423 exchange is often assumed to undermine prosocial behaviors and senti- ments by introducing instrumental rationality into the moral and affective fabric of social relations ðMolm et al. 2000; Zelizer 2005; Ingram and Zou 2008Þ. My results show, however, that people can develop trust and trust- worthiness toward anonymous strangers on the basis of arm’s length, trans- actional exchanges that test and affirm their sense of trust and integrity under conditions of moderate risk. REFERENCES Akerlof, George A. 1970. “The Market for ‘Lemons’: Quality Uncertainty and the Market Mechanism.” Quarterly Journal of Economics 84:488–500. Axelrod, Robert. 1984. The Evolution of Cooperation. New York: Basic. Bajari, Patrick, and Ali Hortaçsu. 2003. “The Winner’s Curse, Reserve Prices and Endogenous Entry: Empirical Insights from Ebay Auctions.” Rand Journal of Eco- nomics 3:329–55. Bem, Daryl J. 1967. “Self-Perception: An Alternative Interpretation of Cognitive Dis- sonance Phenomena.” Psychological Review 74:183–200. Berg, Joyce, John Dickhaut, and Kevin McCabe. 1995. “Trust, Reciprocity, and Social History.” Games and Economic Behavior 10:122–42. Bilton, Nick. 2011. “The Growing Business of Online Reputation Management.” New York Times, April 4. Bohnet, Iris, Bruno S. Frey, and Steffen Huck. 2001. “More Order with Less Law: On Contract Enforcement, Trust, and Crowding.” American Political Science Review 95:131–44. Bolton, Gary, Ben Greiner, and Axel Ockenfels. 2012. “Engineering Trust: Reciprocity in the Production of Reputation Information.” Management Science 59:265–85. Bolton, Gary E., Elena Katok, and Axel Ockenfels. 2004. “How Effective Are Online Reputation Mechanisms? An Experimental Study.” Management Science 50:1587– 1602. ———. 2005. “Cooperation among Strangers with Limited Information about Repu- tation.” Journal of Public Economics 89:1457–68. Bowles, Samuel. 2008. “Policies Designed for Self-Interested Citizens May Undermine ‘the Moral Sentiments’: Evidence from Economic Experiments.” Science 320:1605–9. Buhrmester, Michael, Tracy Kwang, and Samuel D. Gosling. 2011. “Amazon’s Me- chanical Turk.” Perspectives on Psychological Science 6:3–5. Cheshire, Coye. 2011. “Online Trust, Trustworthiness, or Assurance?” Daedalus 140: 49–58. Cheshire, Coye, Alexandra Gerbasi, and Karen S. Cook. 2010. “Trust and Transitions in Modes of Exchange.” Social Psychology Quarterly 73:1–20. Christie, Richard, and Florence L. Geis. 1970. Studies in Machiavellianism. New York: Academic Press. Coleman, James S. 1990. Foundations of Social Theory. Cambridge, Mass.: Belknap Press of Harvard University Press. Cook, Karen, Russell Hardin, and Margaret Levi. 2005. Cooperation without Trust? New York: Russell Sage. Cook, Karen S., Coye Cheshire, Alexandra Gerbasi, and Brandy Aven. 2009. “Assess- ing Trustworthiness in Providers of Online Goods and Services.” Pp. 189–214 in eTrust: Forming Relationships in the Online World, edited by Karen S. Cook, Chris Snijders, Vincent Buskens, and Coye Cheshire. New York: Russell Sage. American Journal of Sociology 1424 Cook, Karen S., and Russell Hardin. 2001. “Norms of Cooperativeness and Networks of Trust.” Pp. 327–47 in Social Norms, edited by Michael Hechter and Karl-Dieter Opp. New York: Russell Sage. Cook, Karen S., Chris Snijders, Vincent Buskens, and Coye Cheshire. 2009. eTrust: Forming Relationships in the Online World. New York: Russell Sage. Deci, Edward L., Richard Koestner, and Richard M. Ryan. 1999. “A Meta-analytic Review of Experiments Examining the Effects of Extrinsic Rewards on Intrinsic Motivation.” Psychological Bulletin 125:627–68. ———. 2001. “Extrinsic Rewards and Intrinsic Motivation in Education: Reconsidered Once Again.” Review of Educational Research 71:1–27. Dellarocas, Chrysanthos. 2003a. “Building Trust Online: The Design of Robust Rep- utation Reporting Mechanisms in Online Trading Communities.” In Information Society or Information Economy? A Combined Perspective on the Digital Era, edited by Georgios I. Doukidis, Nikolaos Mylonopoulos, and Nancy Pouloudi. Hershey, Pa.: Idea Book. ———. 2003b. “The Digitization of Word-of-Mouth: Promise and Challenges of Online Feedback Mechanisms.” Management Science 49:1407–24. ———. 2010. “Online Reputation Systems: How to Design One That Does What You Want.” MIT Sloan Management Review 51 ð3Þ. Diekmann, Andreas, Ben Jann, Wojtek Przepiorka, and Stefan Wehrli. 2014. “Repu- tation Formation and the Evolution of Cooperation in Anonymous Online Markets.” American Sociological Review 79:65–85. Diekmann, Andreas, Ben Jann, and David Wyder. 2009. “Trust and Reputation in Internet Auctions.” Pp. 139–65 in eTrust: Forming Relationships in the Online World, edited by Karen S. Cook, Chris Snijders, Vincent Buskens, and Coye Cheshire. New York: Russell Sage. Durkheim, Émile. 1893. The Division of Labor in Society. New York: Free Press. Fehr, Ernst, and Armin Falk. 2002. “Psychological Foundation of Incentives.” Euro- pean Economic Review 46:687–724. Fehr, Ernst, and Bettina Rockenbach. 2003. “Detrimental Effects of Sanctions on Human Altruism.” Nature 422:137–40. Frank, Robert H. 1988. Passions within Reason: The Strategic Role of the Emotions. New York: Norton. Fukuyama, Francis. 1995. Trust: Social Virtues and the Creation of Prosperity. New York: Free Press. Glaeser, Edward L., David I. Laibson, Jos A. Scheinkman, and Christine L. Soutter. 2000. “Measuring Trust.” Quarterly Journal of Economics 115:811–46. Glanville, Jennifer L. 2004. “Voluntary Associations and Social Network Structure: Why Organizational Location and Type Are Important.” Sociological Forum 19: 465–91. Glanville, Jennifer L., and Pamela Paxton. 2007. “How Do We Learn to Trust? A Confirmatory Tetrad Analysis of the Sources of Generalized Trust.” Social Psy- chology Quarterly 70:230–42. Gneezy, Uri, Stephan Meier, and Pedro Rey-Biel. 2011. “When and Why Incentives ðDon’tÞ Work to Modify Behavior.” Journal of Economic Perspectives 25:191–209. Gneezy, Uri, and Aldo Rustichini. 2000. “Pay Enough or Don’t Pay at All.” Quarterly Journal of Economics 115:791–810. Gouldner, Alvin W. 1960. “The Norm of Reciprocity: A Preliminary Statement.” Ameri- can Sociological Review 25:161–78. Granovetter, Mark. 1985. “Economic Action and Social Structure: The Problem of Em- beddedness.” American Journal of Sociology 91:481–510. Grant, Adam, and Jane Dutton. 2012. “Beneficiary or Benefactor.” Psychological Sci- ence 23:1033–39. Do Reputation Systems Undermine Trust? 1425 Greif, Avner. 1989. “Reputation and Coalitions in Medieval Trade: Evidence on the Maghribi Traders.” Journal of Economic History 49:857–82. Greif, Avner, Margaret Levi, Jean-Laurent Rosenthal, and Barry R. Weingast. 1998. “Self-Enforcing Political Systems and Economic Growth: Late Medieval Genoa.” Pp. 23– 63 in Analytic Narratives, edited by R. H. Bates. Princeton, N.J.: Princeton University Press. Gunnthorsdottir, Anna, Kevin McCabe, and Vernon Smith. 2002. “Using the Machi- avellianism Instrument to Predict Trustworthiness in a Bargaining Game.” Journal of Economic Psychology 23:49–66. Hardin, Russell. 2002. Trust and Trustworthiness. New York: Russell Sage. Hayashi, Nahoko, and Toshio Yamagishi. 1998. “Selective Play: Choosing Partners in an Uncertain World.” Personality and Social Psychology Review 2:276–89. Hillmann, Henning, and Brandy L. Aven. 2011. “Fragmented Networks and Entre- preneurship in Late Imperial Russia.” American Journal of Sociology 117:484–38. Homans, George C. 1961. Social Behavior: Its Elementary Form. New York: Harcourt Brace. Ingram, Paul, and Xi Zou. 2008. “Business Friendships.” Research in Organizational Behavior 28:167–84. Kelley, Harold H. 1967. “Attribution Theory in Social Psychology.” Pp. 192–238 in Nebraska Symposium on Motivation, edited by D. Levine. Lincoln: University of Ne- braska. Keser, Claudia. 2003. “Experimental Games for the Design of Reputation Management Systems.” IBM Systems Journal 42:498–506. Knack, Stephen, and Philip Keefer. 1997. “Does Social Capital Have an Economic Payoff? A Cross-Country Investigation.” Quarterly Journal of Economics 112:1251–88. Kollock, Peter. 1994. “The Emergence of Exchange Structures: An Experimental Study of Uncertainty, Commitment, and Trust.” American Journal of Sociology 100:313–45. ———. 1999. “The Production of Trust in Online Markets.” Advances in Group Pro- cesses 16:99–123. Kosugi, Motoko, and Toshio Yamagishi. 1998. “Generalized Trust and Judgments of Trustworthiness.” Japanese Journal of Psychology 69:349–57. Kramer, Roderick. 1999. “Trust and Distrust in Organizations: Emerging Perspectives, Enduring Questions.” Annual Review in Psychology 50:569–98. Kramer, Roderick M., and Tom R. Tyler. 1996. Trust in Organizations: Frontiers of Theory and Research. Thousand Oaks, Calif.: Sage. Kuwabara, Ko. “Cohesion, Cooperation, and the Value of Doing Things Together.” American Sociological Review 76:560–80. La Porta, Rafael, Florencio Lopez-de-Silanes, Andrei Shleifer, and Robert W. Vishny. 1997. “Trust in Large Organizations.” American Economic Review 87:333–38. Lawler, Edward J. 2001. “An Affect Theory of Social Exchange.” American Journal of Sociology 107:321–52. Lepper, Mark R., David Greene, and Richard E. Nisbett. 1973. “Undermining Chil- dren’s Intrinsic Interest with Extrinsic Reward: A Test of the ‘Overjustification’ Hypothesis.” Journal of Personality and Social Psychology 28:129–37. Luhmann, Niklas. 1979. Trust and Power. London: Wiley. Macy, Michael W., and Yoshimichi Sato. 2002. “Trust, Cooperation, and Market Formation in the U.S. and Japan.” Proceedings of the National Academy of Sciences 99:7214–20. Macy, Michael W., and John Skvoretz. 1998. “The Evolution of Trust and Cooper- ation between Strangers: A Computational Model.” American Sociological Review 63:638–60. Malhotra, Deepak, and J. Keith Murnighan. 2002. “The Effects of Contracts on Inter- personal Trust.” Administrative Science Quarterly 47:534–59. American Journal of Sociology 1426 Mason, Winter, and Siddharth Suri. 2012. “Conducting Behavioral Research on Amazon’s Mechanical Turk.” Behavior Research Methods 44 ð1Þ: 1–23. Mazar, Nina, On Amir, and Dan Ariely. 2008. “The Dishonesty of Honest People: A Theory of Self-Concept Maintenance.” Journal of Marketing Research 45:633–44. Molm, Linda D., Jessica L. Collett, and David R. Schaefer. 2007. “Building Solidarity through Generalized Exchange: A Theory of Reciprocity.” American Journal of So- ciology 113:205–42. Molm, Linda D., Nobuyuki Takahashi, and Gretchen Peterson. 2000. “Risk and Trust in Social Exchange: An Experimental Test of a Classical Proposition.” American Jour- nal of Sociology 105:1296–1427. Molm, Linda D., Monica M. Whitham, and David Melamed. 2012. “Forms of Exchange and Integrative Bonds: Effects of History and Embeddedness.” American Sociological Review 77:141–65. Mulder, Laetitia B., Eric van Dijk, David De Cremer, and Henk A. M. Wilke. 2006. “Undermining Trust and Cooperation: The Paradox of Sanctioning Systems in Social Dilemmas.” Journal of Experimental Social Psychology 42:147–62. Nissenbaum, Helen. 2004. “Will Security Enhance Trust Online, or Supplant It.” Pp. 155–88 in Trust and Distrust in Organizations: Dilemmas and Approaches, edited by Roderick M. Kramer and Karen S. Cook. New York: Russell Sage. Pavlou, Paul A., and Angelika Dimoka. 2006. “The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation.” Information Systems Research 17:392–414. Pavlou, Paul A., and David Gefen. 2004. “Building Effective Online Marketplaces with Institution-Based Trust.” Information Systems Research 15:37–59. Paxton, Pamela. 2007. “Association Memberships and Generalized Trust: A Multilevel Model across 31 Countries.” Social Forces 86:47–76. Powell, Walter W. 2003. “Neither Market nor Hierarchy.” Pp. 104–17 in The Sociology of Organizations: Classic, Contemporary, and Critical Readings, edited by Michael J. Handel. Thousand Oaks, Calif.: Sage. Puranam, Phanish, and Bart S. Vanneste. 2009. “Trust and Governance: Untangling a Tangled Web.” Academy of Management Review 34:11–31. Putnam, Robert D. 1993. Making Democracy Work: Civic Traditions in Modern Italy. Princeton, N.J.: Princeton University Press. ———. 2000. Bowling Alone: The Collapse and Revival of American Community. New York: Simon & Schuster. Raub, Werner, and Jeroen Weesie. 1990. “Reputation and Efficiency in Social Inter- actions: An Example of Network Effects.” American Journal of Sociology 96:626–54. Resnick, Paul, Richard Zeckhauser, Eric Friedman, and Ko Kuwabara. 2000. “Repu- tation Systems.” Communications of the ACM 43:45–48. Resnick, Paul, Richard Zeckhauser, John Swanson, and Kate Lockwood. 2006. “The Value of Reputation on Ebay: A Controlled Experiment.” Experimental Economics 9:79–101. Rotter, Julian B. 1971. “Generalized Expectancies for Interpersonal Trust.” American Psychologist 26:443. Shu, Lisa L., Nina Mazar, Francesca Gino, Dan Ariely, and Max H. Bazerman. 2012. “Signing at the Beginning Makes Ethics Salient and Decreases Dishonest Self-Reports in Comparison to Signing at the End.” Proceedings of the National Academy of Sci- ences 109:15197–200. Simpson, Brent, and Kimmo Eriksson. 2009. “The Dynamics of Contracts and Gen- eralized Trustworthiness.” Rationality and Society 21:59–80. Skinner, B. F. 1969. Contingencies of Reinforcement: A Theoretical Analysis. New York: Appleton-Century-Crofts. Smith, Sandra Susan. 2010. “Race and Trust.” Annual Review in Sociology 36:453–75. Do Reputation Systems Undermine Trust? 1427 Spence, A. Michael. 1974.Market Signaling: Informational Transfer in Hiring and Related Processes. Cambridge, Mass.: Harvard University Press. Stolle, Dietlind. 2001. “Clubs and Congregations: The Benefits of Joining an Associa- tion.” Pp. 202–44 in Trust in Society, edited by K. S. Cook. New York: Russell Sage. Tenbrunsel, Ann E., and David M. Messick. 1999. “Sanctioning Systems, Decision Frames, and Cooperation.” Administrative Science Quarterly 44:684–707. Uslaner, Eric M. 2002. The Moral Foundations of Trust. Cambridge: Cambridge Uni- versity Press. ———. 2012. Segregation and Mistrust: Diversity, Isolation, and Social Cohesion. Cambridge: Cambridge University Press. Willer, Robb. 2009. “Groups Reward Individual Sacrifice: The Status Solution to the Collective Action Problem.” American Sociological Review 74:23–43. Williamson, Oliver E. 1981. “The Economics of Organization: The Transaction Cost Approach.” American Journal of Sociology 87:548–77. Wilson, Rick K. 1997. “‘Liar, Liar’: Cheap Talk and Reputation in Repeated Public Goods Settings.” Journal of Conflict Resolution 41:695–717. Yamagishi, Toshio. 1998. The Structure of Trust: The Evolutionary Games of Mind and Society. Tokyo: University of Tokyo Press. Yamagishi, Toshio, Karen S. Cook, and Motoki Watabe. 1998. “Uncertainty, Trust and Commitment Formation in the United States and Japan.” American Journal of Sociol- ogy 108:165–94. Yamagishi, Toshio, Masafumi Matsuda, Noriaki Yoshikai, Hiroyuki Takahashi, and Yukihiro Usui. 2009. “Solving the Lemons Problem with Reputation.” Pp. 73–109 in eTrust: Forming Relationships in the Online World, edited by Karen S. Cook, Chris Snijders, Vincent Buskens, and Coye Cheshire. New York: Russell Sage. Yamagishi, Toshio, and Midori Yamagishi. 1994. “Trust and Commitment in the United States and Japan.” Motivation and Emotion 18:9–66. Zelizer, Viviana A. Rotman. 2005. The Purchase of Intimacy. Princeton, N.J.: Princeton University Press. American Journal of Sociology 1428 Copyright of American Journal of Sociology is the property of University of Chicago Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.