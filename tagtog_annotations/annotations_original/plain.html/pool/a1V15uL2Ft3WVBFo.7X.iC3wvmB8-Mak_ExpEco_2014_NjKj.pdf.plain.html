<!DOCTYPE html >
<html id="a1V15uL2Ft3WVBFo.7X.iC3wvmB8-Mak_ExpEco_2014_NjKj.pdf" data-origid="Mak_ExpEco_2014_NjKj.pdf" class="anndoc" data-anndoc-version="3.6" lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="UTF-8"/>
    <meta name="generator" content="net.tagtog.anndoc.v3.parsers.general.StubPdfParser_v1_0_0"/>
    <title>a1V15uL2Ft3WVBFo.7X.iC3wvmB8-Mak_ExpEco_2014_NjKj.pdf</title>
  </head>
  <body>
    <article>
      <section data-type="">
        <div class="content">
          <p id="s1p1">Exp Econ (2014) 17:262–284 DOI 10.1007/s10683-013-9366-8 O R I G I NA L PA P E R Experimenting and learning with localized direct communication Vincent Mak · Rami Zwick Received: 6 October 2009 / Accepted: 19 May 2013 / Published online: 7 June 2013 © Economic Science Association 2013 Abstract We report an experiment in which subjects may learn from each other. Specifically, a “queue” of players who are identically informed ex ante make deci- sions in sequence over two lotteries. Every player except the first in the queue ob- serves (only) his immediate predecessor’s choice and payoff before making his own decision. In equilibrium decisions are identical from the first or second player on- wards in all experimental conditions. However, complete adherence to equilibrium play is seldom observed in our experiment. We further analyze our data using a quan- tal response equilibrium approach and test for behavioral regularities related to base rate fallacy/conservatism bias, social conformity/rebelliousness, and preference for experimentation (preferring the lottery with potentially more information spillover value). Our estimations reveal a consistent preference for experimentation across con- ditions, and further analysis offers some support to our surmise that this behavioral regularity is due, in part, to an attempt to influence others behind in the queue. Keywords Communication · Bayesian learning · Social learning · Preference for experimentation JEL Classification C92 · D83 Electronic supplementary material The online version of this article (doi:10.1007/s10683-013-9366-8) contains supplementary material, which is available to authorized users. V. Mak () Cambridge Judge Business School, University of Cambridge, Trumpington Street, Cambridge CB2 1AG, UK e-mail: v.mak@jbs.cam.ac.uk R. Zwick Department of Management and Marketing, School of Business Administration, University of California, Riverside, CA 92521, USA e-mail: ramiz@ucr.edu Experimenting and learning with localized direct communication 263 1 Introduction In this study, we report an experiment in which subjects may learn from each other. Our setup involves a “queue” of players who are identically informed ex ante and who make decisions in sequence over two lotteries. Whichever lottery is played, ei- ther one of two possible payoffs may result. One lottery (the “known lottery”) yields the higher payoff with a fixed, commonly known probability. The other lottery (the “uncertain lottery”) yields the higher payoff with a probability that is not known for sure by the players, who only hold a common prior for that probability at the outset. Every player except the first in the queue observes (only) his immediate predecessor’s (i.e. the player in front) choice and payoff before making his own decision, so that the setup is characterized by localized direct communication. If the predecessor has chosen the known lottery, the realized payoff will not be informative to the decision maker; however, the choice of the predecessor by itself might serve as an informative signal to allow the player to infer what could have happened further up the queue. If the predecessor has chosen the uncertain lottery, both choice and payoff of the pre- decessor are potentially informative signals to the decision maker. Thus the setup is also characterized by path dependence of signal generation. Our experiment thus captures some features of the processes by which social agents learn from each others’ past behavior and experience when choosing between risky alternatives. For example, after a new product is launched, some consumers might purchase the product relatively early and then relate their own choice and con- sumption experience (effectively a “payoff”) to their social contacts. This can occur even when all consumers receive similar prior information about the product through the mass media, or when every consumer only communicates about his purchase with a few friends.1 As in our experimental setup, it typically exhibits two characteristics: (1) Localized direct communication: every decision maker only directly observes the choices and payoffs of an (often very small) subset of the population; (2) Path depen- dence of signal generation: the decision maker’s social contacts learn from their own social contacts, who in turn learn from other social contacts; consequently, the infor- mation or “signals” received by a decision maker potentially depend on the whole history of choices and payoffs among the population right up to the time of observa- tion. A major reference point of our framework is the social learning literature, which studies how social agents might learn from each other through observing each others’ actions. The classic social learning paradigm (Banerjee 1992; Bikhchandani et al. 1992) describes a process that involves the following: (1) agents make sequential, once-in-a-lifetime decisions, (2) all agents’ choices but not payoffs are publicly ob- served, and, in addition, (3) every agent holds an exogenously generated private signal. A main outcome of the classic paradigm is that rational agents may end up choosing the same, inefficient action regardless of their private signals. Recent de- velopments in social learning (e.g. Çelen and Kariv 2004a, 2005; Choi et al. 2008; 1An important example are opinion leaders (Katz and Lazarsfeld 1955), who are (typically category- specific) early adopters of new products and whose choice and experience of new products affect a circle of network contacts. Despite the popular image of opinion leaders influencing many followers, in empirical studies many opinion leaders are found to have direct communication with a few contacts only. 264 V. Mak, R. Zwick Smith and Sørensen 2008; Acemoglu et al. 2009, 2011; Callander and Hörner 2009; Guarino et al. 2011; Guarino and Jehiel 2013, and Larson 2011) typically keep the assumptions of exogenous private signal and observation of only choices, but limit how much each agent may observe about others’ actions; these studies focus on how agents’ actions might or might not converge when observation of others’ actions is incomplete. In the framework studied here, communication is highly localized but involves both choice and payoff, and there is no exogenous private signal. The payoff signal in our framework, being the realization of a random variable which only one agent may directly observe before decision making, can be compared to the private signal in social learning models; however, the generation of private signals in social learning is independent of the choices of (and signals received by) the agents, while pay- off signals in our framework are path dependent.2 This implies that our framework presents a different decision context compared with that in previous social learning experiments.3 As we shall report, our model and assumptions predict a convergence of choice among rational agents that is analogous to many social learning outcomes, but the behavioral regularities in our experiment—in which subjects’ choices deviate far from theoretical predictions—reveal characteristics that have not been featured in comparable previous studies. Localized direct communication and path dependence of signal generation mean that a subject in our setup has at most two “bits” of information (in addition to the prior) at the time of decision making: the choice and the payoff of his predecessor. Consequently, our research questions are: how would subjects’ decisions depend on the various “bits” of information in the above context? Does decision making follow equilibrium predictions? If not, do subjects exhibit base rate fallacy/conservatism bias by overweighting/underweighting informative payoff signals over the predecessor’s choices (cf. Goeree et al. 2007; Camerer 1995), exhibit social conformity (cf. Hung and Plott 2001 and Goeree and Yariv 2007) or rebelliousness (the opposite of con- formity)? Could there be other behavioral regularities that are not often observed in classic social learning experiments? As we shall report, we have indeed found one such behavioral regularity, namely a consistent preference among subjects for exper- imentation (preferring the lottery with potentially more information spillover value). In the remainder of this paper, we first describe the specific design of our experi- ment (Sect. 2) and then present an equilibrium analysis based on common knowledge of Bayesian rationality (Sect. 3). We find that decisions in equilibrium would be iden- tical from the first or second player onwards in all experimental conditions. However, 2An alternative perspective is that in social learning agents try to infer private information from observation of actions, but in our model agents infer private information from observing both actions and outcomes. This perspective invites comparison with the social experimentation paradigm studied by Bala and Goyal (1998) and Gale and Kariv (2003), among others. These models typically describe agents who make re- peated decisions on a multi-armed bandit problem and at the same time learn through observing a subset of other agents’ “experimentation outcomes” (choices and payoffs). 3Laboratory experimentation on social learning includes, among others, Anderson and Holt (1997), Huck and Oechssler (2000), Allsop and Hey (2000), Hung and Plott (2001), Çelen and Kariv (2004b), Kübler and Weizsäcker (2004), and Goeree et al. (2007). These studies are mainly concerned with the empirical observation of informational cascades or convergence of actions as predicted by theory, of which there is supporting laboratory evidence; see Anderson and Holt (2008) for an overall discussion. Experimenting and learning with localized direct communication 265 as we report in Sect. 4.1, complete adherence to equilibrium play is only occasion- ally observed in our experiment in short (four-player) queues and never observed in long (12-player) queues. Because our experimental data exhibit large deviations from equilibrium, we need to incorporate the possibility of erroneous moves when analyz- ing our data. Thus we adopt the approach of quantal response equilibrium (QRE) as we attempt to answer our research questions (see Kübler and Weizsäcker 2004 and Goeree et al. 2007 for examples of applying QRE in experiments on classic so- cial learning). Our estimations following this approach (Sect. 4.2) suggests consistent preference for experimentation across conditions as well as some evidence of conser- vatism bias, social conformity, and rebelliousness. Further analysis suggests that, in conditions in which experimentation is the a priori preferable action and also infor- mative to others, the preference for experimentation at the front of long queues is higher than at the front of short queues and higher than at the end of long queues. This offers some support for our surmise that subjects’ preference for experimenta- tion is due, in part, to an attempt to influence others behind them in the queue. We conclude our findings in Sect. 5. 2 Experimental design We carried out experimental sessions at the Hong Kong University of Science and Technology and the University of California, Riverside, with undergraduate partici- pants at the respective universities. A total of 400 subjects participated, including 88 in the Hong Kong (HK) sessions and 312 in the US sessions. All subjects volunteered to take part in an economic study with payoff contingent on performance. The experimental task can be described as follows: N players are queuing to play a “two arms slot machine game”. Each player, in his/her turn, is faced with a choice between playing one of two arms each of which may yield a high (HK$1304 in the Hong Kong sessions and US$20 in the US sessions) or low (HK$30 in the Hong Kong sessions and US$4 in the US sessions) payoff. The “left-arm lottery” (L) yields the high payoff with fixed, commonly known probability, q0. The “right-arm lottery” (R) is commonly known to be either of Type h or Type l with equal probability. Type h and Type l yield the high payoff with fixed, commonly known probability of q = 0.8 and q = 0.2, respectively. The type of lottery R is randomly determined before the first player in the queue makes his/her choice and, once determined, remains the same (but not revealed) to all N players. Lastly, before a player chooses a lottery, he/she observes the lottery choice and realized payoff of the player immediately ahead (if there is one) in the queue. Both N , the length of the queue, and q0, the probability that the left-arm lottery L yielded the high payoff, were known to all subjects at the beginning of the experimental session and remained the same throughout the session. Across conditions we manipulated N and q0 so that the experiment has a fully crossed 2 (N = 4,12) × 3 (q0 = 0.3,0.4,0.6) between-subject design. Subjects played in groups under a fixed matching scheme. The numbers of groups by session location and condition are summarized in Table 1. Each group consisted 4$1 US=HK$7.8. 266 V. Mak, R. Zwick (A) (B) Fig. 1 (A) Display at decision-making point for the player in position 3 in condition N = 4, q0 = 0.4, US sessions. (B) Feedback screen at the end of a typical game in condition N = 4, q0 = 0.4, US sessions Experimenting and learning with localized direct communication 267 Table 1 The experimental design, the average per subject payoff across all games in each condition (s.d. with group as unit of analysis in parentheses), and average per subject payoff in equilibrium play; the conversion rate US$1 = HK$7.8 is used for calculating the payoffs in US dollars for the Hong Kong sessions. Where the average payoff is significantly different from equilibrium prediction according to the sign test, it is marked by one or more asterisks (∗p &lt; 0.05; ∗∗p &lt; 0.01) Session location N q0 Number of groups Number of subjects Average payoff (s.d.) in US$ Average payoff in equilibrium in US$ Hong Kong 4 0.3 5 20 10.08 (0.53) 10.26 4 0.4 12 48 10.53 (0.67) 10.64 4 0.6 5 20 11.46 (0.56) 11.54 US 4 0.3 6 24 11.42 (0.26)∗ 12 4 0.4 12 48 11.71 (0.92)∗ 12.48 4 0.6 6 24 13.03 (0.69) 13.60 12 0.3 4 48 11.81 (0.17) 12 12 0.4 10 120 11.86 (0.42)∗ 12.59 12 0.6 4 48 12.70 (0.22) 13.60 of N subjects playing 48 games. In each game, subjects were randomly ordered into a “queue” of N ; we additionally imposed the constraint that each subject was in each position exactly 1/N of the time (i.e. 12 games when N = 4 and four games when N = 12). All decisions were made via networked computers; Fig. 1A showed a typical screenshot at decision point for a player in condition N = 4, q0 = 0.4 in the US sessions. At the end of every game, a feedback screen like Fig. 1B was shown to each player which showed: (a) The type of the right-arm lottery that had actually been used in that game i.e. whether its probability of yielding the high payoff was 0.8 or 0.2; (b) the player’s own choice of arm and lottery payoff; (c) the choices of arm and lottery payoffs of the players immediately ahead (if there was one) and immediately behind in the queue (if there was one).5 The instructions to US subjects when N = 4 and q0 = 0.4 are given in online Appendix A. At the end of each session, three games were chosen at random and subjects were paid according to their average payoffs in the three chosen games; in the US sessions, they were additionally paid a show-up fee of US$5. As it turns out, each subject in the Hong Kong session on average received a payment of HK$78 at the end of the experiment; the corresponding figure for the US sessions was US$20 including the show-up fee. Breakdown of average per subject payoffs over all the games by session location and condition is shown in Table 1. Note that the standard deviations in the table are calculated at the group level, at which preliminary statisti- cal analysis of data is conducted (see Sect. 4.1). Individual subjects in the experiment faced much more dramatic period-to-period fluctuations in realized payoff (HK$130 vs HK$30 in the Hong Kong sessions and US$20 vs US$4 in the US sessions) than the standard deviations in Table 1 suggest. Since subject payments were determined 5The provision of play information about the player immediately behind did not formally change the de- cision task, but might facilitate learning as the game proceeded—which is why we adopted this procedure. 268 V. Mak, R. Zwick by averaging payoffs from only three randomly chosen periods, subjects would fore- seeably be highly motivated to consider their decisions carefully.6 3 Equilibrium analysis In this section, we discuss equilibrium behavior in our experimental setup. We apply the concept of perfect Bayesian equilibrium in our analysis. As will be seen, equilib- rium play is distinguished by decisions becoming identical from the first or second player onwards in all experimental conditions. We first point out a basic observation. Suppose that, for a certain player t , the high payoff has Bernoulli utility u while the low payoff has Bernoulli utility v. Next suppose that, upon observing the choice and payoff of the player in front, player t holds a posterior belief of q with posterior mean q̂ . Then t chooses R if and only if7: [ q̂u + (1 − q̂)v] − [q0u + (1 − q0)v ] = (q̂ − q0)(u − v) ≥ 0. That is, the player chooses R if and only if q̂ ≥ q0, and her decision problem becomes simplified to a comparison between the posterior mean for q and the com- monly known probability q0. It can be seen that the particular values of u and v do not matter anymore as far as decision making goes and as long as u &gt; v. The present framing of the decision task thus avoids the problem of players with heterogeneous risk attitude having different Bernoulli utilities for the same monetary payoff. Within any game, we index players by position (counting from the player who first makes a decision), so that player t is in position t in the queue. Player 1 chooses R if the prior mean of q , i.e. 0.5 · 0.8 + 0.5 · 0.2 = 0.5, is higher than q0. Conversely, if q0 &gt; 0.5, player 1 chooses L. Hence, in equilibrium, player 1 chooses R when q0 = 0.3 or 0.4 but chooses L when q0 = 0.6 (the three levels of q0 in our design). Moreover, in the last case, player 2 has no additional information about R at the beginning of period 2 beyond the common prior, and so she also chooses L. As a result, all players choose L in equilibrium when q0 = 0.6 regardless of N . We now focus on q0 = 0.3 and 0.4 to specify equilibrium predictions for the exper- iment. For these two cases, we already know that player 1 chooses R in equilibrium. Player 2 is informed about this choice as well as the subsequent payoff. If player 1 earns the high payoff, then player 2’s posterior mean for q is: q̂ = 0.8 · 0.5 · 0.8 + 0.2 · 0.5 · 0.2 0.5 · 0.8 + 0.5 · 0.2 = 0.68, 6To offer further evidence for our claim, we calculate for each subject the standard deviation of his/her realized per period payoffs over the 48 periods that he/she played. We then average these standard devia- tions among subjects from the same session location, and obtain a mean of HK$49.7 for the Hong Kong sessions and US$8.00 for the US sessions, both of which are more than 1.5 times the low payoff in the lotteries. The standard deviations among the realized subject payments are about 60 % of the aforemen- tioned average standard deviations in both session locations, which are still substantial proportions of the realizable lottery payoffs. 7Our experimental parameters have been chosen to avoid tie breaking problems, which indeed never appear in our equilibrium analysis. It is thus conveniently assumed for exposition at this point that the tie breaking is in favor of choosing R. Experimenting and learning with localized direct communication 269 according to Bayes’ rule. But if player 1 earns the low payoff, then: q̂ = 0.8 · 0.5 · 0.2 + 0.2 · 0.5 · 0.8 0.5 · 0.2 + 0.5 · 0.8 = 0.32. Thus, when q0 = 0.3, whatever player 1’s payoff, player 2 should choose R in equi- librium, since q̂ &gt; 0.3 for both of the above values. This means that, in equilibrium, player 3 is not able to infer player 1’s choice just by observing player 2’s choice and payoff. Player 3 thus also chooses R (i.e. chooses according to the prior only) in equilibrium irrespective of player 2’s payoff. As a result, all players choose R in equilibrium when q0 = 0.3 regardless of N and payoff realizations. Finally, when q0 = 0.4, player 2 chooses R (L) in equilibrium if and only if player 1’s payoff from R is high (low), since 0.68 &gt; 0.4 &gt; 0.32. Player 3 is then able to infer from player 2’s choice the payoff of player 1. Thus, if player 2 chooses L, player 3 should follow and choose L too, and this trend is then repeated in the rest of the queue. If player 2 chooses R and earns the high payoff, player 3 is able to conclude that both players 1 and 2 earn the high payoff from R and should then choose R as well. Indeed, player 3’s posterior mean for q is: q̂ = 0.8 · 0.5 · 0.8 2 + 0.2 · 0.5 · 0.22 0.5 · 0.82 + 0.5 · 0.22 = 0.76 &gt; 0.4. If player 2 chooses R and earns the low payoff, player 3’s posterior mean for q is: q̂ = 0.8 · 0.5 · 0.8 · 0.2 + 0.2 · 0.5 · 0.2 · 0.8 0.5 · 0.8 · 0.2 + 0.5 · 0.2 · 0.8 = 0.5 &gt; 0.4, which means that player 3 should still choose R. To sum up, player 3 chooses R (L) in equilibrium play if and only if player 2 chooses R (L) in equilibrium play, regardless of player 2’s payoff. Player 4 is therefore not able to infer player 2’s payoff, but only able to infer player 1’s payoff (from choosing R) by virtue of observing the choice of player 3. This means that player 4, like player 3, should follow the choice of the player immediately in front regardless of the payoff observation. As a result, when q0 = 0.4, regardless of N , player 1 chooses R in equilibrium, player 2 chooses R (L) when player 1’s payoff is high (low), and all other players make the same choice as player 2. Summing up the results in this section (and noting that they are independent of the queue length N ), we present the following equilibrium proposition to be tested on experimental data: Proposition 1 In the perfect Bayesian equilibrium, regardless of queue length: (a) When q0 = 0.3, all players choose R; (b) When q0 = 0.4, (i) The player in the first position of the queue chooses R; (ii) The player in the second position chooses R (L) if the player in the first position chooses R and obtains the high (low) payoff, and; (iii) All other players make the same choice as the player in the second position. (c) When q0 = 0.6, all players choose L. 270 V. Mak, R. Zwick Table 2 Average proportion of games with complete adherence to equilibrium play (Proposition 1) in various segments of the queue (s.d. in parentheses), with group as the unit of analysis Segment over positions: q0 = 0.3 q0 = 0.4 q0 = 0.6 N = 4, Hong Kong 1 to 4 0.59 (0.19) 0.32 (0.15) 0.44 (0.21) N = 4, US 1 to 4 0.32 (0.13) 0.21 (0.09) 0.08 (0.04) N = 12, US 1 to 4 0.33 (0.11) 0.19 (0.07) 0.05 (0.01) 1 to 8 0 (0) 0 (0) 0 (0) 1 to 12 0 (0) 0 (0) 0 (0) 4 Experimental results and analysis 4.1 Preliminary analysis In this section, we present a brief preliminary analysis of our data that examines ad- herence to equilibrium play and other descriptive statistics. We always calculate de- scriptive statistics with data aggregated at the level of the group, and our preliminary data analysis also uses group as the unit of observation. This ensures independence among data points required for the analysis methods that we shall employ, such as ANOVA and Sign test. 4.1.1 Adherence to equilibrium play in queue segments We first examine adherence to equilibrium in the whole queue (i.e. segment of all four positions) in the N = 4 conditions, and in segments of the first four, eight, and all 12 positions respectively in the N = 12 conditions. For each segment and with each group of players, we calculate the proportion of played games in which there is com- plete adherence to equilibrium play within the segment. Table 2 lists the aggregated means and standard deviations across conditions separately for each session location (i.e. Hong Kong and the US). Evidently, it never occurred in any game in conditions with long queues (N = 12) that all subjects from positions one to eight (let alone the whole queue) made decisions according to equilibrium. Even in conditions with short queues (N = 4), or when only the decisions of the first four players in a long queue are examined, average proportion of games with complete adherence to equi- librium never exceeded 34 % in any condition in either location. Correspondingly, average subject payoffs as shown in Table 1 are lower than equilibrium predictions (calculated using Proposition 1)—although only the US sessions exhibit statistically significant differences from equilibrium payoffs according to the Sign test, as indi- cated in the same table. These highly aggregated statistics, moreover, mask the fact that individual subjects’ per period payoff could fluctuate dramatically from period to period (cf. discussion at the end of Sect. 2). Experimenting and learning with localized direct communication 271 To conclude, Proposition 1 is not supported by the data as complete adherence to equilibrium is non-existent in long queues and only occasionally observed in short queues. 4.1.2 Choice of lottery as a function of position and observation of predecessor We also carry out an analysis on subject decision statistics across session locations and conditions. The results are summarized in Table A1 in online Appendix B. We first define a decision situation by (i) the position of the decision maker in the queue, and (ii) the observed choice and payoff of his/her predecessor, if any. For each group of players and conditioned on each decision situation, we calculate the proportion of times that players in that decision situation chose the uncertain lottery R. Table A1 in online Appendix B also lists the equilibrium predictions from Proposition 1 in de- cision situations along equilibrium paths. As with our analysis in Sect. 4.1.1, Propo- sition 1 fails to predict subject behavior in many decision situations. Further analysis shows that overall subject behavior did not change significantly over the experimen- tal session,8 and hence there was little evidence of learning as subjects played more games. We further carry out ANOVA on this proportional frequency variable (again with group of players as the unit of analysis) for each session location at each level of N in a mixed 3 (q0 = 0.3,0.4,0.6) × N − 1 (position = 2,3, . . .N) × 2 (observed choice of the player in front = L,R) × 2 (observed payoff of the player in front = low, high) design, where q0 is a between subject factor and the others are within subject factors. We focus on significant effects with p &lt; 0.05. As it turns out, for N = 4, for both session locations, we find a significant main effect in q0 and in observed payoff, as well as a significant interaction between observed choice and ob- served payoff. We also find a significant interaction between q0 and observed choice for the Hong Kong data. For N = 12, we find a significant main effect in q0 as well as a significant main effect in observed choice and observed payoff. There are also significant interactions between observed choice and q0 and between observed choice and observed payoff. We find no other effects in these tests at p &lt; 0.1. These results convey a number of messages. Firstly, the significant main effects and interactions involving q0 confirm that subjects were acting in ways that are sensi- tive to the experimental conditions, and show that they could not be making decisions in a completely random manner that does not differentiate between contexts of de- cision making. Secondly, the lack of any significant effect in position suggests that subjects overall were not acting differently according to their position in the queue; this however does not preclude that there could be some subtle effects in position that could be detected in more in-depth analysis (see Sect. 4.2.6). Thirdly, there is some 8We first divide each experimental session into two blocks, one from Game 1 to 24 and the other from Game 25 to 48. For each group and conditioned on each decision situation, we calculate the proportion of times that a player in that decision situation chose the uncertain lottery R. We then perform paired comparison statistical tests on whether this variable changed across blocks separately for each decision situation across conditions and session locations. Sign test shows no significant differences at p &lt; 0.1 in all 213 comparisons except for two cases in q0 = 0.4 in the Hong Kong sessions, one case each in q0 = 0.4 and 0.6 in the US sessions with N = 4, and five cases in q0 = 0.4 in the US sessions with N = 12. 272 V. Mak, R. Zwick support that, when a subject observes that the player in front chooses R, he is more likely to choose R (L) if the player in front obtains a high (low) payoff, regardless of q0. This is consistent with a base rate fallacy bias, which we shall discuss in more de- tails in Sect. 4.2.2. Lastly, when the player in front chooses L, subjects seem overall aware that the subsequent observed payoff is uninformative and therefore disregard it. 4.2 Estimations Our preliminary analysis confirms that the experimental data exhibit large deviations from equilibrium. In general, the deviations may be caused by the presence of be- havioral regularities, as well as random noises in choices interpretable as erroneous moves. In this section, we report further data analysis aimed at identifying the be- havioral regularities upon filtering away the noisy components of our data. We adopt the approach of quantal response equilibrium (QRE) as we attempt to answer our research questions; similar applications can be found in Choi et al. (2008), Kübler and Weizsäcker (2004), and Goeree et al. (2007), among others. We shall first intro- duce the basic formulation of the QRE model for our set up, and then build on the basic model by adding modifications that allow us to test for hypothesized behavioral regularities. 4.2.1 The basic QRE model We first describe a basic model that is formulated according to standard QRE spec- ifications (McKelvey and Palfrey 1995, 1998). Under this model, we estimate only one non-negative parameter, namely the precision parameterλ. An interpretation of λ is that the higher it is, the less noisy or erroneous subjects’ choices were. When λ tends to zero, subject behavior approaches always choosing between the lotteries with equal probabilities; when λ tends to infinity, subject behavior approaches equilibrium play. Furthermore, we assume for this model (as well as subsequent extensions that incorporate various behavioral regularities) that subjects have rational expectations about others’ behavior. For further specifications, we first normalize the Bernoulli utility of the high payoff to one and that of the low payoff to zero. Then the expected utility of lottery L is always q0. Likewise, the expected utility of lottery R for any player should be the posterior mean of q (the probability of R yielding the high pay- off) given the player’s observations of the choice and payoff of the player in front, if any. Recall that we index players by position, so that player t is in position t in the queue. Denote the choice of player t as ct ∈ {L,R}. Because of our normalization, the expected utility of choosing R is simply 0.5 for player 1, since that player has only the prior to base his decision on; denote the probability of that player choosing R as P(c1 = R). Under the basic QRE model, this probability has the following logit expression: P(c1 = R) = exp(0.5λ) exp(0.5λ) + exp(λq0) . Experimenting and learning with localized direct communication 273 Denote as πt (s) the posterior mean of q for player t (t &gt; 1) given that the player’s observation of player t − 1 is st−1 = s ∈ {L,u, v}, where st−1 = L indicates that player t −1 chose L while st−1 = u(st−1 = v) indicates that player t −1 chose R and obtained the high (low) payoff. Conditioned on this observation only, the probability of player t choosing R can be written as P(ct = R|st−1 = s). Under the basic QRE model, this probability can be expressed as follows: P(ct = R|st−1 = s) = exp(λπt (s)) exp(λπt (s)) + exp(λq0) for any t &gt; 1. We next need to specify πt (s). Denote by ω ∈ {h, l} the actual type of R (i.e. the “state of the world”). For t = 1,2,3, . . . , denote as P(ct = R|ω = h)(P (ct = R|ω = l)) the probability that player t chooses R conditioned on R being actually of Type h (Type l), but otherwise unconditional on any specific history of choices or realized payoffs. We thus have: P(c1 = R|ω = h) = P(c1 = R|ω = l) = P(c1 = R). Moreover, for any t &gt; 1, P(ct = R|ω = h) = P(ct−1 = R|ω = h) · 0.8 · P(ct = R|st−1 = u) + P(ct−1 = R|ω = h) · 0.2 · P(ct = R|st−1 = v) + [1 − P(ct−1 = R|ω = h) ] P(ct = R|st−1 = L), while the corresponding expression for P(ct = R|ω = l) can be obtained by changing “h” to “l” and interchanging “0.2” and “0.8” in the right hand side of the above expression. In addition, by Bayes’ rule, for any t &gt; 1: πt (L) = 0.8 · 0.5 · [1 − P(ct−1 = R|ω = h)] + 0.2 · 0.5 · [1 − P(ct−1 = R|ω = l)] 0.5 · [1 − P(ct−1 = R|ω = h)] + 0.5 · [1 − P(ct−1 = R|ω = l)] = 1 − 0.8P(ct−1 = R|ω = h) − 0.2P(ct−1 = R|ω = l) 2 − P(ct−1 = R|ω = h) − P(ct−1 = R|ω = l) . Similarly, applications of Bayes’ rule yield: πt (u) = 0.64P(ct−1 = R|ω = h) + 0.04P(ct−1 = R|ω = l) 0.8P(ct−1 = R|ω = h) + 0.2P(ct−1 = R|ω = l) , and πt (v) = 0.16 · [P(ct−1 = R|ω = h) + P(ct−1 = R|ω = l)] 0.2P(ct−1 = R|ω = h) + 0.8P(ct−1 = R|ω = l) . The expressions for P(c1 = R), P(ct = R|st−1 = s), P(ct = R|ω = h), P(ct = R|ω = l), and πt (s) together complete the formulation of the basic QRE model. Conditioned on a hypothesized value of λ, we can generate the numerical values of P(c1 = R|λ) and P(c1 = L|λ) = 1 − P(c1 = R|λ), as well as any P(ct = R|st−1 = s, λ) and P(ct = L|st−1 = s, λ) = 1 − P(ct = R|st−1 = s, λ) for any t &gt; 1, using the expressions above. This allows us to use the maximum likelihood method to find 274 V. Mak, R. Zwick out the optimal value of λ that best describes the experimental data: specifically, in a game with queue length N , the likelihood of the players making a sequence of choices {c1, c2, . . . , ct · · · cN } given λ and the realized payoffs, is: P(c1|λ) · N∏ t=2 P(ct |st−1, λ), and the likelihood of observing a set of M sequences of choices is: M∏ m=1 [ P(c1,m|λ) Nm∏ t=2 P(ct,m|st−1,m, λ) ] , where ct,m and st−1,m denote the choice and observation of player t in sequence m, respectively, and Nm is the queue length of the game in sequence m. For expositional convenience, we present in Table 3 the estimations for the com- plete pooled data and for the observations at each level of q0 pooling data from ses- sions in different locations and with different queue lengths. Separate estimations by session location and experimental conditions are listed in Table A2 in online Ap- pendix B; the more detailed estimations in general support the conclusions from the estimations in Table 3. We also list the maximized log likelihood (logL), the Akaike information criterion (AIC), and the Bayesian information criterion (BIC), for ev- ery estimation result. The AIC and BIC are measures that aid model selection, and are motivated by the tradeoff between improving log likelihood and the number of parameters. They are defined as follow: AIC = 2k − 2 logL, and BIC = k logM − 2 logL, where k = number of estimated parameters, and M = number of observations. Mod- els with lower AIC are to be considered more preferable to models with higher AIC, and similarly for BIC. We shall use these measures when comparing between com- bined models in Sect. 4.2.5. A prevalent result is that the estimated λ is always significantly greater than zero; this is the case not only with the basic QRE model but also with all other models to be introduced in this section. In other words, subjects were never behaving as if they always chose between the lotteries with equal (i.e. 50 %) probabilities. This is consistent with our preliminary analysis reported in Sect. 4.1. In the following paragraphs, we outline a number of hypothesized behavioral reg- ularities that might appear in our experimental data. In each case, we present a mod- ification of the basic QRE model that incorporates the bias and offers a test of it through the estimation of one or more new parameters. Table 3 lists the estimation results for the various models to be discussed. 4.2.2 Incorporating base rate fallacy/conservatism bias There is evidence that subjects in social learning experiments tend to overweight their private signal at the expense of public information (see e.g. Çelen and Kariv 2004b; Experimenting and learning with localized direct communication 275 Table 3 Parameter estimates for the models in Sects. 4.2.1–4.2.5 (standard errors in parentheses) at each level of q0. Also presented are estimations for the pooled data. All estimations of λ are significantly differ- ent from zero at p &lt; 0.01. Where an estimation of a behavioral regularity-related parameter is significantly different from the null hypothesis prediction (α = 1 and β = γ = δ = 0), it is marked by one or more as- terisks (∗p &lt; 0.05; ∗∗p &lt; 0.01). Where the AIC or BIC of a model is among the three lowest values in the same column, the index is presented in bold # Obs. q0 = 0.3 4416 q0 = 0.4 10368 q0 = 0.6 4416 Pooled 19200 Basic QRE λ 6.55 (0.21)∗∗ 6.33 (0.15)∗∗ 4.06 (0.22)∗∗ 5.94 (0.10)∗∗ Log L −2135.0 −5950.3 −2887.5 −11014 AIC 4272.0 11902.6 5777.0 22031 BIC 4278.4 11909.9 5783.4 22039 BRF λ 6.72 (0.21)∗∗ 7.48 (0.21)∗∗ 3.50 (0.31)∗∗ 6.50 (0.13)∗∗ α 0.83 (0.053)∗∗ 0.67 (0.033)∗∗ 1.46 (0.26) 0.77 (0.027)∗∗ Log L −2130.6 −5916.0 −2884.2 −10988 AIC 4265.2 11835.9 5772.3 21980 BIC 4278.0 11850.4 5785.1 21996 PE λ 5.56 (0.27)∗∗ 5.55 (0.16)∗∗ 5.17 (0.29)∗∗ 5.52 (0.11)∗∗ β 0.053 (0.012)∗∗ 0.056 (0.0055)∗∗ 0.048 (0.0072)∗∗ 0.055 (0.0036)∗∗ Log L −2120.5 −5878.0 −2871.7 −10871.1 AIC 4244.9 11759.9 5747.4 21747 BIC 4257.7 11774.4 5760.1 21762 SC λ 6.41 (0.26)∗∗ 6.16 (0.17)∗∗ 4.47 (0.28)∗∗ 5.79 (0.12)∗∗ γ 0.0062 (0.0097) 0.026 (0.0060)∗∗ 0.010 (0.012)∗∗ 0.041 (0.0048)∗∗ δ −0.081 (0.014)∗∗ −0.059 (0.0069)∗∗ −0.0067 (0.013) −0.055 (0.0051)∗∗ Log L −2115.9 −5906.7 −2852.4 −10917 AIC 4237.7 11819.4 5710.8 21840 BIC 4256.9 11841.1 5730.0 21864 BRF-PE λ 3.63 (0.18)∗∗ 4.08 (0.38)∗∗ 10.06 (1.11)∗∗ 5.37 (0.15)∗∗ α 10.42 (188.4) 1.85 (0.43)∗ 0.40 (0.064)∗∗ 1.074 (0.052) β 0.20 (0.017)∗∗ 0.12 (0.022)∗∗ 0.071 (0.0043)∗∗ 0.059 (0.0048)∗∗ Log L −2107.2 −5869.4 −2858.7 −10870 AIC 4220.4 11744.8 5723.4 21746 BIC 4239.6 11766.5 5742.6 21770 BRF-SC λ 7.38 (0.47)∗∗ 9.30 (0.49)∗∗ 4.37 (0.71)∗∗ 6.00 (0.17)∗∗ α 0.76 (0.077)∗∗ 0.54 (0.039)∗∗ 1.035 (0.25) 0.924 (0.042) γ −0.031 (0.015)∗ −0.033 (0.0066)∗∗ 0.10 (0.014)∗∗ 0.034 (0.0060)∗∗ δ −0.052 (0.016)∗∗ −0.025 (0.0059)∗∗ −0.0043 (0.022) −0.053 (0.0051)∗∗ Log L −2112.6 −5881.9 −2852.4 −10916 AIC 4233.2 11771.7 5712.8 21839 BIC 4258.7 11800.7 5738.4 21870 276 V. Mak, R. Zwick Goeree et al. 2007). This behavioral regularity, known as “base rate fallacy” (BRF), would be manifested in our setting as an overweighting of the observed payoff of the player in front if that player chooses the lottery R, at the expense of the observation of the choice itself. But because of path dependence, a choice observation of R could be almost as salient as the related payoff observation, as the generation of the latter must be preceded by the former. Moreover, because of localized direct communication, the choice observation in our setting could be perceived as a “private signal” like the pay- off observation. Thus it is not obvious that base rate fallacy must have as significant a presence in our experiment as in previous social learning experiments. Instead, as noted in Goeree et al. (2007, footnote 36; see also Camerer 1995, pp. 601–602, for a more general discussion), an underweighting bias, which can be called “conservatism bias”, is also possible even though it has less support in the experimental literature. To proceed, we estimate a “BRF model” that differs from the basic QRE model in the following expected utility expressions for t &gt; 1: πt (u) = 0.8 · 0.5 · P(ct−1 = R|ω = h) · 0.8 α + 0.2 · 0.5 · P(ct−1 = R|ω = l) · 0.2α 0.5 · P(ct−1 = R|ω = h) · 0.8α + 0.5 · P(ct−1 = R|ω = l) · 0.2α = 0.8 1+αP (ct−1 = R|ω = h) + 0.21+αP (ct−1 = R|ω = l) 0.8αP (ct−1 = R|ω = h) + 0.2αP (ct−1 = R|ω = l) , and πt (v) = 0.8 · 0.5 · P(ct−1 = R|ω = h) · 0.2 α + 0.2 · 0.5 · P(ct−1 = R|ω = l) · 0.8α 0.5 · P(ct−1 = R|ω = h) · 0.2α + 0.5 · P(ct−1 = R|ω = l) · 0.8α = 0.8(0.2 α)P (ct−1 = R|ω = h) + 0.2(0.8α)P (ct−1 = R|ω = l) 0.2αP (ct−1 = R|ω = h) + 0.8αP (ct−1 = R|ω = l) , where α is a new parameter to be estimated along with the precision parameter λ. Ev- idence of BRF (conservatism) would correspond to estimation results showing that α is significantly larger (less) than one, indicating that the payoff signal from an ob- served choice of R is overweighted (underweighted) relative to the choice observation itself. The estimation results of the BRF model are shown in Table 3. They indicate that, wherever α is significantly different from the null hypothesis prediction of one, it is significantly less than one. That is, in these cases, subject behavior exhibited sig- nificant conservatism bias, which is opposite to the observations of previous social learning experiments such as Goeree et al. (2007). This apparent bias could be driven by a general preference for experimentation with which subjects had an overall incli- nation to choose R regardless of their observations. Since the parameter α in the BRF model only had an impact on decisions when the choice of the player in front was R, a conservatism bias could be confounded with preference for experimentation. 4.2.3 Incorporating preference for experimentation Our next hypothesized behavioral regularity, called preference for experimentation or PE, relates to a tendency to experiment by choosing the uncertain lottery R. PE could be a purely psychological inclination that is not socially related. Or, possibly, it could be driven by an inclination to influence the players behind, as R has potentially more Experimenting and learning with localized direct communication 277 information spillover value than L (see Sect. 4.2.6). This behavioral regularity can be captured by a “PE model” that differs from the basic QRE model in the following probabilistic choice function expressions: P(c1 = R) = exp(0.5λ) exp(0.5λ) + exp(λ(q0 − β)) , and P(ct = R|st−1 = s) = exp(λπt (s)) exp(λπt (s)) + exp(λ(q0 − β)) for any t &gt; 1 and any s ∈ {L,u, v}, where β is a new parameter to be estimated along with λ. If the estimation of this parameter is significantly larger than zero, then the result suggests evidence of PE among subjects.9 The estimation results of the PE model are shown in Table 3. All estimates of β in the table are significantly larger than the null hypothesis prediction of zero. Note that, when q0 = 0.3, R is often the rational choice, and PE as a behavioral regularity might be less detectable than at higher values of q0; yet our estimations yield evidence of PE even when q0 = 0.3. Thus we conclude that, in our experiment, subject behavior exhibited consistent and significant preference for experimentation. In Sect. 4.2.6, we report additional analysis that gives clues regarding why subjects exhibited PE in our experiment. 4.2.4 Incorporating social conformity/rebelliousness Our third hypothesized behavioral regularity, called social conformity or SC, relates to the tendency to follow the choice of the player in front (see e.g. Hung and Plott 2001 and Goeree and Yariv 2007). This can be captured by an “SC model” that differs from the basic QRE model in the following probabilistic choice function expressions for any t &gt; 1: P(ct = R|st−1 = s) = exp(λ(πt (s) + γ )) exp(λ(πt (s) + γ )) + exp(λq0) when s ∈ {u,v}, and P(ct = R|st−1 = L) = exp(λ(πt (L))) exp(λ(πt (L))) + exp(λ(q0 + δ)) , where γ and δ are new parameters that measure the inclination of the players to conform to an observed choice of R and L, respectively, and are to be estimated along with λ. If any of these new parameters is significantly larger than zero, then the data exhibit evidence of conformity. The estimation results for this model are given in Table 3; in the estimations we do not preclude that these parameters could be negative, that is, we do not preclude the presence of anti-conformity or “rebelliousness”. 9We do not preclude the possibility of a preference for not experimenting among subjects, in which case the estimated β would be significantly less than zero. 278 V. Mak, R. Zwick The estimation results indicate that, wherever γ is significantly different from the null hypothesis prediction of zero, it is significantly positive. But wherever δ is sig- nificantly different from the null hypothesis prediction of zero, it is significantly less than zero. Thus we conclude that, in our experiment, subject behavior exhibited con- sistent and significant conformity to an observed choice of R but rebelliousness to an observed choice of L, with a stronger presence of the latter than the former. The disparity in apparent conforming/rebellious behavior when the observed choice of the player in front is R/L throws in doubt whether social conformity would be the most parsimonious construct that explains this set of estimations. Instead, the results can be re-interpreted as suggesting different degrees of preference for exper- imentation that depend on the observed choice. As with the BRF model, we shall remain agnostic on these interpretations and only claim that our estimations suggest possible evidence of social conformity among subjects. 4.2.5 Estimation results of combined models We also carry out estimations of a number of combinations of the models introduced earlier. In the last two rows in Table 3, we present estimations of a model combining base rate fallacy and preference for experimentation (the BRF-PE model) and a model combining base rate fallacy and social conformity (the BRF-SC model).10 The BRF- PE estimations exhibit a significant presence of preference for experimentation in all cases, a significant presence of conservatism bias when q0 = 0.6, and a significant presence of base rate fallacy when q0 = 0.4. The presence of PE further strengthens our previous conclusion from estimating the PE model that subjects in our experiment did exhibit preference for experimentation. The apparent presence of base rate fallacy when q0 = 0.4, meanwhile, is not consistent with the presence of conservatism in all other relevant estimations; moreover, when the data from different session locations and under different experimental conditions are estimated separately (see Table A2 in online Appendix B), all estimations of α are significantly less than one, while the presence of PE remains strong. The BRF-SC estimations are less conclusive but suggest evidence of rebellious- ness when the observed choice is L, conformity when the observed choice is R and q0 = 0.6, as well as conservatism bias. The AICs and BICs of the different models that we have estimated are generally close to each other (controlling for condition and session location); but the better models by any one of these measures often include a significant presence of PE (i.e. a significantly positive β). Given the preceding discussion, a parsimonious interpreta- tion of these results is that they all point to a general preference for experimentation among subjects. This further motivates us to investigate what could be the driving motivations behind such preference, which is discussed in the following section. 10In addition, we have estimated models that involve both PE and SC (not presented here), which yield similar overall conclusions as reported in this section. Note that, except for the choice of player 1, the PE model is a special case of the SC model with β = γ = −δ. Indeed, if subjects exhibited equal conformity to an observed choice of R regardless of payoff but rebelliousness of equal magnitude to an observed choice of L, then their behavior would be indistinguishable from a preference for experimentation except for the choices of players in position 1. Experimenting and learning with localized direct communication 279 4.2.6 Additional estimations Our estimation results consistently highlight PE as an important factor in subject behavior. In this section, we report additional exploratory analysis that offers some clues to understanding the reasons behind observing PE in our experiment. Dependence of preference for experimentation on position: Could PE be driven by the desire to influence? The path dependent nature of signal generation in our exper- iment has an interesting potential implication. A decision maker who is not the last in the queue would be aware that, if he chooses R, the uncertain lottery, the choice would generate an informative payoff signal that potentially influences the choice of the player behind, which may have further repercussions to players further down the queue. But if he chooses L, the known lottery, only his choice might ever have an influence. In a situation where players often make erroneous moves (as suggested by our data) the generation of informative payoff signal is particularly useful. As such, a choice of the uncertain lottery has potentially more information spillover value than the known lottery. Consequently, a subject may exhibit PE because of a desire to influence others. In fact, there is empirical evidence that people who often influ- ence others’ new product choices tend to be adventurous in trying out new products (Rogers 2003). While an adventurous personality might be pivotal in making the per- son become early adopters in the first place, we consider the following possibility: once a person has been put in the role of an early adopter either by expectation of others or by some exogenous circumstances (such as the randomizing arrangement in our experiment), the mere fact that he has the potential to influence others might lead to a higher preference for experimentation. We explore this possibility in the following analysis. We first analyze how PE may depend on the position of the player. Our analysis is motivated by the following question: is preference for experimentation an intrinsic tendency, or is it driven by the desire to influence? To be specific, even if the estimated β is significantly larger than zero, it might be due to: (1) an intrinsic psychological tendency that is not socially motivated (e.g. curiosity), and/or (2) an inclination to influence the players behind. As an attempt to tease out these two possibilities, we observe that PE would be independent of the player’s position if it is only driven by (1), but might be position dependent if (2) is a major driver. This is because the desire to influence should increase with the subject’s own perceived potential to in- fluence others, for which the number of players behind serves as a good proxy. The implication is that PE would be stronger towards the front of the queue. Therefore, we estimate the following “PE-φ model” that differs from the basic QRE model in the follow probabilistic choice functions, and is to be applied only to conditions with queues of 12 players: P(c1 = R) = exp(0.5λ) exp(0.5λ) + exp(λ(q0 − β)) , P (ct = R|st−1 = s) = exp(λπt (s)) exp(λπt (s)) + exp(λ(q0 − β)) when t = 2 to 4; P(ct = R|st−1 = s) = exp(λπt (s)) exp(λπt (s)) + exp(λ(q0 − β(1 − φ1))) when t = 5 to 8, 280 V. Mak, R. Zwick and P(ct = R|st−1 = s) = exp(λπt (s)) exp(λπt (s)) + exp(λ(q0 − β(1 − φ2))) when t = 9 to 12, where β , φ1, and φ2 are parameters to be estimated along with λ, and we should obtain β &gt; 0, 1 &gt; φ1 &gt; 0, and 1 &gt; φ2 &gt; 0, if the desire to influence was a major driver of PE. The estimation results are given in Table 4. They are consistent with previous estimates in that β is significantly larger than zero when q0 = 0.4 and 0.6. Moreover, as discussed previously, β is expectedly not significantly different from zero when q0 = 0.3 because the rational choice in that condition tends to be in favor of R. Meanwhile, the estimates of the new parameter φ2 is significantly different from zero only when q0 = 0.4, in which case its value of 0.38 is between zero and one and significantly larger than zero (the estimate of 0.28 for φ1 is marginally significantly larger than zero, p ≈ .08). These results lead us to conclude that subject behavior exhibited a desire to influence when q0 = 0.4. Yet another way to tease out the two possible drivers of PE is to compare the PE among the first four players in queues of 12 and the players in queues of four, controlling for the session location. If the desire to influence was a major driver of PE, we should observe a stronger presence of PE among the first four players in queues of 12 compared with their counterparts in queues of four. Therefore, we estimate a “PE- κ model” that differs from the basic QRE model in the follow probabilistic choice functions, and is to be applied only to the US data: P(c1 = R) = exp(0.5λ) exp(0.5λ) + exp(λ(q0 − β)) when N = 4, and P(ct = R|st−1 = s) = exp(λπt (s)) exp(λπt (s)) + exp(λ(q0 − β)) when t &gt; 1 and N = 4; P(c1 = R) = exp(0.5λ) exp(0.5λ) + exp(λ(q0 − β(1 + κ))) when N = 12, and P(ct = R|st−1 = s) = exp(λπt (s)) exp(λπt (s)) + exp(λ(q0 − β(1 + κ))) when t &gt; 1 and N = 12, where β and κ are parameters to be estimated along with λ, and we should obtain β &gt; 0 and κ &gt; 0 if the desire to influence is a major driver of PE. The estimation results are given in Table 4. They show that β is in general significantly larger than zero while κ is significantly larger than zero only when q0 = 0.4 and otherwise not significantly different from zero. To conclude, both sets of estimations offer some support that preference for exper- imentation is dependent on position when q0 = 0.4, and thus PE seems to be at least partly driven by a desire to influence in that condition. This leads to the question why q0 = 0.4 is the only condition that shows evidence of the desire to influence. Here we observe that, when q0 = 0.4, R is the a priori preferable choice (as opposed to q0 = 0.6) and at the same time informative to others (as opposed to q0 = 0.3), as a high/low payoff from choosing R could lead to opposite preference ordering between Experimenting and learning with localized direct communication 281 Table 4 Parameter estimates for the models in Sect. 4.2.6 (standard errors in parentheses). All estimations of λ are significantly different from zero at p &lt; 0.01. Where an estimation of a behavioral regularity- related parameter is significantly different from the null hypothesis prediction (i.e. β = φ1 = φ2 = κ = σ1 = σ2 = σ = 0), it is marked by one or more asterisks (∗p &lt; 0.05; ∗∗p &lt; 0.01) # Obs. US sessions, N = 12 q0 = 0.3 2304 q0 = 0.4 5760 q0 = 0.6 2304 PE-φ λ 5.17 (0.34) 5.14 (0.21) 5.28 (0.36) β 0.016 (0.021) 0.080 (0.012)∗∗ 0.11 (0.015)∗∗ φ1 −1.06 (2.52) 0.28 (0.16) 0.20 (0.18) φ2 −0.42 (1.89) 0.38 (0.16)∗ 0.15 (0.18) Log L −1191.2 −3291.5 −1492.4 AIC 2390.4 6591.0 2992.8 BIC 2413.4 6617.6 3015.8 PE-φ-σ λ 5.13 (0.34) 5.15 (0.21) 5.26 (0.36) β 0.018 (0.021) 0.080 (0.012)∗∗ 0.11 (0.015)∗∗ φ1 2.59 (3.26) 1.04 (0.33) ∗∗ 0.38 (0.36) σ1 −1.47 (1.93) −0.30 (0.13)∗ −0.07 (0.13) φ2 2.58 (3.23) 0.58 (0.33) −0.45 (0.40) σ2 −1.21 (1.67) −0.078 (0.12) 0.24 (0.13) Log L −1189.1 −3288.2 −1490.4 AIC 2390.2 6588.4 2992.8 BIC 2424.7 6628.4 3027.3 # Obs. US sessions, N = 4 and 12, positions 1–4 only q0 = 0.3 1920 q0 = 0.4 4224 q0 = 0.6 1920 PE-κ λ 5.59 (0.41) 4.55 (0.27) 3.41 (0.42) β 0.033 (0.019) 0.043 (0.012)∗∗ 0.088 (0.018)∗∗ κ −0.86 (0.59) 1.35 (0.65)∗ 0.21 (0.35) Log L −977.8 −2519.4 −1298.5 AIC 1961.6 5044.8 2603.0 BIC 1978.3 5063.9 2619.7 PE-κ-σ λ 5.56 (0.41) 4.55 (0.27) 3.39 (0.42) β 0.033 (0.019) 0.043 (0.012)∗∗ 0.088 (0.018)∗∗ κ −2.78 (1.66) 0.073 (0.72) 1.05 (0.76) σ 0.79 (0.60) 0.52 (0.28) −0.34 (0.24) Log L −976.2 −2517.2 −1297.4 AIC 1960.4 5042.4 2602.8 BIC 1982.6 5067.8 2625.0 282 V. Mak, R. Zwick the two lotteries for the player behind (as would happen in equilibrium in position 2). Hence our surmise is that subjects become especially prone to the potential of exper- imentation to influence others when q0 = 0.4, compared with when q0 is 0.3 or 0.6. While this surmise is consistent with our data and our estimations, more research is needed to confirm its validity. Whenever preference for experimentation is driven by the desire to influence, is it motivated by altruism or expectations of reciprocity? Whenever PE appears to be partly driven by the desire to influence, two deeper motivations might lie behind its occurrence, which we explore in a set of further analysis. The first motivation is an intrinsic, unconditional tendency to act benevolently toward others, which we label altruism (see e.g. Andreoni and Miller 2008). In the present case, choosing R benefits the player immediately behind in the form of a payoff observation that may have an impact on the latter’s choice, which may in turn affect the choices of players further back. Another possible motivation springs from the fact that subjects are randomly placed in different positions of the queue across games (which can be seen as a styl- ization of the fact that, in real-life situations, one person could be an early purchaser in one category of products while his friend could be an early purchaser in another category). A subject i who is an early decision maker in a certain game might experi- ment to benefit the player immediately behind hoping that, when that player becomes an earlier decision maker than him in another game, the player would experiment and benefit i in return. We label this motivation expectations of reciprocity (see e.g. Hoffman et al. 2008); if it is predominant over altruism, there should be a time trend with which the position dependence of the preference for experimentation decreases towards the end of the session, since the opportunities for the players behind to re- ciprocate decrease with the number of games left. On the other hand, if altruism is predominant over expectations of reciprocity, there should be a lack of time trend. Hence we carry out a test of time trend in the position dependence of the preference for experimentation. To do so, we group observations into four blocks of 12 games each, so that block d (d = 1,2,3,4) consists of Game 12(d − 1) + 1 to Game 12d . We then estimate the following models: 1. A “PE-φ-σ model” that is identical to the PE-φ model except that φ1 is replaced by φ1 + σ1d and φ2 is replaced by φ2 + σ2d in the probabilistic choice functions, and 2. A “PE-κ-σ model” that is identical to the PE-κ model except that κ is replaced by κ + σd in the probabilistic choice functions, where d represents the block in which the data point was observed, and σ1, σ2, and σ are additional parameters to be estimated. If expectations of reciprocity were a major motivation behind the desire to influence, we should obtain estimates of σ1, σ2, and σ that are significantly less than zero. The estimation results are given in Table 4. In principle, we should only look at q0 = 0.4 as this is the only case where the simpler versions of these models identify the desire to influence as a major driver of PE. But for completeness, we also present corresponding estimation results for other values of q0. As it turns out, most estimations of σ1, σ2, and σ are not significantly different from zero except for σ1 when q0 = 0.4. Thus it seems that both altruism Experimenting and learning with localized direct communication 283 and expectations of reciprocity were drivers of the desire to influence, especially the former. 5 Concluding remarks Our study contributes to the experimental literature by offering a simple laboratory operationalization of a type of experimenting and learning behavior among con- sumers. We also contribute to recent works in social learning (e.g. Goeree et al. 2007) and more generally, interpersonal impacts in Bayesian decision making (see e.g. Boyce et al. 2009; Grosse 2010), by finding evidence of conservatism bias and conformity/rebelliousness in a comparable context. Even more importantly, we find evidence of consistent preference for experimentation across conditions. This points to a new, rarely explored phenomenon, and could lead to a deeper understanding of how interpersonal communication shapes economic behavior. In addition, in some of the treatment conditions, we find some support for our surmise that subjects’ prefer- ence for experimentation is due, in part, to an attempt to influence others behind them in the queue. Our setting can be categorized under social learning, but differs from the classic social learning paradigm in that direct communication is localized and private signal generation is path dependent. Systematic experimentation comparing behavior in the present setup and that in classic social learning could reveal more insights into how people react differently when communication is entirely localized vs when it has a global component, as well as when signal generation is entirely path dependent vs when private signals are exogenously generated. For example, one of our findings is that, while subject behavior deviated greatly from equilibrium, social conformity or rebelliousness might have some impact on behavior. This could be compared with Hung and Plott (2001) who showed that social conformity could not explain informa- tional cascades in the classic social learning paradigm, while subjects did act accord- ing to equilibrium to form a cascade in many instances. Our findings could also be compared with Goeree and Yariv (2007), who found a great deal of non-equilibrium conformist behavior in their setting, in which subjects could choose between un- informative observation of others’ actions and an informative private signal. More rigorous experimentation could be of use to find out why and how subject behavior differs under these different contexts of interactions. References Acemoglu, D., Ozdaglar, A., &amp; ParandehGheibi, A. (2009). Spread of (mis)information in social networks. Games and Economic Behavior, 70, 194–227. Acemoglu, D., Dahleh, M. A., Lobel, I., &amp; Ozdaglar, A. (2011). Bayesian learning in social networks. Review of Economic Studies, 78, 1201–1236. Allsop, L., &amp; Hey, J. D. (2000). Two experiments to test a model of herd behavior. Experimental Eco- nomics, 3(2), 121–136. Anderson, L. R., &amp; Holt, C. A. (1997). Information cascades in the laboratory. The American Economic Review, 87(5), 847–862. Anderson, L. R., &amp; Holt, C. A. (2008). Information cascade experiments. In V. Smith &amp; C. A. Plott (Eds.), Handbook of experimental economics results. Amsterdam: North-Holland. 284 V. Mak, R. Zwick Andreoni, J., &amp; Miller, J. H. (2008). Analyzing choice with revealed preference: is altruism rational? In C. R. Plott &amp; V. L. Smith (Eds.), Handbook of experimental economics results. Amsterdam: North- Holland. Bala, V., &amp; Goyal, S. (1998). Learning from neighbours. Review of Economic Studies, 65(3), 595–621. Banerjee, A. (1992). A simple model of herd behavior. The Quarterly Journal of Economics, 107(3), 797– 817. Bikhchandani, S., Hirshleifer, D., &amp; Welch, I. (1992). A theory of fads, fashion, custom, and cultural change as informational cascades. Journal of Political Economy, 100(5), 992–1026. Boyce, J. R., Bruner, D. M., &amp; McKee, M. (2009). Be my guinea pig: information spillovers in a one-armed bandit game. Working paper. Callander, S., &amp; Hörner, J. (2009). The wisdom of the minority. Journal of Economic Theory, 144, 1421– 1439.e2. Camerer, C. (1995). Individual decision making. In J. Kagel &amp; A. Roth (Eds.), Handbook of experimental economics. Princeton: Princeton University Press. Çelen, B., &amp; Kariv, S. (2004a). Observational learning under imperfect information. Games and Economic Behavior, 47, 72–86. Çelen, B., &amp; Kariv, S. (2004b). Distinguishing informational cascades from herd behavior in the laboratory. The American Economic Review, 94(3), 484–498. Çelen, B., &amp; Kariv, S. (2005). An experimental test of observational learning under imperfect information. Economic Theory, 26(3), 677–699. Choi, S., Gale, D., &amp; Kariv, S. (2008). Social learning in networks: a quantal response equilibrium analysis of experimental data. Journal of Economic Theory, 143(1), 302–330. Gale, D., &amp; Kariv, S. (2003). Bayesian learning in social networks. Games and Economic Behavior, 45, 329–346. Goeree, J. K., Palfrey, T. R., Rogers, B. W., &amp; McKelvey, R. D. (2007). Self-correcting information cas- cades. Review of Economic Studies, 74, 733–762. Goeree, J. K., &amp; Yariv, L. (2007). Conformity in the lab. Working paper, California Institute of Technology. Grosse, N. D. (2010). Experimenting with strategic experimentation: risk taking, neighborhood size and network structure. Jena Economic Research Papers, ISSN, 1864–7057. Guarino, A., Harmgart, H., &amp; Huck, S. (2011). Aggregate information cascades. Games and Economic Behavior, 73, 167–185. Guarino, A., &amp; Jehiel, P. (2013). Social learning with coarse inference. American Economic Journal: Microeconomics, 5, 147–174. doi:101257/mic.5.1.147 Hoffman, E., McCabe, K., &amp; Smith, V. (2008). Reciprocity in ultimatum and dictator games: an introduc- tion. In C. R. Plott &amp; V. L. Smith (Eds.), Handbook of experimental economics results. Amsterdam: North-Holland. Huck, S., &amp; Oechssler, J. (2000). Informational cascades in the laboratory: do they occur for the right reasons? Journal of Economic Psychology, 21, 661–671. Hung, A., &amp; Plott, C. R. (2001). Information cascades: replication and an extension to majority rule and conformity-rewarding institutions. The American Economic Review, 91(5), 617–628. Katz, E., &amp; Lazarsfeld, P. F. (1955). Personal influence: the part played by people in the flow of mass communications. New York: Free Press. Kübler, D., &amp; Weizsäcker, G. (2004). Limited depth of reasoning and failure of cascade formation in the laboratory. Review of Economic Studies, 71, 425–441. Larson, N. (2011). Inertia in social learning from a summary statistic. Working paper. McKelvey, R. D., &amp; Palfrey, T. R. (1995). Quantal response equilibria for normal-form games. Games and Economic Behavior, 10, 6–38. McKelvey, R. D., &amp; Palfrey, T. R. (1998). Quantal response equilibria for extensive form games. Experi- mental Economics, 1, 9–41. Rogers, E. M. (2003). Diffusion of innovations. New York: Free Press. Smith, L., &amp; Sørensen, P. N. (2008). Informational herding and optimal experimentation. Working paper.</p>
        </div>
      </section>
    </article>
  </body>
</html>
