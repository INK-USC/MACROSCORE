{
    "abstractText": "We investigated the effects of language on vision by focusing on a well-known problem: the binding and maintenance of color-location conjunctions. Four-yearolds performed a task in which they saw a target (e.g., a split square, red on the left and green on the right) followed by a brief delay and thenwere asked to find the target in an array including the target, its reflection (e.g., red on the right and green on the left), and a square with a different geometric split. Errors were overwhelmingly reflections. This finding shows that the children failed to maintain color-location conjunctions. Performance improved when targets were accompanied by sentences specifying color and direction (e.g., \u2018\u2018the red is on the left\u2019\u2019), but not when the conjunction was highlighted using a nonlinguistic cue (e.g., flashing, pointing, changes in size), nor when sentences specified a nondirectional relationship (e.g., \u2018\u2018the red is touching the green\u2019\u2019). The relation between children\u2019s matching performance and their long-term knowledge of directional terms suggests two distinct mechanisms by which language can temporarily bridge delays, providing more stable representations. There is a natural tension in the cognitive sciences between the view that cognitive systems are specialized and possibly modular and the fact that these systems regularly interact in everyday tasks. A paradigm case involves language and vision: The two systems have structures that are quite different, embodying different representational bases and computational properties, yet the systems obviously interact, as people can talk about what they see. Little is known, however, about the mechanisms underlying the interactions between language and vision, and even less is known about how they emerge during development. In this article, we report experiments showing that language serves at least one clear function as it interacts with the visual system: It can help maintain the conjunction of visual features\u2014color and location\u2014that is otherwise quite fragile. Our framework for thinking about language-vision interactions starts with Jackendoff\u2019s (1987) observation that although language and vision are specialized, each can enhance functions that are weak in the other. For example, language can naturally capture the distinction between a category exemplar (\u2018\u2018a chair\u2019\u2019) and a token of that category (\u2018\u2018my chair\u2019\u2019); vision can naturally capture the distinctions among geese, ducks, and swans (e.g., neck length). Thus, language and vision are complementary, each adding selectively to the expressive power of the other. Using this framework, we tested the possibility that language enhances cognitive representations, affording additional expressive power beyond what vision alone provides. This possibility fits squarely within current debates about the effects of language on thought. Views range from strongly Whorfian, suggesting that language causes new kinds of representations in domains such as space and number (Carey, 2001; HermerVazquez, Spelke, & Katsnelson, 1999; Levinson, 1996), to strongly anti-Whorfian, suggesting that language merely reflects what people can already represent (Munnich & Landau, 2003; Papafragou, Massey, & Gleitman, 2002). Through our studies, we explored a third possibility: Language may temporarily enrich areas of visual representation that are fragile on their own. We considered a test case that is well known to vision scientists: the difficulty of forming and maintaining feature conjunctions (e.g., color and location). This process appears to require focused attention in adults (e.g., Treisman & Gelade, 1980; Wheeler & Treisman, 2002). We asked whether language can play a role in forming or maintaining such conjunctions in young children, who have underdeveloped control of attention and for whom language may therefore play an especially crucial role. Evidence suggests that in adults, attention is required to form and maintain feature conjunctions. When people search for a single feature in a display (e.g., a redO among greenOs), search Address correspondence to Banchiamlack Dessalegn, Cognitive Science Department, Johns Hopkins University, Room # 237, Krieger Hall, 3400 N. Charles St., Baltimore, MD 21218, e-mail: banchi@ cogsci.jhu.edu. PSYCHOLOGICAL SCIENCE Volume 19\u2014Number 2 189 Copyright r 2008 Association for Psychological Science at NANYANG TECH UNIV LIBRARY on May 26, 2015 pss.sagepub.com Downloaded from time does not vary as a function of set size, but when they search for a feature conjunction (e.g., a red O among green Os and red Ls), search time increases linearly with set size. The latter finding suggests that in conjunction search, individual items must be attended separately. More dramatic is the occasional failure to bind: When people observe a briefly presented display containing a red O next to a green L, they may mistakenly report that they have seen either a red L or a green O. Such illusory conjunctions reflect failure to bind color and shape, a process requiring active allocation of attention to the target\u2019s location (Treisman & Schmidt, 1982). If attention is the mechanism for binding and maintaining feature conjunctions, then young children, who undergo prolonged development of attention (Ruff & Capozzoli, 2003), might bind features incorrectly more often than adults. Although we know of no direct evidence for illusory conjunctions in young children, binding color and location and maintaining such conjunctions may be difficult for them. Hoffman, Landau, and Pagani (2003) found that 6-year-olds given a complex matching task with targets that were internally split by color (e.g., red on the left and green on the right) often erroneously matched these targets to their reflections (e.g., green on the left and red on the right). Language might help children maintain such color-location conjunctions. Spivey, Tyler, Eberhard, and Tanenhaus (2001) showed that adults\u2019 performance in traditional conjunction search is modulated on-line by language. In that study, participants searched for targets defined by two features, for example, a red vertical line among green vertical and red horizontal lines. When participants heard a question instructing them what to look for (e.g., \u2018\u2018Is there a red vertical?\u2019\u2019) just prior to onset of the display, Spivey et al. found the standard increase in reaction time as set size increased. But this effect was reduced when the instructions were presented concurrently with the onset of the display. Spivey et al. argued that as soon as the color word was heard (e.g., \u2018\u2018red\u2019\u2019), participants narrowed their attention to items that had the named property (e.g., the red items), essentially creating an efficient search for a single feature (e.g., vertical orientation). Spivey et al. concluded that language can temporarily drive attention, thus modulating visual feature processing. Language might have especially pronounced effects in children. Smith and her colleagues argued that during word learning, language comes to automatically direct young children\u2019s attention to relevant object properties (e.g., Smith, Jones, & Landau, 1996). In this view, naming an object drives attention to that object. The same mechanism might also result in enhanced binding and maintenance of object-internal properties. Such a mechanism is similar to temporary deictic pointers (or indexes), which have been argued to powerfully modulate cognitive activities (Ballard, Hayhoe, Pook, & Rao, 1997; Pylyshyn, 2003). Encoding spatial relationships (e.g., color and location) might be especially susceptible to effects of language. Gentner (e.g., Loewenstein & Gentner, 2005) has proposed that relational labels invite children to extract abstract properties and relations, moving them away from analyses of single properties. For example, 3-year-olds match using object identity when instructed to look for \u2018\u2018the same\u2019\u2019 object, but match using relational (sizebased) concepts when instructed to look for \u2018\u2018the baby\u2019\u2019 (i.e., smallest) object (Ratterman & Gentner, 1998). Language could also be crucial to forming and maintaining visual property conjunctions that are fragile on their own. In our experiments, we asked whether children experience failures to maintain feature conjunctions, whether language can play a role in strengthening these representations, and, if so, what aspects of language work and by what mechanism. We first asked whether labeling the target with an object name increases the likelihood of correctly binding and maintaining color and location.",
    "authors": [
        {
            "affiliations": [],
            "name": "Banchiamlack Dessalegn"
        },
        {
            "affiliations": [],
            "name": "Barbara Landau"
        }
    ],
    "id": "SP:b69cd2a546e0783df05cd778fd4a8b2de7a9eeb2",
    "references": [
        {
            "authors": [
                "D. Ballard",
                "M. Hayhoe",
                "P. Pook",
                "R. Rao"
            ],
            "title": "Deictic codes for the embodiment of cognition [Target article and commentaries",
            "venue": "Behavioral and Brain Sciences,",
            "year": 1997
        },
        {
            "authors": [
                "S. Carey"
            ],
            "title": "Bridging the gap between cognition and developmental neuroscience: The example of number representation",
            "venue": "C.A. Nelson & M. Luciana (Eds.), Handbook of developmental cognitive neuroscience (pp. 415\u2013431). Cambridge, MA: MIT Press.",
            "year": 2001
        },
        {
            "authors": [
                "L. Hermer-Vazquez",
                "E.S. Spelke",
                "A.S. Katsnelson"
            ],
            "title": "Sources of flexibility in human cognition: Dual-task studies of space and language",
            "venue": "Cognitive Psychology,",
            "year": 1999
        },
        {
            "authors": [
                "J. Hoffman",
                "B. Landau",
                "B. Pagani"
            ],
            "title": "Spatial breakdown in spatial construction: Evidence from eye fixations in children with Williams syndrome",
            "venue": "Cognitive Psychology,",
            "year": 2003
        },
        {
            "authors": [
                "R. Jackendoff"
            ],
            "title": "On beyond zebra: The relation of linguistic and visual information",
            "venue": "Cognition, 26, 89\u2013114.",
            "year": 1987
        },
        {
            "authors": [
                "B. Landau",
                "J.E. Hoffman"
            ],
            "title": "Parallels between spatial cognition and spatial language: Evidence from Williams syndrome",
            "venue": "Journal of Memory and Language,",
            "year": 2005
        },
        {
            "authors": [
                "S.C. Levinson"
            ],
            "title": "Language and space",
            "venue": "Annual Review of Anthropology, 25, 353\u2013382.",
            "year": 1996
        },
        {
            "authors": [
                "J. Loewenstein",
                "D. Gentner"
            ],
            "title": "Relational language and the development of relational mapping",
            "venue": "Cognitive Psychology,",
            "year": 2005
        },
        {
            "authors": [
                "E. Munnich",
                "B. Landau"
            ],
            "title": "The effects of spatial language on spatial representation: Setting some boundaries",
            "year": 2003
        },
        {
            "authors": [
                "A. Papafragou",
                "C. Massey",
                "L. Gleitman"
            ],
            "title": "Shake, rattle, \u2018n\u2019 roll: The representation of motion in language and cognition",
            "year": 2002
        },
        {
            "authors": [
                "Z. Pylyshyn"
            ],
            "title": "Seeing and visualizing: It\u2019s not what you think",
            "venue": "Cambridge, MA: MIT Press.",
            "year": 2003
        },
        {
            "authors": [
                "M. Ratterman",
                "D. Gentner"
            ],
            "title": "More evidence for a relational shift in the development of analogy: Children\u2019s performance on a causal-mapping",
            "venue": "task. Cognitive Development,",
            "year": 1998
        },
        {
            "authors": [
                "H. Ruff",
                "M. Capozzoli"
            ],
            "title": "Development of attention and distractibility in the first 4 years of life",
            "venue": "Developmental Psychology,",
            "year": 2003
        },
        {
            "authors": [
                "L. Smith",
                "S. Jones",
                "B. Landau"
            ],
            "title": "Naming in young children: A dumb attentional mechanism",
            "venue": "Cognition,",
            "year": 1996
        },
        {
            "authors": [
                "E.S. Spelke",
                "S. Tsivkin"
            ],
            "title": "Language and number: A bilingual training study",
            "venue": "Cognition,",
            "year": 2001
        },
        {
            "authors": [
                "M.J. Spivey",
                "M.J. Tyler",
                "K.M. Eberhard",
                "M.K. Tanenhaus"
            ],
            "title": "Linguistically mediated visual search",
            "venue": "Psychological Science,",
            "year": 2001
        },
        {
            "authors": [
                "A. Treisman",
                "G. Gelade"
            ],
            "title": "A feature integration theory of attention",
            "venue": "Cognitive Psychology,",
            "year": 1980
        },
        {
            "authors": [
                "A. Treisman",
                "H. Schmidt"
            ],
            "title": "Illusory conjunctions in the perception of objects",
            "venue": "Cognitive Psychology,",
            "year": 1982
        },
        {
            "authors": [
                "M. Wheeler",
                "A. Treisman"
            ],
            "title": "Binding in short-term visual memory",
            "venue": "Journal of Experimental Psychology: General,",
            "year": 2002
        }
    ],
    "sections": [
        {
            "text": "sion by focusing on a well-known problem: the binding and maintenance of color-location conjunctions. Four-yearolds performed a task in which they saw a target (e.g., a split square, red on the left and green on the right) followed by a brief delay and thenwere asked to find the target in an array including the target, its reflection (e.g., red on the right and green on the left), and a square with a different geometric split. Errors were overwhelmingly reflections. This finding shows that the children failed to maintain color-location conjunctions. Performance improved when targets were accompanied by sentences specifying color and direction (e.g., \u2018\u2018the red is on the left\u2019\u2019), but not when the conjunction was highlighted using a nonlinguistic cue (e.g., flashing, pointing, changes in size), nor when sentences specified a nondirectional relationship (e.g., \u2018\u2018the red is touching the green\u2019\u2019). The relation between children\u2019s matching performance and their long-term knowledge of directional terms suggests two distinct mechanisms by which language can temporarily bridge delays, providing more stable representations.\nThere is a natural tension in the cognitive sciences between the view that cognitive systems are specialized and possibly modular and the fact that these systems regularly interact in everyday tasks. A paradigm case involves language and vision: The two systems have structures that are quite different, embodying different representational bases and computational properties, yet the systems obviously interact, as people can talk about what they see. Little is known, however, about the mechanisms underlying the interactions between language and vision, and even less is known about how they emerge during\ndevelopment. In this article, we report experiments showing that language serves at least one clear function as it interacts with the visual system: It can help maintain the conjunction of visual features\u2014color and location\u2014that is otherwise quite fragile.\nOur framework for thinking about language-vision interactions starts with Jackendoff\u2019s (1987) observation that although language and vision are specialized, each can enhance functions that are weak in the other. For example, language can naturally capture the distinction between a category exemplar (\u2018\u2018a chair\u2019\u2019) and a token of that category (\u2018\u2018my chair\u2019\u2019); vision can naturally capture the distinctions among geese, ducks, and swans (e.g., neck length). Thus, language and vision are complementary, each adding selectively to the expressive power of the other.\nUsing this framework, we tested the possibility that language enhances cognitive representations, affording additional expressive power beyond what vision alone provides. This possibility fits squarely within current debates about the effects of language on thought. Views range from strongly Whorfian, suggesting that language causes new kinds of representations in domains such as space and number (Carey, 2001; HermerVazquez, Spelke, & Katsnelson, 1999; Levinson, 1996), to strongly anti-Whorfian, suggesting that language merely reflects what people can already represent (Munnich & Landau, 2003; Papafragou, Massey, & Gleitman, 2002). Through our studies, we explored a third possibility: Language may temporarily enrich areas of visual representation that are fragile on their own. We considered a test case that is well known to vision scientists: the difficulty of forming and maintaining feature conjunctions (e.g., color and location). This process appears to require focused attention in adults (e.g., Treisman & Gelade, 1980; Wheeler & Treisman, 2002). We asked whether language can play a role in forming or maintaining such conjunctions in young children, who have underdeveloped control of attention and for whom language may therefore play an especially crucial role.\nEvidence suggests that in adults, attention is required to form and maintain feature conjunctions. When people search for a single feature in a display (e.g., a redO among greenOs), search Address correspondence to Banchiamlack Dessalegn, Cognitive Science Department, Johns Hopkins University, Room # 237, Krieger Hall, 3400 N. Charles St., Baltimore, MD 21218, e-mail: banchi@ cogsci.jhu.edu.\nVolume 19\u2014Number 2 189Copyright r 2008 Association for Psychological Science at NANYANG TECH UNIV LIBRARY on May 26, 2015pss.sagepub.comDownloaded from\ntime does not vary as a function of set size, but when they search for a feature conjunction (e.g., a red O among green Os and red Ls), search time increases linearly with set size. The latter finding suggests that in conjunction search, individual items must be attended separately. More dramatic is the occasional failure to bind: When people observe a briefly presented display containing a red O next to a green L, they may mistakenly report that they have seen either a red L or a green O. Such illusory conjunctions reflect failure to bind color and shape, a process requiring active allocation of attention to the target\u2019s location (Treisman & Schmidt, 1982).\nIf attention is the mechanism for binding and maintaining feature conjunctions, then young children, who undergo prolonged development of attention (Ruff & Capozzoli, 2003), might bind features incorrectly more often than adults. Although we know of no direct evidence for illusory conjunctions in young children, binding color and location and maintaining such conjunctions may be difficult for them. Hoffman, Landau, and Pagani (2003) found that 6-year-olds given a complex matching task with targets that were internally split by color (e.g., red on the left and green on the right) often erroneously matched these targets to their reflections (e.g., green on the left and red on the right).\nLanguage might help children maintain such color-location conjunctions. Spivey, Tyler, Eberhard, and Tanenhaus (2001) showed that adults\u2019 performance in traditional conjunction search is modulated on-line by language. In that study, participants searched for targets defined by two features, for example, a red vertical line among green vertical and red horizontal lines. When participants heard a question instructing them what to look for (e.g., \u2018\u2018Is there a red vertical?\u2019\u2019) just prior to onset of the display, Spivey et al. found the standard increase in reaction time as set size increased. But this effect was reduced when the instructions were presented concurrently with the onset of the display. Spivey et al. argued that as soon as the color word was heard (e.g., \u2018\u2018red\u2019\u2019), participants narrowed their attention to items that had the named property (e.g., the red items), essentially creating an efficient search for a single feature (e.g., vertical orientation). Spivey et al. concluded that language can temporarily drive attention, thus modulating visual feature processing.\nLanguage might have especially pronounced effects in children. Smith and her colleagues argued that during word learning, language comes to automatically direct young children\u2019s attention to relevant object properties (e.g., Smith, Jones, & Landau, 1996). In this view, naming an object drives attention to that object. The same mechanism might also result in enhanced binding and maintenance of object-internal properties. Such a mechanism is similar to temporary deictic pointers (or indexes), which have been argued to powerfully modulate cognitive activities (Ballard, Hayhoe, Pook, & Rao, 1997; Pylyshyn, 2003).\nEncoding spatial relationships (e.g., color and location) might be especially susceptible to effects of language. Gentner (e.g., Loewenstein & Gentner, 2005) has proposed that relational la-\nbels invite children to extract abstract properties and relations, moving them away from analyses of single properties. For example, 3-year-olds match using object identity when instructed to look for \u2018\u2018the same\u2019\u2019 object, but match using relational (sizebased) concepts when instructed to look for \u2018\u2018the baby\u2019\u2019 (i.e., smallest) object (Ratterman & Gentner, 1998). Language could also be crucial to forming and maintaining visual property conjunctions that are fragile on their own.\nIn our experiments, we asked whether children experience failures to maintain feature conjunctions, whether language can play a role in strengthening these representations, and, if so, what aspects of language work and by what mechanism. We first asked whether labeling the target with an object name increases the likelihood of correctly binding and maintaining color and location.\nEXPERIMENT 1"
        },
        {
            "heading": "Participants",
            "text": "Twenty-four 4-year-olds (mean age5 4 years 6 months, range5 4 years 0 months through 5 years 0 months) were randomly assigned to a label (n 5 12) or a no-label (n 5 12) condition."
        },
        {
            "heading": "Design, Stimuli, and Procedure",
            "text": "On each trial in the no-label condition (see Fig. 1a), one of eight different targets appeared at the top center of a computer screen. All targets were square blocks split in half by color (red, green) along one of three axes (vertical, horizontal, or diagonal; see\n190 Volume 19\u2014Number 2\nat NANYANG TECH UNIV LIBRARY on May 26, 2015pss.sagepub.comDownloaded from\nFig. 1b). The children were told, \u2018\u2018Look at this. I want you to help me find one that is exactly the same.\u2019\u2019 The target then disappeared, and after a 1-s delay, three test objects appeared at the bottom of the screen: the target\u2019s match, its reflection, and a differently partitioned square (referred to hereafter as the different distractor; see Fig. 1a). The children were asked to select the square that \u2018\u2018looks exactly the same as the one you just saw.\u2019\u2019 The label condition was identical, except that the children were told, \u2018\u2018Look!\u2019\u2019 and then heard a sentence labeling the target with one of eight novel nouns (\u2018\u2018This is a dax/wazzle/tam/dige/zav/ feingle/jic/bevit\u2019\u2019). They were then told, \u2018\u2018I want you to help me find one that is exactly the same,\u2019\u2019 and when the test items appeared, they were prompted to find the one that \u2018\u2018looks exactly the same as the ___ you just saw.\u2019\u2019 Thus, we used a mixed design including one between-subjects factor (condition: no-label or label) and one within-subjects factor (target type: vertical, horizontal, or diagonal split).\nEach target was presented three times, for a total of 24 trials, ordered randomly. Before the experiment, the children received 6 practice trials, 2 using familiar targets (e.g., animals) and test items from different categories, and 4 using novel symmetric shapes split in half by color and test items that included the target, its reflection, and a second distractor."
        },
        {
            "heading": "Results and Discussion",
            "text": "As Figure 2 shows, the children chose the match more often than expected by chance (33%), both in the no-label condition, t(11) 5 7.23, p< .001, prep> .99, and in the label condition, t(11)5 6.42, p< .001, prep> .99. They also chose more reflections than different distractors in both the no-label condition, Wilcoxon t(11) 5 2.87, p < .01, prep > .96, and the label condition, Wilcoxon t(11)5 2.28, p< .01, prep> .96. The children accurately represented the internal geometry of the square (vertical, horizontal, or diagonal split), as 92% of the choices were the match (64%) or the reflection (28%). Errors largely involved incorrect assignment of color to location, with 77% of the errors being\nreflections. Thus, the children had difficulty maintaining the conjunction of color and location, a finding consistent with previous results (Hoffman et al., 2003), and with abundant work with adult participants showing fragility of such visual conjunctions.\nThe presence of a novel label had no effect on performance. A mixed analysis of variance with condition (no label or label) and target type (vertical, horizontal, or diagonal split) as factors showed only a significant effect of target type, F(2, 44) 5 3.36, p < .05, prep > .88. The children performed significantly better with horizontally split targets (70.8%correct) thanwith vertically split targets (56.9% correct), t(23) 5 2.32, p < .05, prep > .90.\nOur main manipulation, labeling the target, was not effective in improving children\u2019s performance above the level in the nolabel condition, so naming the objects did not enhance maintenance of the property conjunctions. Given that the crucial distinguishing factor between targets and their reflections was the relation between color and location, we hypothesized that relational terms might be required to enhance performance. In Experiment 2, we provided relational terms that gave explicit color and location information.\nEXPERIMENT 2"
        },
        {
            "heading": "Method",
            "text": "Twenty-four 4-year-olds participated (mean age 5 4 years 6 months, range 5 4 years 2 months through 4 years 11 months). The design, stimuli, and procedures were identical to those in Experiment 1 except for the verbal instructions. After the children looked at the square, the experimenter said, \u2018\u2018Let\u2019s ask where the red is. Where is the red?\u2019\u2019 She then clicked on the target, saying, \u2018\u2018The red is . . .,\u2019\u2019 and an audio file played a voice that completed the sentence appropriately (e.g., \u2018\u2018on the left\u2019\u2019). For the vertically split targets, the recorded voice said \u2018\u2018left\u2019\u2019 or \u2018\u2018right\u2019\u2019; for the horizontally split targets, the voice said \u2018\u2018top\u2019\u2019 or \u2018\u2018bottom.\u2019\u2019 For the diagonally split squares (which could be labeled either way), half of the children were told the red was on\nVolume 19\u2014Number 2 191\nat NANYANG TECH UNIV LIBRARY on May 26, 2015pss.sagepub.comDownloaded from\nthe \u2018\u2018left\u2019\u2019 or \u2018\u2018right,\u2019\u2019 and the other half were told it was on the \u2018\u2018top\u2019\u2019 or \u2018\u2018bottom.\u2019\u20191 Then, as in Experiment 1, the children were told, \u2018\u2018I want you to help me find one that is exactly the same,\u2019\u2019 and when the test items appeared, the children were prompted to find the one that \u2018\u2018looks exactly the same as the one you just saw.\u2019\u2019\nTo evaluate the children\u2019s understanding of these terms, we carried out two posttests, following Landau and Hoffman (2005). In the production test, the children viewed a square on the computer screen, and a small face appeared four times next to each of the four sides. The 16 trials were presented in random order. On each trial, the children were asked, \u2018\u2018Where is the face to the square?\u2019\u2019 and they were prompted with, \u2018\u2018The face is ___.\u2019\u2019 Next, in the comprehension task, the children were asked to put an X on the left, right, top, or bottom of a solid square presented on a sheet of paper. Each instruction was presented four times, in random order, for a total of 16 trials."
        },
        {
            "heading": "Results and Discussion",
            "text": "The children again chose the match more often than expected by chance, t(23) 5 30.5, p < .001, prep > .99, and when they made an error, they selected the reflection more often than the different distractor, Wilcoxon t(23)5 4.34, p< .001, prep> .99; see Fig. 2). They performed better with horizontally split targets (92.36% correct) than with vertically split targets (78.5% correct) or diagonally split targets (75.7% correct), both ts(23) > 3.00, ps < .01, preps > .96.\nThe overall percentage correct was reliably higher in Experiment 2 than in the no-label condition of Experiment 1, t(34)5 4.13, p< .001, prep > .99, d5 1.30. Thus, hearing the location of the red part helped children select the target. How did this verbal information help? One possibility is that the children correctly represented the directional word on each trial (top, bottom, left, or right) and used this word plus their visual representation of the target\u2019s split to encode and retain the location of the red part over the delay. This explanationmight work for the terms top and bottom, but it does not work for left and right. In the production test, the children correctly produced top and bottom on 98.3% of the trials calling for these terms, and in the comprehension test, they placed the X in the correct location on 89.7% of the trials asking them to indicate the top or bottom. Thus, they knew the spatial meanings of these terms. However, the same children produced left and right correctly on only 64.3% of the trials in the production test calling for these terms, and placed the X in the correct location on only 66.5% of the trials asking them to indicate the right or left. Errors on left and right trials showed that the children\u2019s knowledge of these two terms included the correct (horizontal) axis, but not the direction: Left and right confusion errors accounted for 94% of the errors in production and 91% of the errors in comprehension.\nProduction was reliably worse for left and right than for top and bottom, t(21) 5 4.03, p < .01, prep > .96, d 5 1.25, as was comprehension, t(21) 5 3.54, p < .01, prep > .95, d 5 1.09. 2\nDid children use their long-term knowledge of the four directional terms to select the correct test item? It seems unlikely: Neither children\u2019s production accuracy for left and right nor their comprehension accuracy for these terms was reliably correlated with their overall accuracy in the matching task, Pearson\u2019s r(20) 5 .14 and r(20) 5 .24, ps > .10, respectively.3 Nor were these measures of production and comprehension correlated with accuracy on trials with the vertically split targets, rs(20) < .37, ps > .10. We return to this issue in Experiment 4.\nDirectional language helped children retain the color and location structure of the targets, but how? One possibility is that directional language drew attention to the red part and its location. If so, one might expect nonlinguistic attentional cues to work as well. We tested this possibility in Experiment 3.\nEXPERIMENT 3"
        },
        {
            "heading": "Method",
            "text": "Thirty-six 4-year-olds (mean age5 4 years 5 months, range5 4 years 0 months through 4 years 11 months) were randomly assigned to the flashing (n5 12), growing (n5 12), or pointing (n 5 12) condition. The design, stimuli, and procedure were the same as in Experiments 1 and 2, except that after the children were told \u2018\u2018Look!\u2019\u2019 the red part of the target was made salient. In the flashing condition, the red part flashed on for 200 ms and off for 200 ms, and this was repeated five times; in the growing condition, the red part grew for 200 ms and shrunk for 200 ms, and this was repeated five times; and in the pointing condition, children were told \u2018\u2018Point to the red part.\u2019\u2019 All children complied. After the attentional manipulation and while the target was still on the screen, the children were told, \u2018\u2018I want you to help me find one that is exactly the same.\u2019\u2019"
        },
        {
            "heading": "Results and Discussion",
            "text": "In each condition, the children again chose thematchmore often than expected by chance, all ts(11)> 4.9, ps< .001, preps> .98, and when they made an error, they chose the reflection more often than the different distractor, all ts(11) > 2.94, ps < .01, preps > .97 (see Fig. 2). A 3 (condition) 3 (target type) mixed analysis of variance showed only a main effect of target type, F(2, 66)5 3.83, p< .05, prep > .91; performance was better for horizontally and diagonally split targets than for vertically split targets, both ts(35) > 2.1, ps < .05, preps > .87.\n1The labels used for the diagonally split targets (i.e., left or right vs. top or bottom) had no significant effect in this experiment or in Experiment 4; hence, we collapsed the data across these conditions.\n2Two participants did not complete these tasks. 3Performance was at ceiling for top and bottom, so only performance for left\nand right was entered into correlations.\n192 Volume 19\u2014Number 2\nat NANYANG TECH UNIV LIBRARY on May 26, 2015pss.sagepub.comDownloaded from\nRemarkably, the average percentage correct for each of these conditions was no different from the percentage correct in the no-label condition of Experiment 1, ts(22) < 1, but was significantly lower than the percentage correct in Experiment 2, all ts(34)> 3.5, ps< .001, preps> .98, ds> 1.12. Directional terms (Experiment 2) were more effective than the nonlinguistic attention-grabbers in helping children maintain the conjunction of color and location.\nHow did directional expressions improve performance? The directional phrases used in Experiment 2\u2014for example, \u2018\u2018x is on the left [of y]\u2019\u2019\u2014are both relational (i.e., left defines a relation between x and y) and directional (i.e., \u2018\u2018x is on the left of y\u2019\u2019 entails that \u2018\u2018y is on the left of x\u2019\u2019 is false). In our final experiment, we investigated whether the relational nature of the terms alone was sufficient to improve performance. To do this, we replicated Experiment 2, adding nondirectional (neutral) relational terms, such as touching.\nEXPERIMENT 4"
        },
        {
            "heading": "Method",
            "text": "Thirty-six 4-year-olds (mean age5 4 years 6 months, range5 4 years 0 months through 4 years 11 months) were randomly assigned to the neutral (n5 12) or directional (n5 24) condition. The design, stimuli, and procedure were the same as in Experiment 2 except that in the neutral condition, targets were labeled using relational but nondirectional terms: \u2018\u2018The red is touching/ connected to/next to/up against the green.\u2019\u2019 Children in the directional condition heard the same sentence, except with directional terms: \u2018\u2018The red is to the left/right/top/bottom of the green.\u2019\u2019 As in Experiment 2, we administered production and comprehension tasks to evaluate the children\u2019s knowledge of the directional terms."
        },
        {
            "heading": "Results and Discussion",
            "text": "In each condition, the children again chose thematchmore often than expected by chance, ts(11) > 7.0, ps < .001, preps > .99, and when they made errors, they chose the reflection more often than the different distractor, ts(11) > 3.0, ps < .01, preps > .98 (see Fig. 2). A 2 (condition) 3 (target type) mixed analysis of variance on percentage correct showed a main effect of condition,F(1, 34)5 5.8, p< .05, prep> .90, d5 0.82, but no effect of target type and no interaction. Children who heard directional terms chose more matches than those who heard neutral terms. Because both the directional terms and the neutral terms were relational, this finding indicates that the relational nature of the labels alone was not sufficient to enhance children\u2019s performance.\nAs in Experiment 2, children\u2019s performance in the production and comprehension tasks did not predict their performance on the matching task. In the production test, the children correctly produced top and bottom on 98.9% of the trials calling for these\nterms, and in the comprehension test, they placed the X in the correct location on 96.9% of the trials asking them to indicate the top or bottom. However, the same children produced left and right correctly on only 65.5% of the trials calling for these terms, and placed the X in the correct location on only 79.0% of the trials asking them to indicate the left or right. Production and comprehension were reliably worse for left and right than for top and bottom, ts(23)> 3.20, ps< .01, preps> .96, ds> 0.78. There was no reliable correlation between children\u2019s accuracy in producing and comprehending left and right and their overall accuracy on the matching task, nor was there a correlation between these measures of production and comprehension and children\u2019s accuracy on trials with the vertically split targets, rs(23) < .29, ps > .08.\nIn a final set of comparisons, we examined the performance of all children who heard directional terms (those in Experiments 2 and 4, N 5 48). These children performed significantly better than children in the no-label condition of Experiment 1, t(58)5 3.38, p< .001, prep> .98, d5 0.95, and significantly better than the children in each of the attention conditions of Experiment 3, ts(58) > 3.47, ps < .001, preps > .98, ds > 0.88. 4 Directional labels apparently trump all other instructional conditions. However, even for this larger group, there was still no significant correlation between matching accuracy and accuracy of producing and comprehending left and right, all rs < .32, n.s. The scatter plots in Figure 3 suggest why: Some children might have used their long-term understanding of left and right to help them in the matching task, and therefore did well on both tasks. But a substantial number of children did well on the matching task despite doing poorly on the production and comprehension tasks. In the General Discussion, we propose two different mechanisms to explain these two performance patterns."
        },
        {
            "heading": "GENERAL DISCUSSION",
            "text": "We asked whether language can modulate children\u2019s ability to bind and maintain conjunctions of color and location\u2014feature combinations that are known to be fragile even in adults. We found that 4-year-olds robustly encode the internal geometry of a target (i.e., the vertical, horizontal, or diagonal split), but have difficulties maintaining color-location conjunctions. The children\u2019s retention of these conjunctions was enhanced when they heard sentences with directional terms, relative to when they heard no label, an object label, or a nondirectional term, or when the relevant part was made more salient by flashing, changes in\n4Percentage correct was higher in Experiment 2 (82%) than in the directional condition of Experiment 4 (75%), t(46) 5 2.41, p < .05, prep > .92. However, even children in the directional condition of Experiment 4 performed significantly better than those in the no-label condition of Experiment 1, t(32)5 2.29, p < .05, prep > .87, and in each attentional condition of Experiment 3, ts(56) > 2.0, ps < .05, preps > .89.\nVolume 19\u2014Number 2 193\nat NANYANG TECH UNIV LIBRARY on May 26, 2015pss.sagepub.comDownloaded from\nsize, or having the child point to it. The failure of the nonlinguistic manipulations suggests that the language effects were not due to general attentional enhancement; the failure of the relational nondirectional terms (e.g., touching) suggests that the effects were specific to directional terms.\nWe suggest two possible mechanisms to account for these results. Both mechanisms entail the momentary use of language to enhance the maintenance of color-location combinations. We propose that some children used their long-term, stable knowledge of the directional terms to maintain the conjunctions of color and location. Although the children retained the geometric split (e.g., vertical) with or without language (as shown by the high percentage of trials on which either the match or the reflection was chosen in all conditions), children who also had strong knowledge of the directional terms and their syntax could use this knowledge to create a \u2018\u2018hybrid\u2019\u2019 representation that allowed both geometry and the color-location conjunction to be maintained over the 1-s delay. This mechanism would be shortlived in that it would operate solely to bridge the brief delay, but it would depend on long-term knowledge of the directional meanings of the terms. The linguistic representation would serve to represent the conjunction of color and location\u2014not well retained with vision alone\u2014but it would not be required to represent the geometry of the target (which was maintained with or without language).\nA different mechanism is required, however, to explain the performance of children who did not have strong knowledge of the terms, that is, those who knew that left and right were opposite ends of the horizontal axis, but did not know which end was which. We suggest that for these children, the directional terms may have acted as temporary directional pointers. When children saw the target and heard \u2018\u2018The red is on the left,\u2019\u2019 their (partial) understanding of left could have been temporarily matched to their current representation of the red part\u2019s location, in effect, telling them which direction was left. This representation\u2014again temporary\u2014could have been used to bridge the delay, allowing a correct match. Ten minutes later, when these\nchildren were given the production and comprehension tasks, this representation was gone, resulting in failure to distinguish between left and right.\nBoth of these mechanisms suggest that language and vision can interact to create powerful but temporary hybrid representational schemes. These schemes were used to augment the representation of the target in the context of the task, working to maintain the conjunction of color and location in the moment of test.\nOur proposed mechanisms are consistent with recent findings showing that language provides a powerful but temporary modulation of attention (Smith et al., 1996; Spivey et al., 2001), and also consistent with the growing recognition that many cognitive functions are mediated by temporary \u2018\u2018pointers\u2019\u2019 or \u2018\u2018indexes\u2019\u2019 that can enhance performance, especially by reducing the burden of visual-spatial memory (Ballard et al., 1997; Pylyshyn, 2003). The fact that our nonlinguistic manipulations did not have the same effects as language suggests that language provides a particularly powerful means of encoding and carrying forward visual information, at least in children. We do not know whether this is also true for adults; however, preliminary findings in our lab show that verbal shadowing in this task drives adults\u2019 performance down to that of 4-year-olds, with the same predominance of errors of reflection. This suggests that language might be automatically and obligatorily recruited in adults when they perform tasks such as ours, a notion that is consistent with other findings supporting the idea that adults automatically use language in complex cognitive tasks (e.g., Spelke & Tsivkin, 2001). Over development, language may become an obligatory driver of attention, fostering selective but temporary encoding of certain properties of the world (see Landau, Dessalegn, & Goldberg, in press, for discussion).\nOur broader conclusions point to a resolution of some current issues regarding language and thought. We take a position consistent with a strongly anti-Whorfian view and suggest that for children to use the language of left and right correctly, they must first be able to represent these directions nonlinguistically.\n194 Volume 19\u2014Number 2\nat NANYANG TECH UNIV LIBRARY on May 26, 2015pss.sagepub.comDownloaded from\nLanguage can maintain only what is already represented. However, our findings suggest that using language to enrich a visual representation might really enhance one\u2019s capabilities. Our specific example is limited, but it opens up the possibility that many so-called Whorfian effects are, in fact, effects of the temporary modulation of attention through language. The potential power of such effects should not be underestimated.\nAcknowledgments\u2014We appreciate input from James Hoffman\nand the Johns Hopkins University Language and Cognition Lab.\nThis research was supported in part by Research Grant FY-12-\n04-46 from the March of Dimes Birth Defects Foundation, Grant\nRO1-050876 from the National Institute of Neurological Dis-\norders and Stroke, and the National Science Foundation\u2019s In-\ntegrated Graduate Education and Research Training grant to the\nCognitive Science Department at Johns Hopkins University."
        }
    ],
    "title": "The Role of Language in Binding and Maintaining Feature Conjunctions",
    "year": 2009
}